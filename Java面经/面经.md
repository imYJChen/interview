# 面经

来源：**JavaGuide**、二哥的 Java 进阶之路、**面渣逆袭**、面试指北、**小林coding**

阅读过程中记录的一些重要知识点，不包含全部知识点，详细知识点建议看上面两个网站

[JavaGuide](https://javaguide.cn/home.html)：主要用于对知识的全面了解，也给出了哪些知识点是重点，哪些知识了解即

二哥的 Java 进阶之路：也是对知识的全面了解，可以跟JavaGuide互补着看

[面渣逆袭](https://javabetter.cn/home.html)：以题目的形式，整理了每一板块的题目，可以背诵

[面试指北（付费）](https://www.yuque.com/snailclimb/mf2z3k)：JavaGuide总结出来的一些面经、一些付费知识点等等

[小林coding](https://xiaolincoding.com/)：主要知识点更详细

------

## 计网 :white_check_mark:

重点：HTTP1.0~3.0、HTTPS、TCP、UDP、DDos

##### 网络分层模型

###### 各层数据格式

> 四层传输层数据被称作**段**（Segments）
>
> - TCP
>
> ![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/fd8c2eb9ec2d7aea21e3e1c4e42f658f.png)
>
> 三层网络层数据被称做**包/分组**（Packages）
>
> ![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/00fb975e6752d4b778d8942fa4313f80.png)
>
> 二层数据链路层时数据被称为**帧**（Frames）
>
> ![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/84aaa507ef704b3dab97c5c24d7605ea.png)
>
> 一层物理层时数据被称为**比特流**（bits）
>
> ![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/b9b9fe2010689e549cb5698f05b93b57.png)



##### 在浏览器输入一个网址后回车，会发生什么？:star:

> **在浏览器地址栏键入URL，回车：**
>
> 1.DNS解析：浏览器会依据URL逐层查询DNS服务器，解析URL中的域名对应的IP地址
>
> 2.TCP连接：三次握手
>
> 3.发送HTTP请求
>
> 4.服务器处理请求并返回HTTP报文
>
> 5.浏览器解析渲染页面
>
> 6.连接结束：四次挥手



##### HTTP响应码

**1xx：信息性状态码**

表示请求已接收，客户端可以继续请求

| 状态码                      | 含义                                                 |
| --------------------------- | ---------------------------------------------------- |
| **100 Continue**            | 服务器收到请求的**初始部分**，客户端继续发送剩余请求 |
| **101 Switching Protocols** | 服务器同意**切换协议**（如 HTTP/1.1 -> WebSocket）   |
| **102 Processing**          | 服务器正在处理请求，但还未完成                       |

**2xx：成功状态码**

请求已被服务器正确处理

| 状态码             | 含义                                                         |
| ------------------ | ------------------------------------------------------------ |
| **200 OK**         | **请求成功**（常见，如 GET/POST 成功返回数据）               |
| **201 Created**    | **请求已成功**，且创建了新资源（如 `POST /users` 创建新用户） |
| **202 Accepted**   | 服务器接受请求，但还未处理（异步操作）                       |
| **204 No Content** | 请求成功，但**无返回内容**（如 `DELETE` 成功但无返回）       |

**3xx：重定向状态码**

客户端需要执行额外的操作（如跳转到新 URL）。

| 状态码                     | 含义                                                         |
| -------------------------- | ------------------------------------------------------------ |
| **301 Moved Permanently**  | 资源已**永久**移动，浏览器会自动跳转                         |
| **302 Found**              | 资源临时移动，通常用于**重定向**（浏览器会跟随）             |
| **303 See Other**          | `POST` 请求后的**跳转**，应该用 `GET` 请求新地址             |
| **304 Not Modified**       | 资源未修改，客户端使用**缓存**数据（常用于 `ETag` 和 `Last-Modified`） |
| **307 Temporary Redirect** | 临时重定向，保持原请求方法（与 302 类似但更严格）            |

重定向有两种：临时重定向和永久重定向

​	永久重定向：301、308

​	临时重定向：302、303、307

**4xx：客户端错误状态码**

请求有问题，服务器拒绝处理。

| 状态码                     | 含义                                                  |
| -------------------------- | ----------------------------------------------------- |
| **400 Bad Request**        | **请求错误**，服务器无法解析（参数错误、格式错误）    |
| **401 Unauthorized**       | **未授权**，需要认证（如 API 需要 `token`）           |
| **403 Forbidden**          | **禁止访问**（权限不足）                              |
| **404 Not Found**          | **资源不存在**（URL 错误或资源被删除）                |
| **405 Method Not Allowed** | **请求方法不允许**（如 `POST /users` 但只支持 `GET`） |
| **408 Request Timeout**    | **请求超时**，服务器未收到完整请求                    |
| **409 Conflict**           | **请求冲突**（如创建资源时数据已存在）                |
| **413 Payload Too Large**  | **请求体过大**，服务器拒绝处理                        |
| **429 Too Many Requests**  | **请求过多**（触发 API 限流）                         |

**5xx：服务器错误状态码**

服务器未能正确处理请求。

| 状态码                             | 含义                                 |
| ---------------------------------- | ------------------------------------ |
| **500 Internal Server Error**      | **服务器内部错误**                   |
| **501 Not Implemented**            | 服务器**不支持**该请求方法           |
| **502 Bad Gateway**                | **网关错误**，上游服务器无响应       |
| **503 Service Unavailable**        | **服务器暂时不可用**（如服务器过载） |
| **504 Gateway Timeout**            | **网关超时**，上游服务器无响应       |
| **505 HTTP Version Not Supported** | **HTTP 版本不支持**                  |





##### HTTP1.0 => HTTP1.1 => HTTP2.0 => HTTP3.0

> ###### HTTP1.0 vs. HTTP1.1
>
> **连接方式** : 
>
> ​	HTTP/1.0 为**短连接**，HTTP/1.1 支持**长连接**
>
> ​	HTTP1.0 请求只能一个一个发，上一个返回响应了下一个才能发
>
> ​	HTTP1.1 在同一个 TCP 连接里面，客户端可以发起多个请求，只要第一个请求发出去了，不必等其回来，就可以发第二个请求出去。不过服务端还是按**顺序处理请求**
>
> **请求方式**：
>
> ​	HTTP1.0：`GET`、`POST`、`HEAD`
>
> ​	HTTP1.1：`DELETE`、`PUT`、`PATCH`、`TRACE`、`CONNECT`、`OPTIONS`
>
> **状态响应码** : 
>
> ​	HTTP/1.1 中新加入了大量的状态码，如: 
>
> ​	`100 (Continue)`——在请求大资源前的预热请求（客户端在发送请求体之前先发送请求头进行试探服务器是否接收请求体）
>
> ​	`206 (Partial Content)`——范围请求的标识码，配合`range`头使用
>
> ​	`409 (Conflict)`——请求与当前资源的规定冲突
>
> ​	`410 (Gone)`——资源已被永久转移，而且没有任何已知的转发地址
>
> **缓存机制** : 
>
> ​	在 HTTP/1.0 中主要使用 Header 里的 If-Modified-Since、Expires 来做为缓存判断的标准
>
> ​	在 HTTP/1.1 则引入了更多的缓存控制策略例如 Entity tag，If-Unmodified-Since, If-Match, If-None-Match 等更多可供选择的缓存头来控制缓存策略
>
> **带宽**：
>
> ​	HTTP/1.0 中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能
>
> ​	HTTP/1.1 则在请求头引入了 `range` 头域，它允许只请求资源的某个部分，即返回码是 `206`（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接。
>
> **Host 头处理**：
>
> ​	HTTP/1.1 引入了 `Host` 头字段，允许**在同一 IP 地址上托管多个域名**，从而支持**虚拟主机**的功能，如果请求不带Host会返回 `400` Bad Request
>
> ​	而 HTTP/1.0 没有 Host 头字段，无法实现虚拟主机
>
> ​	如果多个域名<u>a.com b.com c.com</u>都映射到同一个ip上，那么访问这三个网站时，DNS都会获取同一个IP，接着你访问的是index.html，那请求为 GET /index.html，这样就不知道具体访问哪个站点的index.html
>
> ​	所以需要host，它可以记录具体的域名，即当你访问a.com/index.html时，发送的请求为：
>
> ```
> GET /index.html·
> HOST: a.com
> ```
>
> 
>
> ###### HTTP1.x vs. HTTP2.0
>
> **多路复用（Multiplexing）**：
>
> ​	HTTP/2.0 在同一连接上可以**同时传输多个请求和响应**（可以看作是 HTTP/1.1 中长链接的升级版本），互不干扰，每个请求都有一个id，多个请求共用一个TCP连接
>
> ​	HTTP/1.1 则使用**串行**方式，每个请求和响应都需要独立的连接，而浏览器为了控制资源会有 6-8 个 TCP 连接的限制
>
> **二进制帧（Binary Frames）**：
>
> ​	HTTP/2.0 使用**二进制帧**进行数据传输，而 HTTP/1.1 则使用**文本格式**的报文
>
> **头部压缩（Header Compression）**：
>
> ​	HTTP/1.1 支持`Body`压缩，`Header`不支持压缩，每次都会重复发送
>
> ​	HTTP/2.0 支持对`Header`压缩，使用了专门为`Header`压缩而设计的 **HPACK 算法**，通讯双方各自缓存一份header fields表，避免了重复Header的传输，减少了网络开销
>
> **服务器推送（Server Push）**：
>
> ​	HTTP/2.0 支持**服务器推送**，可以在客户端请求一个资源时，将其他相关资源一并推送给客户端，从而减少了客户端的请求次数和延迟。
>
> ​	而 HTTP/1.1 需要客户端自己发送请求来获取相关资源。
>
> **数据流**：
>
> ​	HTTP2.0中每个请求/响应的所有数据包组成一个数据流，每个数据流都有独一无二的编号。
>
> ​	规定客户端发出的请求数据流编号为奇数，服务端响应的数据流编号为偶数
>
> 
>
> **注：**HTTP2.0的多路复用和HTTP1.X中的长连接复用有什么区别
>
> > - HTTP/1.1的Pipeling为若干个请求排队串行化单线程处理，后面的请求等待前面请求的返回才能获得执行机会，一旦有某请求超时等，后续请求只能被阻塞，毫无办法；
> > - HTTP2.0多个请求可同时在一个连接上并行执行，某个请求任务耗时严重，不会影响到其它连接的正常执行
> > - HTTP/1.1 中的管道传输中如果有一个请求阻塞了，那么队列后请求也统统被阻塞住了
> > - HTTP/2 多请求复用一个TCP连接，一旦发生丢包，就会阻塞住所有的 HTTP 请求（因为TCP不知道该连接上是有多个HTTP请求的，只能把它们都看成一个HTTP请求）
>
> 
>
> ###### HTTP2.0 vs. HTTP3.0
>
> - **传输协议**：
>
>   HTTP/2.0 是基于 **TCP** 协议实现的
>
>   HTTP/3.0 新增了 **QUIC**（Quick UDP Internet Connections） 协议来实现可靠的传输，提供与 TLS/SSL 相当的安全性，具有较低的连接和传输延迟。你可以将 QUIC 看作是 **UDP** 的升级版本，在其基础上新增了很多功能比如加密、重传等等。
>
> - **连接建立**：
>
>   HTTP/2.0 需要经过经典的 TCP 三次握手过程，3个RTT
>
>   HTTP/3.0 由于 QUIC 协议的特性，连接建立仅需 0-RTT 或者 1-RTT
>
> - **头部压缩**：
>
>   HTTP/2.0 使用 **HPACK** 算法进行头部压缩
>
>   HTTP/3.0 使用更高效的 **QPACK** 头压缩算法。
>
> - **队头阻塞**：
>
>   HTTP/2.0 多请求复用一个 TCP 连接，一旦发生丢包，就会**阻塞住所有的 HTTP 请求**
>
>   HTTP/3.0 在一定程度上解决了队头阻塞问题，一个连接建立多个不同的数据流，这些数据流之间独立互不影响，某个数据流发生丢包了，其数据流不受影响（本质上是多路复用+轮询）。
>
> - **连接迁移**：
>
>   HTTP/3.0 支持连接迁移，因为 QUIC 使用 64 位 ID 标识连接，只要 ID 不变就不会中断，网络环境改变时（如从 **Wi-Fi 切换到移动数据**）也能保持连接
>
>   而 TCP 连接是由（源 IP，源端口，目的 IP，目的端口）组成，这个四元组中一旦有一项值发生改变，这个连接也就不能用了
>
> - **错误恢复**：
>
>   HTTP/3.0 具有更好的错误恢复机制，当出现丢包、延迟等网络问题时，可以更快地进行恢复和重传
>
>   而 HTTP/2.0 则需要依赖于 TCP 的错误恢复机制。
>
> - **安全性**：
>
>   在 HTTP/2.0 中，TLS 用于加密和认证整个 HTTP 会话，包括所有的 HTTP 头部和数据负载。TLS 的工作是在 TCP 层之上，它加密的是在 TCP 连接中传输的应用层的数据，并不会对 TCP 头部以及 TLS 记录层头部进行加密，所以在传输的过程中 TCP 头部可能会被攻击者篡改来干扰通信。
>
>   而 HTTP/3.0 的 QUIC 对整个数据包（包括报文头和报文体）进行了加密与认证处理，保障安全性。



##### HTTP => HTTPS  :star:

> ###### HTTP vs. HTTPS
>
> ①**端口号**
>
> ​	HTTP 默认是 80
>
> ​	HTTPS 默认是 443
>
> ②**URL 前缀**
>
> ​	HTTP 的 URL 前缀是 `http://`
>
> ​	HTTPS 的 URL 前缀是 `https://`
>
> ③**安全性**
>
> ​	HTTP 是**明文**传输，客户端和服务器端都无法验证对方的身份
>
> ​	HTTPS 是运行在 **SSL** 之上的 HTTP 协议，SSL 运行在 TCP 之上。
>
> ​	所有传输的内容都经过**加密**，加密采用**对称加密**，但<u>对称加密的密钥用服务器方的证书进行了非对称加密</u>
>
> ​	HTTP 安全性没有 HTTPS 高，但是 HTTPS 比 HTTP 耗费更多服务器资源
>
> ④**SEO（搜索引擎优化）**
>
> ​	搜索引擎通常会<u>更青睐使用 HTTPS 协议的网站</u>，因为 HTTPS 能够提供更高的安全性和用户隐私保护。使用 HTTPS 协议的网站在搜索结果中可能会被优先显示，从而对 SEO 产生影响。



##### HTTPS原理 :star:

对称加密：双方使用同一把密钥加密解密

非对称加密：客户端随机生成一个对称密钥，再使用服务端提供的**公钥**，对对称密钥进行加密，发给服务端，服务端用**私钥**解密，得到了对称密钥

> - Client发起一个HTTPS的请求
> - Server把事先配置好的公钥证书返回给客户端。
> - Client验证公钥证书：比如是否在有效期内，证书的用途是不是匹配Client请求的站点，是不是在CRL吊销列表里面，它的上一级证书是否有效，这是一个递归的过程，直到验证到根证书（操作系统内置的Root证书或者Client内置的Root证书），如果验证通过则继续，不通过则显示警告信息。
> - Client使用**伪随机数生成器生成加密所使用的对称密钥**，然后用证书的公钥加密这个对称密钥，发给Server。
> - Server使用自己的私钥解密这个消息，得到对称密钥。至此，Client和Server双方都持有了相同的对称密钥。
> - Server使用对称密钥加密明文内容A，发送给Client。
> - Client使用对称密钥解密响应的密文，得到明文内容A。
> - Client再次发起HTTPS的请求，使用对称密钥加密请求的明文内容B，然后Server使用对称密钥解密密文，得到明文内容B。

问题：客户端怎么知道服务器发送过来的公钥是可信的？

> 这就要借助**数字证书认证机构CA**，将服务器的公钥放在**数字证书**里，只要证书是可信的，公钥就是可信的

数字签名：

> 将一段文本先用Hash函数生成消息摘要，然后用发送者的私钥加密生成数字签名，与原文一起传送给接收者
>
> 接收者只有用发送者的公钥才能解密被加密的摘要信息，然后用HASH函数对收到的原文产生一个摘要信息，对比这两个摘要信息
>
> 如果相同，则说明收到的信息是完整的，在传输过程中没有被修改，否则说明信息被修改过，因此数字签名能够验证信息的完整性
>
> 这里就可以把公钥当成文本，来发送，保证公钥没有被修改

为什么要使用对称加密？

> HTTPS 采用**非对称加密进行密钥交换**，之后使用**对称加密进行数据传输**，这样既保证安全性，又提高了效率。
>
> 这种方式被称为 **"混合加密"**
>
> 对称加密效率高，对服务器的开销少，而且加密密钥已经经过非对称加密来交换，安全性得以保证
>
> **大整数幂模运算**，如 c= m^e mod n



##### GET和POST的区别 :star:

> 1.请求参数
>
> ​	GET 请求的请求参数是添加到 head 中，可以在 url 中看到
>
> ​	POST 请求的请求参数是添加到body中，在 url 中不可见
>
> 2.数据包
>
> ​	GET请求产生一个数据包（只有请求头，没有请求体）
>
> ​	POST请求产生两个数据包（先发送请求头，再发送请求体）
>
> ​		客户端第一个请求中包含了`Expect:Continue`
>
> ​		服务端返回`100 Continue`
>
> ​		客户端才继续发送请求体
>
> ​	减少不必要的开销
>
> 3.幂等性：重复执行得到的结果是相同的
>
> ​	GET幂等，POST不是幂等的
>
> 4.用途：
>
> ​	GET一般用于获取数据
>
> ​	POST一般用于修改数据、提交数据



##### Cookie和Session的区别 :star:

> Cookie存储在客户端，Session存储在服务端
>
> 
>
> 对于Cookie，浏览器每次请求会自动携带 Cookie 发送到服务器，
>
> 对于Session，服务器维护 Session，客户端只存 Session ID
>
> 
>
> 对于Cookie，服务器无存储压力，适合存放简单信息
>
> 对于Session，由服务端存储，会占用内存
>
> 
>
> 对于Cookie，存在客户端，容易被窃取、篡改
>
> 对于Session，存在服务器，更安全

###### Session的认证过程：

> 1. 客户端第一次发送请求到服务端，服务端根据信息创建对应的 Session，并在响应头返回 SessionID
> 2. 客户端接收到服务端返回的 **SessionID** 后，会将此信息**存储在 Cookie 上**，同时会记录这个 SessionID 属于哪个域名
> 3. 当客户端再次访问服务端时，请求会自动判断该域名下是否存在 Cookie 信息，如果有则发送给服务端，服务端会从 Cookie 中拿到 SessionID，再根据 SessionID 找到对应的 Session，如果有对应的 Session 则通过，继续执行请求，否则就中断





##### TCP vs. UDP

都是应用层的协议

|                        | TCP                                   | UDP                     |
| ---------------------- | ------------------------------------- | ----------------------- |
| 是否面向连接           | 是                                    | 否                      |
| 是否可靠               | 是（有流量控制和拥塞控制）            | 否（没有）              |
| 是否有状态             | 是                                    | 否                      |
| 传输效率               | 较慢                                  | 较快                    |
| 传输形式               | 面向字节流                            | 面向数据报文段          |
| 是否拆包粘包           | 是                                    | 否                      |
| 首部开销               | 20 bytes                              | 8 bytes                 |
| 是否提供广播或多播服务 | 否                                    | 是                      |
| 基于此协议的应用层协议 | HTTP1.x/2.0、HTTPS、FTP、TELNET、SMTP | DNS、DHCP、HTTP3.0、RIP |

选择：

> **UDP 一般用于即时通信**，比如：语音、 视频、直播等等。这些场景对传输数据的准确性要求不是特别高，比如你看视频即使少个一两帧，实际给人的感觉区别也不大。
>
> **TCP 用于对传输准确性要求特别高的场景**，比如文件传输、发送和接收邮件、远程登录等等



##### TCP三次握手和四次挥手 :star:

###### 建立连接 —— 三次握手 1.5RTT

SYN和ACK是TCP报文段中头部的两个标志位

<img src="面经.assets/tcp-shakes-hands-three-times.png" alt="TCP 三次握手图解" style="zoom:67%;" />

**一次握手**: 客户端发送带有 SYN（seq=x） 标志的数据包 -> 服务端，然后客户端进入 **SYN_SEND** 状态，等待服务端的确认

> seq=x的SYN报文是用来建立客户端to服务端之间的连接

**二次握手**: 服务端发送带有 SYN+ACK(seq=y,ack=x+1) 标志的数据包 –> 客户端,然后服务端进入 **SYN_RECV** 状态

> ack=x+1是对客户端seq=x的SYN数据包的确认，此时客户端到服务端的单向连接已建立
>
> seq=y的SYN是用来建立服务端to客户端之间的连接

**三次握手**: 客户端发送带有 ACK(ack=y+1) 标志的数据包 –> 服务端，然后客户端和服务端都进入**ESTABLISHED** 状态，完成 TCP 三次握手

> ack=y+1是对seq=y的SYN数据包的确认，此时服务端到客户端的单向连接也建立
>
> 至此客户端和服务端的双向连接建立

**问题1**：为什么要三次连接？

> 三次握手的目的是让客户端和服务端双方都确认自己和对方的发送、接受是否都正常
>
> 第一次握手：<u>客户端什么都无法确认</u>（不知道自己发送是否正常），但服务端知道客户端发送正常，自己接受正常
>
> 第二次握手：客户端（收到ACK后）知道自己发送正常，接受正常，对方发送和接受正常【Done】；
>
> ​						服务端仍然只知道对方发送正常，自己接受正常，但不知道自己发送和对方接受是否正常
>
> 第三次握手：服务端（收到ACK后）知道自己发送正常，对方接受正常【Done】
>
> 经过三次握手，双方都知道了自己和对方的发送、接受都正常
>

**问题2**：第 2 次握手传回了 ACK，为什么还要传回 SYN？

> ①服务端传回发送端所发送的 ACK 是为了告诉客户端：“我接收到的信息确实就是你所发送的信号了”，这表明从客户端到服务端的通信是正常的
>
> ②回传 SYN 则是为了建立从服务端到客户端的通信。
>

**问题3**：三次握手过程中可以携带数据吗？

> 一旦完成前两次握手，第三次握手就可以携带数据了。因为此时客户端已经知道服务端的发送和接受功能都正常
>



###### 断开连接 —— 四次挥手

<img src="https://oss.javaguide.cn/github/javaguide/cs-basics/network/tcp-waves-four-times.png" alt="TCP 四次挥手图解" style="zoom: 67%;" />

**第一次挥手**：客户端发送一个 FIN（SEQ=x） 标志的数据包->服务端，用来<u>关闭客户端到服务端的连接</u>

然后客户端进入 **FIN-WAIT-1** 状态

**第二次挥手**：服务端收到这个 FIN（SEQ=X） 标志的数据包，它发送一个 ACK （ACK=x+1）标志的数据包->客户端 

然后服务端进入 **CLOSE-WAIT** 状态，客户端进入 **FIN-WAIT-2** 状态

> 此时客户端到服务端的连接已经关闭

**第三次挥手**：服务端发送一个 FIN (SEQ=y)标志的数据包->客户端，用来<u>关闭服务端到客户端的连接</u>

然后服务端进入 **LAST-ACK** 状态

**第四次挥手**：客户端发送 ACK (ACK=y+1)标志的数据包->服务端，然后客户端进入**TIME-WAIT**状态

服务端在收到 ACK (ACK=y+1)标志的数据包后进入 **CLOSE** 状态

BUT：此时如果客户端等待 **2MSL** 后依然没有收到回复，就证明服务端已正常关闭，随后客户端也可以关闭连接了。

前两次挥手和后两次挥手其实是**对称的**



**问题1**：为什么第二次挥手的ACK和第三次挥手的FIN不能合并起来，像建立连接一样三次握手？

> 因为服务端收到客户端断开连接的请求时，可能<u>还有一些数据没有发完</u>，这时先回复 ACK，表示接收到了断开连接的请求。等到数据发完之后再发 FIN，断开服务端到客户端的数据传送
>

**问题2**：为什么第四次挥手客户端需要**等待 2*MSL**（报文段最长寿命）时间后才进入 CLOSED 状态？

> 第四次挥手时，客户端发送给服务端的 ACK 有可能丢失，如果服务端因为某些原因而没有收到 ACK 的话，服务端就会重发 FIN，如果客户端在 2*MSL 的时间内收到了 FIN，就会重新发送 ACK 并再次等待 2MSL，防止 Server 没有收到 ACK 而不断重发 FIN
>
> MSL：一个片段在网络中最大的存活时间
>

**问题3**：MSL固定吗？

>  `TIME_WAIT` 状态中所等待的 2MSL 时间在现代操作系统中 **是可以配置的，并且不同系统默认值可能不同**



##### TCP如何保证传输的可靠性

> **基于数据块传输**：应用数据被分割成 TCP 认为最适合发送的数据块，再传输给网络层，数据块被称为**报文段**（Segment）
>
> **对失序数据包重新排序以及去重**：TCP 为了保证不发生丢包，就给每个包一个序列号`Seq`，有了序列号能够将接收到的数据根据序列号排序，并且去掉重复序列号的数据就可以实现数据包去重。
>
> **校验和** : TCP 将保持它首部和数据的检验和
>
> **失败重传机制** : 在数据包丢失或延迟的情况下，重新发送数据包，直到收到对方的确认应答（ACK）
>
> **流量控制** : TCP 利用**滑动窗口**实现流量控制，双方都有固定大小的**缓冲空间**，TCP 的接收端只允许发送端发送接收端缓冲区能接纳的数据。当接收方来不及处理发送方的数据，能提示发送方降低发送的速率，防止包丢失。TCP 使用的流量控制协议是可变大小的滑动窗口协议
>
> **拥塞控制** : 当网络拥塞时，减少数据的发送。TCP 在发送数据的时候，需要考虑两个因素：一是接收方的接收能力，二是网络的拥塞程度。接收方的接收能力由**滑动窗口**表示，表示接收方还有多少缓冲区可以用来接收数据。网络的拥塞程度由**拥塞窗口**表示，它是发送方根据网络状况自己维护的一个值，表示发送方认为可以在网络中传输的数据量。发送方发送数据的大小是滑动窗口和拥塞窗口的最小值，这样可以保证发送方既不会超过接收方的接收能力，也不会造成网络的过度拥塞。
>
> * **慢启动**：发送窗口从1开始，每次加倍：1->2->4->8->16 直到大于`ssthresh`，进入拥塞控制
> * **拥塞避免**：每次窗口只加1，一直到发生网络拥塞，此时将发送窗口**重置为1**，重新开始慢启动，并把ssthresh设置为发送拥塞时窗口大小的一半
>
> ![img](https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/cs/wangluo-314eba07-1388-4e8b-9307-8dd8d03b0dfe.png)
>
> * **快重传**：接收方在收到一个失序的报文段后就**立即发出重复确认**，发送方只要一连收到**三个相同的ACK确认**（ACK为失序的报文段）就应当立即重传对方尚未收到的报文段，而不必继续等待设置的**重传计时器**时间到期
>   * 3个重复的ACK：防止有些包只是顺序错了，但其实没丢，导致的预判
>   * 3是一个经验权衡值
>   * **SACK**：选择性重传。在快速重传的基础上，接收方返回最近收到报文段的**序列号范围**，这样发送方就知道接收方哪些数据包是没收到的。这样就很清楚应该重传哪些数据包
>   * **D-SACK**（Duplicate SACK），接收方用来告诉发送方，有哪些数据包重复接受了
> * **快恢复**：当发送方连续收到三个重复确认时，就把**ssthresh减半**，并不进行慢开始，而是将发送窗口大小设置为ssthresh，并进行拥塞避免
>
> ![img](https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/cs/wangluo-3a424e43-405f-494d-b700-093781b63035.png)



##### TCP拆包和粘包 :star:

> **原因**：TCP是面向**字节流**的，并不会对数据包**进行边界处理**。TCP把上层应用层发送的数据当成**字节流**，然后拆成大小不固定的数据包进行发送（拆包），TCP接受方接收到多个数据包后，再合并为字节流的形式传输给应用层（粘包）
>
> * 发送方的发送缓冲区空间不足或者接收方的接受缓冲区空间不足
> * MTU=1500字节，超过MTU的数据包也需要进行拆包
> * TCP<u>流量控制和拥塞控制</u>也会调整发送数据包的大小
>
> **解决策略**：
>
> 首先这只能由
>
> 来自行处理数据的边界
>
> * 固定长度协议：规定每个消息的固定长度，不足的用占位符填充
> * 添加特殊的分隔符来表示这是一条完整的消息
> * 在消息头中指出消息的长度
>
> **UDP是面向报文的，对于应用层交下来的报文不拆包不粘包**



##### TCP如何避免流量控制引发的死锁

> **死锁产生原因**：
>
> 当发送者收到了一个**窗口为0的应答**，发送者便停止发送，等待接收者的下一个应答。
>
> 等到接收者的接受缓冲区空闲了，就向发送者发送一个**窗口不为0的应答**，但是如果这个窗口不为0的应答在传输过程**丢失**，发送者一直等待下去，而接收者以为发送者已经收到该应答，等待接收新数据，这样双方就相互等待，从而产生死锁。
>
> **解决**：使用了**持续计时器**
>
> 每当发送者收到一个零窗口的应答后就启动该计时器。时间一到便主动发送报文询问接收者的窗口大小。若接收者仍然返回零窗口，则重置该计时器继续等待；若窗口不为0，则表示应答报文丢失了，此时重置发送窗口后开始发送，这样就避免了死锁的产生



##### ARQ协议：自动重传协议

- 停止等待ARQ协议
- 连续ARQ协议 => 回退N帧
- 



##### ARP地址解析协议的工作原理

每个主机都会有自己的ARP缓存区中建立一个ARP列表，以表示IP地址和MAC地址之间的对应关系

> 当源主机要发送数据时，首先检测ARP列表中是否对应IP地址的目的主机的MAC地址，如果有，则直接发送数据，如果没有，就**向本网段的所有主机**发送ARP数据包
>
> 当本网络的所有主机收到该ARP数据包时，首先检查数据包中的IP地址是否是自己的IP地址，如果不是，则忽略该数据包，如果是，则首先**从数据包中取出源主机的IP和MAC地址写入到自己ARP列表中**，如果存在，则覆盖然后将自己的MAC地址写入ARP响应包中，**告诉源主机自己是它想要找的MAC地址**
>
> 源主机收到ARP响应包后，**将目的主机的IP和MAC地址写入ARP列表**，并利用此信息发送数据，如果源主机一直没有收到ARP响应数据包，表示ARP查询失败

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/74bd6aa0b20bde880a4540a832dc5e6d.png)



##### 网络攻击 DDOS（分布式拒绝服务）

详情：https://javaguide.cn/cs-basics/network/network-attack-means.html

###### SYN Flood

**SYN Flood** 是互联网上最原始、最经典的 DDoS。常见的是TCP SYN Flood

SYN Flood 利用了 TCP 协议的三次握手机制，攻击者通常利用工具或者控制僵尸主机向服务器**发送海量的变源 IP 地址或变源端口的 TCP SYN 报文**，服务器响应了这些报文后就会**生成大量的半连接**，当系统资源被耗尽后，服务器将无法提供正常的服务

> **TCP**是「**双工**」连接，同时支持双向通信，也就是双方同时可向对方发送消息，其中 **SYN** 和 **SYN-ACK** 消息开启了 A→B 的单向通信通道（B 获知了 A 的消息序号）；**SYN-ACK** 和 **ACK** 消息开启了 B→A 单向通信通道（A 获知了 B 的消息序号）
>
> 假设 B 通过某 **TCP** 端口提供服务，B 在收到 A 的 **SYN** 消息时，积极的反馈了 **SYN-ACK** 消息，使连接进入**半开状态**，因为 B 不确定自己发给 A 的 **SYN-ACK** 消息或 A 反馈的 ACK 消息是否会丢在半路，所以会给每个待完成的半开连接都设一个**Timer**，如果超过时间还没有收到 A 的 **ACK** 消息，则重新发送一次 **SYN-ACK** 消息给 A，直到重试超过一定次数时才会放弃

具体攻击方式：

> 1.直接攻击：不伪造IP地址，而是直接阻止个人机器对服务器的 SYN-ACK 数据包做出响应（设置防火墙）
>
> 2.欺骗攻击：伪造不同的IP地址
>
> 3.分布式攻击DDos：使用僵尸网络

如何缓解：

> 1.回收最先创建的TCP半连接
>
> 2.增加操作系统允许的最大半开连接数目



###### UDP Flood

**UDP Flood** 也是一种拒绝服务攻击，将大量的用户数据报协议（**UDP**）数据包发送到目标服务器，目的是压倒该设备的处理和响应能力。防火墙保护目标服务器也可能因 **UDP** 泛滥而耗尽，从而导致对合法流量的拒绝服务

<img src="https://oss.javaguide.cn/p3-juejin/23dbbc8243a84ed181e088e38bffb37a~tplv-k3u1fbpfcp-zoom-1.png" alt="img" style="zoom:50%;" />

原理：



> 在正常情况下，当服务器在特定端口接收到 **UDP** 数据包时，会经过两个步骤：
>
> - 服务器首先检查是否正在运行正在侦听指定端口的请求的程序
> - 如果没有程序在该端口接收数据包，则服务器使用 **ICMP**（ping）数据包进行响应，以**通知发送方目的地不可达**
>
> 由于目标服务器利用资源检查并响应每个接收到的 **UDP** 数据包的结果，当接收到大量 **UDP** 数据包时，目标的资源可能会迅速耗尽，导致对正常流量的拒绝服务

缓解：

> 限制了 **ICMP** 报文的响应速率，以中断需要 ICMP 响应的 **DDoS** 攻击



###### HTTP Flood

旨在利用 HTTP 请求使目标服务器不堪重负。目标因请求而达到饱和，且无法响应正常流量后，将出现拒绝服务，拒绝来自实际用户的其他请求



###### DNS Flood

DNS Flood 攻击是一种分布式拒绝服务（DDoS）攻击，攻击者用大量流量淹没某个域的 DNS 服务器，以尝试中断该域的 DNS 解析



###### TCP重置攻击

通过向通信方发送伪造的**重置报文段**，欺骗通信双方提前关闭 TCP 连接



###### 中间人攻击 MITM（Man-In-The-MiddleAttack)

指攻击者与通讯的两端分别创建独立的联系，并交换其所收到的数据，使通讯的两端认为他们正在通过一个私密的连接与对方 直接对话，但事实上整个会话都被攻击者完全控制



##### 网络安全加密算法

###### 摘要/散列算法

`MD5`：可以用来生成一个 128 位的消息摘要，它是目前应用比较普遍的散列算法

​	MD5 是将任意长度的文章转化为一个 128 位的散列值，可是在 2004 年，**MD5** 被证实了容易发生**碰撞**，即两篇原文产生相同的摘要

`SHA`：安全散列算法

​	接收一段明文数据，通过不可逆的方式将其转换为固定长度的密文

​	SHA-1 SHA-2 SHA-256

`SM3`：国密算法

###### 对称加密（加密方与解密方使用同一秘钥）

`DES`：密钥表面上是 64 位的，其中56位是有效的，剩余8位用来奇偶校验

`AES`：DES的优化，使用更长的密钥：128 位、192 位和 256位

###### 非对称加密 （公钥私钥）

公钥加密的数据只能由私钥来解密，私钥加密的数据只能由公钥来解密

`ECC`：基于椭圆曲线提出。是目前加密强度最高的非对称加密算法

`SM2`：同样基于椭圆曲线问题设计。最大优势就是国家认可和大力支持

###### 数字证书

发送方先对**文本**(合同等)进行SHA生成一段**摘要**，再使用自己的**私钥**对**摘要**进行加密

接着将<u>公钥</u>、<u>加密后的摘要</u>和<u>原文本</u>（合同）一起发送给接收方

接收方收到后，使用公钥对加密后的摘要进行解密，得到摘要1。接着再使用SHA对原文本生成一段摘要2

如果摘要2和摘要1不一致，那么说明文本被篡改；否则，原文本是正确的

<img src="https://oss.javaguide.cn/p3-juejin/e4b7d6fca78b45c8840c12411b717f2f~tplv-k3u1fbpfcp-zoom-1.png" alt="img" style="zoom:50%;" />









------

## OS :white_check_mark:

操作系统四大板块都要掌握

#### 进程管理

##### 什么是进程和线程？

> **进程（Process）** 是指进操作系统中**资源分配的基本单位**，它是一个正在运行的**程序**实例以及其状态
>
> **线程（Thread）** 是 **CPU 调度的基本单位**，它是进程中的一个执行流，一个进程可以包含多个线程，共享进程的资源
>
> 一个进程中可以有多个线程，多个线程共享进程的堆和方法区资源，但是每个线程有自己的**程序计数器**、**虚拟机栈**和**本地方法栈**
>
> - 用户线程：由用户空间程序管理和调度的线程，运行在用户空间（专门给应用程序使用）
> - 内核线程：由操作系统内核管理和调度的线程，运行在内核空间（只有内核程序可以访问）

##### 什么是协程？

> **协程**是一种比线程更轻量级的并发执行单元，**它是用户态的线程，调度由程序自身控制，而不是操作系统内核**
>
> 可以看成: **可暂停、可恢复** 的函数，可以在某个地方停下来，等到需要时再从那停的地方继续执行
>
> 协程可以在一个线程内**主动让出执行权**，避免线程切换的开销，提高并发效率
>
>
> **协程 vs 线程**：
>
> - 线程是由操作系统管理的，而协程是由**程序自身管理**的
> - 线程切换由操作系统决定，协程切换由用户代码控制
> - 一个线程可以运行多个协程，但协程不能脱离线程存在

###### 有栈协程和无栈协程

> 有栈协程为每个协程分配独立的调用栈，允许在任意嵌套层级挂起和恢复，开销大
>
> 无栈协程不拥有独立的调用栈，而是依赖于一个共享的调用栈，JavaScript 的 async/await通常就是无栈协程的实现形式





##### 进程状态图

###### 三态图

<img src="面经.assets/image-20210110195613170.png" alt="image-20210110195613170" style="zoom:50%;" />

###### 五态图

<img src="面经.assets/image-20210110195324360.png" alt="image-20210110195324360" style="zoom:50%;" />

###### 七态图

<img src="面经.assets/image-20210111211024653.png" alt="image-20210111211024653" style="zoom:50%;" />



##### 进程控制块PCB

> **PCB（Process Control Block）** 即进程控制块，是操作系统中用来管理和跟踪进程的数据结构，每个进程都对应着一个独立的 PCB，每个进程在创建时OS都会为其分配一个唯一的PID，并创建一个进程控制块
>
> PCB 主要包含下面几部分的内容：
>
> - 进程的描述信息，包括进程的**名称**、**标识符**等等
> - 进程的调度信息，包括进程阻塞原因、进程**状态**（就绪、运行、阻塞等）、进程**优先级**（标识进程的重要程度）等等
> - 进程对资源的需求情况，包括 CPU 时间、内存空间、I/O 设备等等
> - 进程**打开的文件信息**，包括文件描述符、文件类型、打开模式等等
> - 处理机的状态信息（由处理机的各种寄存器中的内容组成的），包括**通用寄存器**、指令计数器、程序状态字 PSW、用户栈指针
>



##### 进程的通讯方式 :star:

> 1. **管道/匿名管道(Pipes)** ：用于具有亲缘关系的<u>父</u><u>子进程间或者兄弟进程</u>之间的通信，<u>存在**内存**中</u>
>
> 2. **有名管道(Named Pipes)** : 匿名管道由于没有名字，只能用于亲缘关系的进程间通信。为了克服这个缺点，提出了有名管道。有名管道严格遵循 先进先出(First In First Out) 。<u>有名管道以**磁盘**文件的方式存在</u>，可以实现本机任意两个进程通信
>
> 3. **信号(Signal)** ：信号是一种比较复杂的通信方式，用于<u>通知</u>接收进程某个事件已经发生
>
> 4. **消息队列(Message Queuing)** ：消息队列是消息的链表,具有特定的格式,存放在内存中并由消息队列标识符标识。
>
>    管道和消息队列的通信数据都是先进先出的原则
>
>    与管道（无名管道：只存在于<u>内存</u>中的文件；命名管道：存在于实际的<u>磁盘</u>介质或者文件系统）不同的是消息队列<u>存放在**内核**中</u>，只有在内核重启(即，操作系统重启)或者显式地删除一个消息队列时，该消息队列才会被真正的删除。消息队列可以实现消息的随机查询,消息不一定要以先进先出的次序读取,也可以按消息的类型读取.比 FIFO 更有优势。消息队列克服了信号承载信息量少，管道只能承载无格式字 节流以及缓冲区大小受限等缺点。
>
> 5. **信号量(Semaphores)** ：信号量是一个**计数器**，用于多进程对共享数据的访问，信号量的意图在于进程间同步。这种通信方式主要用于解决与同步相关的问题并避免竞争条件。**PV操作**、Semaphore类
>
> 6. **共享内存(Shared memory)** ：使得多个进程可以访问同一块内存空间，不同进程可以及时看到对方进程中对共享内存中数据的更新。这种方式需要依靠某种同步操作，如互斥锁和信号量等。可以说这是最有用的进程间通信方式。
>
> 7. **套接字(Sockets)**/网络 : 此方法主要用于在<u>客户端和服务器之间</u>通过网络进行通信。套接字是支持 TCP/IP 的网络通信的基本操作单元，可以看做是不同主机之间的进程进行双向通信的端点，简单的说就是通信的两方的一种约定，用套接字中的相关函数来完成通信过程



##### 进程的调度算法

![常见进程调度算法](https://oss.javaguide.cn/github/javaguide/cs-basics/network/scheduling-algorithms-of-process.png)





##### 僵尸进程 和 孤儿进程 :star:

子进程一般通过父进程`fork()`系统调用来创建，父子进程之间独立运行

当子进程调用exit()系统调用后，子进程结束，内核会释放该进程的所有资源，但是不包括该子进程的PCB，这些信息只有在父进程调用 `wait()` 或 `waitpid()` 系统调用时才会被释放，以便让父进程得到子进程的状态信息。

这样可以防止僵尸进程

> - **僵尸进程**：僵尸进程是已完成且处于终止状态，但在进程表中却仍然存在的进程
>
>   子进程已经终止，但是其父进程仍在运行，且父进程 **没有调用 `wait()` 或 `waitpid()`** 等系统调用来获取子进程的状态信息，释放子进程占用的资源，导致子进程的 **PCB 依然存在**于系统中，但无法被进一步使用
>
>   这种情况下，子进程被称为“僵尸进程”
>
>   避免僵尸进程的产生，父进程需要及时调用 wait() 或 waitpid() 系统调用来回收子进程。（已死但肉体还在）
>
> - **孤儿进程**：一个进程的父进程已经终止或者不存在，但是该进程仍在运行。这种情况下，该进程就是孤儿进程。孤儿进程通常是由于父进程意外终止或未及时调用 wait() 或 waitpid() 等系统调用来回收子进程导致的。为了避免孤儿进程占用系统资源，操作系统 **会将孤儿进程的父进程设置为 init 进程**（进程号为 1），由 init 进程来回收孤儿进程的资源

###### 僵尸进程如何处理？

`Top` 命令查找，`zombie` 值表示僵尸进程的数量，再手动清除



##### 死锁产生的四个必要条件 :star:

只要破坏其中一个，就可以避免产生死锁

> **1. 互斥条件**：进程对某些资源的访问是**互斥的**，即一个资源在某个时间点**只能被一个进程占有**，其他进程必须等待
>
> **2. 请求与保持条件**：进程**已经持有至少一个资源**，但仍然在**请求额外资源**，且不会释放已持有的资源
>
> **3. 不可剥夺条件**：资源**不能被强行抢占**，只能由占有它的进程主动释放。
>
> **4. 循环等待条件**：存在一组进程 {P1, P2, ..., Pn}，每个进程都持有下一个进程需要的资源，形成**环形等待链**

解决死锁的四种方法：**预防、避免、检测、解除**

> 预防：破坏四个必要条件中任意一个就可以预防，一般考虑第二和第四个
>
> * 静态分配资源——破坏“请求与保持条件”：一个进程在执行前就获得所有需要的资源
> * 层次分配策略——破坏“循环等待”：按照层次分配资源
>
> 避免：系统在分配资源时，根据资源的使用情况**提前做出预测**
>
> * 银行家算法
>
> 检测：检测系统中是否存在死锁，并解除
>
> * 进程-资源分配图
>
> 解除：
>
> * 立即结束**所有进程**的执行，重新启动操作系统
> * 撤销**涉及死锁的所有进程**，解除死锁后继续运行
> * **逐个撤销**涉及死锁的进程，回收其资源直至死锁解除
> * 抢占资源

###### Java怎么检测死锁？

> `jstack` ：输出中会有 `Found one Java-level deadlock:`的字样，后面会跟着死锁相关的线程信息、
>
> `jconsole`：进入后选择应用程序，然后点击检查死锁



##### 什么时候用户态会进入内核态？

系统调用、中断、异常、陷入、信号量

| 场景                    | 举例                                                         |
| ----------------------- | ------------------------------------------------------------ |
| **系统调用（syscall）** | 例如：`read()`, `write()`, `open()`, `socket()`, `send()` 等 |
| **中断处理**            | 比如磁盘读完、网卡收到数据、定时器到了（硬件中断）           |
| **缺页异常**            | 页不在物理内存里，触发缺页中断，OS 处理后恢复用户态          |
| **陷入（trap）**        | 比如除以 0、访问非法内存，系统捕获并处理                     |
| **信号处理**            | 比如进程收到 `SIGTERM`，系统切到内核态分发信号               |



##### fork() vs. clone()

> `fork()` 用于创建一个新的 **子进程**，子进程会**复制**父进程的地址空间，但数据是 **独立** 的
>
> `clone()` 是 Linux 为创建**线程**设计的，是`fork`的升级版本，不仅可以创建进程或者线程，还可以指定创建新的命名空间（namespace）、有选择的继承父进程的内存、甚至可以将创建出来的进程变成父进程的兄弟进程



##### 读写锁

使用信号量[semaphore]和PV操作实现三种读写策略：读者优先、写者优先、读写公平

###### 读者优先

> 需要两个信号量：
>
> * mutex_rcnt = 1 保护读者计数器 readCount
> * mutex= 1 保护临界资源（文件等），读者和写者要互斥访问
>
> 读线程
>
> ```c
> int readCount = 0; // 正在读的读者数量
> Semaphore mutex = 1;
> Semaphore wrt = 1;
> 
> void reader() {
>     // 先更新readerCount+1
>     P(mutex_rcnt );
>     readCount++;
>     if (readCount == 1)
>         P(mutex);     // 第一个读者锁住写者
>     V(mutex_rcnt);
> 
>     read(); // 读
> 
>     // readerCount-1
>     P(mutex_rcnt);
>     readCount--;
>     if (readCount == 0)
>         V(mutex);     // 最后一个读者释放写锁
>     V(mutex_rcnt );
> }
> ```
>
> 写者
>
> ```c
> void writer() {
>     P(mutex); 
> 	write();
>     V(mutex);
> }
> ```
>
> 问题：写者可能饥饿，尽管写者先到，后面来的读者都可以直接进入临界资源

###### 写者优先

> 一旦有写者等待，**后来的读者不能再进入**
>
> 信号量：
>
> * mutex_rcnt = 1：保护readCount
> * mutex_wcnt = 1：保护writeCount
> * mutex：保护读写者只有一方进入临界资源（读写锁）
> * has_writer = 1：当有写者在等待时，锁住后来的读者（写者优先）
> * mutex_writer = 1：保护所有写者只有一个写者进入临界资源（独占写锁）
>
> 读者：
>
> ```c
> void reader() {
>     // 先判断是否有写者在等待
>     P(has_writer); // 如果当前有写者在等待，那后来的读者都需要再次等待
>     // 如果当前没有写者在等待，可以进入
>     
>     // 更新readCount
>     P(mutex_rcnt);
>     readCount++;
>     if (readCount == 1)
>         P(mutex);   // 第一个读者需要对临界资源加锁（等待写者）
>     V(mutex_rcnt);
>     
>     read();
>     
>     // 更新readCount
>     P(mutex_rcnt);
>     readCount--;
>     if (readCount == 0)
>         V(mutex);   // 没有读者了释放临界资源，让写者进入
>     V(mutex_rcnt);
> }
> ```
>
> 写者：
>
> ```c
> void writer() {
>     // 更新writeCount
>     P(mutex_wcnt);
>     writeCount++;
>     if(writeCount == 1) 
>         P(mutex); // 第一个写者需要对临界资源加锁（等待读者释放）
>     V(mutex_wcnt);
>     
>     // 写者内部排队（独占写锁）
>     P(mutex_writer);
>     wirte();
>     V(mutex_writer);
>     
>     // 更新writeCount
>     P(mutex_wcnt);
>     writeCount--;
>     if(writeCount == 0) 
>         V(mutex);  // 没有写者了才让读者进入
>     	V(has_wirter); // 并通知之后到达的读者可以进入临界资源
>     V(mutex_writter);
>     
> }
> ```
>
> 

###### 读写公平

> 直接使用一个信号链queue来让所有线程排队
>
> 读者
>
> ```c
> void reader() {
>     P(queue); // 读写公平锁
>     
>     P(mutex_rcnt);
>     readCount++;
>     if(readCount == 1) 
>         P(mutex);
>     V(mutex_rcnt);
>     
>     V(queue);
>     
>     read();
>     
>     P(mutex_rcnt);
>     readCount--;
>     if(readCount == 0) 
>         V(mutex);
>     V(mutex_rcnt);
> }
> ```
>
> 写者
>
> ```c
> void writer() {
>     P(queue); // 读写公平锁
>     
>     P(mutex);
>     write();
>     V(mutex);
>     
>     V(queue);
> }
> ```
>
> 





#### 内存管理

##### 虚拟内存和物理内存

> **物理内存**：计算机中实际存在的硬件内存 
>
> ​	=> 对应物理地址
>
> **虚拟内存**：一种内存管理技术，它使得应用程序认为自己有连续的、独立的内存空间，而实际上，这个虚拟内存是物理内存上加上**磁盘**上的**交换区**（交换区就是用不到的页会临时交换到磁盘的交换区上）
>
>  ​    => 对应逻辑地址

##### 分页与分段、页表与段表

每个进程都有自己的虚拟地址空间，虚拟内存使用的是逻辑地址

> **分页**：将虚拟内存划分为大小固定的页，将物理内存页划分为大小固定的页（页大小相等，页内偏移量才相等）
>
> 由于物理内存不够，操作系统会将不常用的页暂时存储到磁盘的**交换区**，所以==虚拟内存=物理内存+磁盘交换区==
>
> 操作系统通过 **页表** 将虚拟地址映射到物理地址：	
>
> ​	**虚拟地址** = ==虚拟页号== + 页内偏移量
>
> ​	通过**页表**，将虚拟页号映射为物理页号
>
> ​					  ==物理页号== + 页内偏移量 = **物理地址**
>
> <img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/os-4cdd5179-4b88-4aa6-b9c2-9ef8fdc745dc.png" alt="内存分页" style="zoom:50%;" />
>
> 缺点：容易产生**内部碎片**

> **分段**：程序是由若⼲个逻辑分段组成的，段大小由程序指定
>
> 操作系统通过**段表**将虚拟地址映射到物理地址：
>
> ​	虚拟地址 = 段号 + 段内偏移
>
> ​	通过段表，将虚拟段号映射为物理段号
>
> ​	物理段号 + 段内偏移 = 物理地址
>
> <img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/os-075df152-7b77-40c7-abdb-1aa0280d958b.png" alt="虚拟地址、段表、物理地址" style="zoom:50%;" />
>
> 缺点：容易产生内存外部碎片

##### 缺页中断

> 缺页中断（Page Fault）：当一个程序访问的页**不在物理内存中**时，就会发生**缺页中断**。
>
> 操作系统需要从磁盘上的**交换区**中将缺失的页调入内存
>
> ###### 分页替换算法
>
> <img src="面经.assets/os-6effefb6-67d2-4155-a3fc-4b27a319391a.png" alt="三分恶面渣逆袭：常见页面置换算法" style="zoom: 80%;" />



##### 快表TLB

`局部性原理`： 即在⼀段时间内，整个程序的执⾏仅限于程序中的某⼀部分。相应地，执⾏所访问的存储空间也局限于某个内存区域

> 把**最常访问的⼏个⻚表项**存储到访问速度更快的硬件中，就是**TLB**
>
> 查询页表之前，先查询快表，如果hit，直接可以得出物理地址，如果miss，再去查页表
>
> <img src="https://oss.javaguide.cn/github/javaguide/cs-basics/operating-system/page-table-tlb.png" alt="使用 TLB 之后的地址翻译流程" style="zoom:67%;" />



##### 内部碎片 & 外部碎片

###### 内部碎片的解决方式：slab

> 按照预定固定的大小，将分配的内存分割成特定长度的块，以完全解决内存碎片问题
>
> slab 分配器将分配的内存**分割成各种尺寸的块**，并把相同尺寸的块**分成组**
>
> <img src="面经.assets/017255e89c34906755966550976ba13b.png" alt="img" style="zoom:50%;" />



###### 外部碎片的解决方式：伙伴算法

> - 每个内存块都有自己的伙伴块，两者可以合并也可以拆分
>
> - **伙伴关系**： 两个内存块，大小相同，地址连续，同属于一个大块区域
>
> - 256KB的内存要划分一个21KB的内存块
>
>   - 首先，内核将256KB的内存进行分割，变成两个128KB的内存块，**AL和AR**，这两个内存块称为伙伴
>
>   - 因为128KB远大于21KB，于是继续分割为两个64KB的内存块，再继续划分为两个32KB的内存块
>
>   - 最后将CL(32KB)分配出去
>
>     <img src="面经.assets/0cd0b830bcb61935ce705f6236447d7d.png" alt="img" style="zoom:50%;" />
>
>   - 当这块内存归还时，如果它的伙伴CR空闲，那么可以合并为BL
>
>   - 同理，如果BR空闲，可以继续递归的合并
>
> - 伙伴算法虽然能完全解决外部碎片，但是是**以产生内存碎片为代价**的





#### 文件管理

##### 软链接与硬链接

> 硬链接：在目录下创建一个条目，记录着文件名与 inode 编号，这个 inode 就是**源文件的 inode**
>
> ​			删除任意一个条目，文件还是存在，只要引用数量不为 0
>
> ​			不能跨越文件系统（因为每个文件系统都维护了自己的一个inode表，可能会冲突）
>
> ​			不能对目录创建硬链接，只能对文件创建
>
> ​			`ln`命令用于创建硬链接
>
> <img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/os-d3f778f9-506b-4b93-9fb7-40eb0a79874e.png" alt="硬链接-来源参考[3]" style="zoom: 67%;" />
>
> 软链接：重新创建⼀个⽂件，这个⽂件有**独⽴的** **inode**，但是这个**⽂件的内容是另外⼀个⽂件的路径**
>
> ​			快捷方式，可以跨越文件系统
>
> ​			`ln -s` 用于创建软连接
>
> <img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/os-81abf13c-5c60-4263-8fcb-c79c33d865e8.png" alt="软链接-来源参考[3]" style="zoom:67%;" />



##### Linux文件系统

https://segmentfault.com/a/1190000023615225



### IO管理

##### 什么是零拷贝？

> 假如需要文件传输，使用传统 I/O，数据读取和写入是用户空间到内核空间来回赋值，而内核空间的数据是通过操作系统的 I/O 接口从磁盘读取或者写入，这期间发生了多次用户态和内核态的上下文切换，以及**多次数据拷贝**
>
> <img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/os-1e595664-6585-4d56-8939-08b7ce510218.png" alt="传统文件传输示意图-来源参考[3]" style="zoom: 67%;" />
>
> 为了提升 I/O 性能，就需要**减少用户态与内核态的上下文切换**和**内存拷贝的次数**。
>
> 这就用到了我们零拷贝的技术，零拷贝技术实现主要有两种：
>
> ①mmap+write
>
> `mmap()` 系统调⽤函数会直接把**内核缓冲区**⾥的数据「**映射**」到⽤户空间，这样，操作系统内核与⽤户空间就**不需要再进⾏任何的数据拷⻉操作**
>
> <img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/os-6dc49f9d-0bc3-4956-a650-7c7236f234a2.png" alt="mmap示意图-来源参考[3]" style="zoom:67%;" />
>
> ②sendfile
>
> `sendfile()`可以替代前⾯的 `read()` 和 `write()` 这两个系统调⽤，这样就可以减少⼀次系统调⽤，即减少了 2 次上下⽂切换的开销
>
> 可以直接把**内核缓冲区**⾥的数据拷⻉到 **socket 缓冲区** ⾥，不再拷⻉到⽤户态
>
> <img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/os-0b087b8a-8d51-4aad-898d-d99c38d36592.png" alt="sendfile示意图-来源参考[3]" style="zoom:67%;" />

 Kafka、RocketMQ 都采用了零拷贝技术来提升 IO 效率

下图展示了各种零拷贝技术的对比图：

|                            | CPU 拷贝 | DMA 拷贝 | 系统调用   | 上下文切换 |
| -------------------------- | -------- | -------- | ---------- | ---------- |
| 传统方法                   | 2        | 2        | read+write | 4          |
| mmap+write                 | 1        | 2        | mmap+write | 4          |
| sendfile                   | 1        | 2        | sendfile   | 2          |
| sendfile + DMA gather copy | 0        | 2        | sendfile   | 2          |



##### 同步IO、异步IO

> 同步IO：应该程序发起 `read` 调⽤时，内核将数据从内核空间拷⻉到应⽤程序空间，这个过程应用程序一直再等待
>
> <img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/os-86e54fa3-ad36-43c7-9d2d-5a68139c310f.png" alt="基于非阻塞的I/O多路复用" style="zoom:67%;" />
>
> 异步IO：应用程序发起 `aio_read` 之后，就⽴即返回，内核⾃动将数据**从内核空间拷⻉到应⽤程序空间**，这个拷⻉过程同样是异步的，内核拷贝完成后，通知应用程序处理数据
>
> <img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/os-869021ed-5e4e-4490-9174-7291d8ddf55c.png" alt="异步/IO" style="zoom:67%;" />



##### 阻塞IO、非阻塞IO

同步IO分为阻塞IO、非阻塞IO

> **阻塞** **I/O** ：当⽤户程序执⾏ `read`，线程会被阻塞，⼀直等到**内核数据准备好**，并把**数据从内核缓冲区拷⻉到应⽤程序的缓冲区**中，当拷⻉过程完成， `read` 才会返回
>
> ​	阻塞等待的是`内核数据准备好`和`数据从内核态拷⻉到⽤户态`这两个过程
>
> <img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/os-f06db5ff-661c-4ddf-9115-4ed9c9a21d01.png" alt="阻塞I/O" style="zoom:67%;" />
>
> **非阻塞I/O**： `read` 请求在数据未准备好的情况下⽴即返回，可以继续往下执⾏，此时应⽤程序**不断轮询内核**，直到数据准备好，内核将数据拷⻉到应⽤程序缓冲区， read 调⽤才可以获取到结果
>
> <img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/os-771e014e-7ed9-4101-8bb5-4413b8069fd6.png" alt="非阻塞I/O" style="zoom:67%;" />





------

## Java 基础

#### 常识、基本数据类型、变量、方法

##### JDK vs JRE

> JDK：Java Development Kit，Java开发工具包，包含了一些javac、jar、java等命令   是一种SDK
>
> JRE：Java Runtime Environment，Java运行时环境，只包含运行 Java 程序所需的环境和类库（JVM和Java基础类库）
>
> 关系：
>
> <img src="https://oss.javaguide.cn/github/javaguide/java/basis/jdk-include-jre.png" alt="jdk-include-jre" style="zoom:67%;" />

##### JIT（Just In Time，Java即时编译）

字节码(.class)是面向JVM的，JVM可理解的代码就是字节码（所以Java程序是可移植的）

<img src="https://oss.javaguide.cn/github/javaguide/java/basis/java-code-to-machine-code.png" alt="Java程序转变为机器代码的过程" style="zoom:67%;" />

原因：Java代码的运行是JVM加载字节码文件，再由解释器一行一行的解析运行的，所以效率比较低

> 对于**热点代码**（经常需要被调用的方法或代码块），通过引进 **JIT（Just in Time Compilation）** 编译器：
>
> ​	当 JIT 编译器完成第一次编译后，其会**将字节码对应的机器码**保存下来，下次可以直接使用
>
> <img src="https://oss.javaguide.cn/github/javaguide/java/basis/java-code-to-machine-code-with-jit.png" alt="Java程序转变为机器代码的过程" style="zoom:67%;" />
>
> 对于**解释执行次数**达到阈值的方法，会触发编译，编译成机器码  ==> 所以说JIT是**动态编译**
>
> <img src="https://p0.meituan.net/travelcube/ba83857ecf9f344e4972fd551c4973d653952.png@648w_454h_80q" alt="img" style="zoom: 80%;" />

==> 所以说 **Java 是编译与解释共存的语言**，由javac将java代码编译成字节码，由解释器负责解析字节码，热点代码由JIT负责编译成机器码

##### AOT（Ahead of Time）

> JDK 9 引入了一种新的编译模式 **AOT**
>
> 与 JIT 不同的是，这种编译模式会在程序被执行前就将其编译成**机器码**，属于**静态编译**
>
> **优点**：避免了 JIT 预热等各方面的开销，可以提高 Java 程序的启动速度，避免预热时间长
>
> ​		  能减少内存占用和增强 Java 程序的安全性 （AOT编译后的代码相比字节码 不容易被反编译）
>
> ​		 启动时间短、内存占用小、打包体积小
>
> ​		 更适合云原生
>
> **缺点**：AOT 编译无法支持 Java 的一些动态特性，如反射、动态代理、动态加载





##### 为什么说Java语言是编译与解释共存的语言？

> **编译型**：通过**编译器**将源代码一次性翻译成可被该平台执行的**机器码**。
>
> 一般情况下，编译语言的执行速度比较快，开发效率比较低。常见的编译性语言有 C、C++、Go、Rust 等等
>
> **解释型**：通过**解释器**一句一句的将代码解释（interpret）为机器代码后再执行
>
> 解释型语言开发效率比较快，执行速度比较慢。常见的解释性语言有 Python、JavaScript、PHP 等等
>
> **而Java**：
>
> Java 程序要经过先编译，后解释两个步骤：
>
> ​	①Java 程序需要先经过编译步骤，生成字节码（`.class` 文件）
>
> ​	②这种字节码必须由 Java 解释器来解释执行
>
> ​	③对于热点代码，由JIT进行编译成机器码



##### JDK、JRE、JVM、JIT

![JDK、JRE、JVM、JIT 这四者的关系](https://oss.javaguide.cn/github/javaguide/java/basis/jdk-jre-jvm-jit.png)



##### 移位运算符

> 如果移动的**位数大于32**，那么对位数**会取模**，即 << 48 相当于 << (48 % 32) 即<< 16
>
> `<<` :左移运算符，向左移若干位，高位丢弃，低位补零。`x << n`,相当于 x 乘以 2 的 n 次方(不溢出的情况下)。
>
> `>>` :带符号右移，向右移若干位，高位补符号位，低位丢弃。正数高位补 0,负数高位补 1。`x >> n`,相当于 x 除以 2 的 n 次方
>
> `>>>` :无符号右移，忽略符号位，空位都以 0 补齐
>
> 移位运算符**只支持int和long**，其他类型（short、char、byte）都会由编译器转化为int



##### Java八种基本数据类型占字节数

这 8 种基本数据类型的默认值以及所占空间的大小如下：

| 基本类型  | 位数 | 字节            | 默认值  | 取值范围                                                     |
| :-------- | :--- | :-------------- | :------ | ------------------------------------------------------------ |
| `byte`    | 8    | 1               | 0       | -128 ~ 127                                                   |
| `short`   | 16   | 2               | 0       | -32768（-2^15） ~ 32767（2^15 - 1）                          |
| `int`     | 32   | 4               | 0       | -2147483648 ~ 2147483647                                     |
| `long`    | 64   | 8               | 0L      | -9223372036854775808（-2^63） ~ 9223372036854775807（2^63 -1） |
| `char`    | 16   | ==2==           | 'u0000' | 0 ~ 65535（2^16 - 1）                                        |
| `float`   | 32   | 4               | 0f      | 1.4E-45 ~ 3.4028235E38                                       |
| `double`  | 64   | 8               | 0d      | 4.9E-324 ~ 1.7976931348623157E308                            |
| `boolean` | 1    | 取决于JVM的实现 | false   | true、false                                                  |

特别注意：java的`char`类型占2个字节，不同于c++中的char只占1个字节

**引用类型：8字节** 如果开启压缩是4字节（Hotspot默认开启压缩）

##### 基本数据类型 vs 包装类

**包装类** 是为基本数据类型提供的对象封装，使基本类型可以作为对象使用，可以用于泛型，并提供了一定的缓存机制

|              | 基本数据类型（int、long...)                              | 包装类（Integer、Long）                |
| ------------ | -------------------------------------------------------- | -------------------------------------- |
| **用途**     | 全局、局部变量/常量                                      | 除此之外，可用于**泛型**               |
| **存储方式** | 局部变量：虚拟机栈<br />成员变量：堆（因为对象存在堆中） | 因为是对象，所以==一般==存活在**堆**中 |
| **占用空间** | 小                                                       | 大                                     |
| **默认值**   | 有默认值                                                 | null                                   |
| **比较方式** | ==                                                       | equals()                               |

###### 包装类的缓存机制

> `Byte`,`Short`,`Integer`,`Long` 这 4 种包装类默认创建了数值 **[-128，127]** 的相应类型的缓存数据
>
> `Character` 创建了数值在 **[0,127]** 范围的缓存数据
>
> `Boolean` 直接返回 `True` or `False`
>
>  `Float`,`Double` 并没有实现缓存机制
>
> ```java
> public static Integer valueOf(int i) {
>        // 如果i处于缓存方位，直接从缓存池中获取并返回
>        if (i >= IntegerCache.low && i <= IntegerCache.high)
>            return IntegerCache.cache[i + (-IntegerCache.low)];
>        return new Integer(i);
> }
> ```

###### 包装类的自动装箱和拆箱

> **装箱**：将基本类型用它们对应的引用类型包装起来：`Integer i = 10` ==> `Integer i = Integer.valueOf(10);`
>
> **拆箱**：将包装类型转换为基本数据类型：`int n = i`  ==> `int n = i.intValue();`
>
> **如果频繁拆装箱的话，也会严重影响系统的性能**



##### 为什么静态方法只允许访问该类的静态成员变量和其他静态方法？

因为静态方法是在**类初始化**的时候分配内存的，而非静态成员变量是实例创建的时候才有的



##### 重写 vs 重载

> **重载（Overload）**就是方法名相同的方法，参数不同（包括参数个数、类型、顺序），返回值和访问修饰符可以相同可以不同
>
> ​	**编译器**必须挑选出具体执行哪个方法，它通过用各个方法给出的参数类型与该方法调用所使用的值类型进行匹配来挑选出相应的方法。 如果编译器找不到匹配的参数， 就会产生编译时错误
>
> **重写（Override）**就是子类中重新定义父类的某个方法
>
> ​	**方法签名**必须相同，包括方法名、参数个数、类型、顺序
>
> ​	子类重写的方法的**返回类型**必须是父类该方法方法返回类型的**相同类型或者子类型**
>
> ​	子类重写的方法的**访问权限**不得低于父类的该方法
>
> ​	子类方法**抛出的异常**可以是父类异常的子类，但不得是其他类
>
> 重载发生在编译期，是**静态**的，编译期间就要确定调用哪个方法
>
> 重写发生在运行期，是**动态**的（根据具体的对象类型来调用该类型的方法）
>
> <u>构造方法可以重载（参数不同），不可以重写（子类没法继承父类的构造方法，只能通过super()调用）</u>



##### 变长参数

> 只能放在方法参数列表的**最后一个**
>
> 形式：method(<其他参数>, **String... args**)
>
> 变长参数经过编译器编译成一个数组
>
> 对于重载方法的匹配优先级：先匹配固定参数的方法，如果没有匹配的，再尝试匹配变长参数



#### 面向对象 OOP

##### 面向对象三大特点：封装、继承、多态

> **封装**是将数据职责和行为职责结合在一起，对外隐藏实现细节，只提供接口调用
>
> **继承**是通过现有的类（父类）来创建新类（子类），子类继承父类的属性和方法，通过重写来增强实现
>
> **多态**是指同一操作作用于不同的对象时，表现出不同的行为。子类可以提供父类方法的不同实现，并通过**父类引用**指向不同的子类对象，表现出不同的行为
>
> ​	引用类型变量发出的方法调用到底是哪个类中的方法，必须在**程序运行期间**才能确定

##### 对象实例 vs 对象引用

> **对象实例**指的是通过**类**创建出来的具体**对象**。每次通过 `new` 关键字创建一个对象时，都会产生一个新的对象实例
>
> **对象引用**则是一个变量，保存了对某个对象实例的**引用**或**地址**
>
> 对象实例存放在**堆**中，对象引用存放在**方法栈**中
>
> 一个对象实例可以有多个对象引用来指向它，一个对象引用最多可以指向一个对象实例
>
> **对象实例的相等**一般比较的是内存中<u>存放的内容是否相等</u>
>
> **对象引用的相等**一般比较的是他们指向的<u>内存地址是否相等</u>，是否指向同一个对象实例

##### 接口 vs 抽象类

接口可以看成是完全抽象类

> ①接口中的方法默认是`abstract`，不能提供具体实现，需要实现该接口的类来实现（不过JDK8之后接口可以写`default`方法）
>
> ​	抽象类中的非抽象方法可以提供默认实现，抽象方法`abstract`则不能提供具体实现且必须由子类实现
>
> ②接口的成员变量只能是`static final`静态成员常量，抽象类的成员变量没有这个限制
>
> ③接口的方法访问权限只能是`public`（不过JDK9后提供了`private`），抽象类的方法没有这个限制
>
> ④类只能继承**一个**抽象类，但可以实现**多个**接口



#### Object：所有类的父类

##### 其中包含的11个方法

```java
/**
 * native 方法，用于返回当前运行时对象的 Class 对象，使用了 final 关键字修饰，故不允许子类重写。
 */
public final native Class<?> getClass()
/**
 * native 方法，用于返回对象的哈希码，主要使用在哈希表中，比如 JDK 中的HashMap。
 */
public native int hashCode()
/**
 * 用于比较 2 个对象的内存地址是否相等，String 类对该方法进行了重写以用于比较字符串的值是否相等。
 */
public boolean equals(Object obj)
/**
 * native 方法，用于创建并返回当前对象的一份拷贝 ==> 浅拷贝，深拷贝得重写
 */
protected native Object clone() throws CloneNotSupportedException
/**
 * 返回类的名字实例的哈希码的 16 进制的字符串。建议 Object 所有的子类都重写这个方法。
 */
public String toString()
/**
 * native 方法，并且不能重写。唤醒一个在此对象监视器上等待的线程(监视器相当于就是锁的概念)。如果有多个线程在等待只会任意唤醒一个。
 */
public final native void notify()
/**
 * native 方法，并且不能重写。跟 notify 一样，唯一的区别就是会唤醒在此对象监视器上等待的所有线程，而不是一个线程。
 */
public final native void notifyAll()
/**
 * native方法，并且不能重写。暂停线程的执行。注意：sleep 方法没有释放锁，而 wait 方法释放了锁 ，timeout 是等待时间。
 */
public final native void wait(long timeout) throws InterruptedException
/**
 * 多了 nanos 参数，这个参数表示额外时间（以纳秒为单位，范围是 0-999999）。 所以超时的时间还需要加上 nanos 纳秒。。
 */
public final void wait(long timeout, int nanos) throws InterruptedException
/**
 * 跟之前的2个wait方法一样，只不过该方法一直等待，没有超时时间这个概念
 */
public final void wait() throws InterruptedException
/**
 * 实例被垃圾回收器回收的时候触发的操作
 */
protected void finalize() throws Throwable { }
```

###### 注意点：

1.`clone()`是**浅拷贝**，需要自己重写实现深拷贝

2.`toString()`如果不重写，默认输出

 `getClass().getName() + "@" + Integer.toHexString(hashCode())` 建议重写

3.引用对象的`==`是比较两个引用对象指向的**地址**，`equals()`才是比较内容

4.`equals()`方法默认是采用`==`进行比较，需要自己重写来实现内容的比较（除了`String`类的equals就重写了）

5.重写`equals()`方法时必须重写`hashcode()`方法：因为两个equals的对象的hashcode必须相等！

6.`hashcode()`和`equals()`的区别：hashcode相等的两对象不一定时同一个对象， equals为true的两个对象hashcode一定相等



#### String类 :star:

##### String vs. StringBuilder vs. StringBuffer :star:

| 特性           | String                                                       | StringBuilder                  | StringBuffer                 |
| -------------- | ------------------------------------------------------------ | ------------------------------ | ---------------------------- |
| **可变性**     | ❌ 不可变，每次修改都会创建新对象<br />旧对象会变成垃圾等待 GC | ✅ 可变，不会创建新对象         | ✅ 可变，不会创建新对象       |
| **线程安全性** | ✅ 线程安全(因为不可变)                                       | ❌ 线程不安全                   | ✅ 线程安全（同步）           |
| **性能**       | 较慢（因创建新对象）                                         | **最快**（无同步开销）         | 较慢（有同步开销）           |
| **适用场景**   | 字符串内容不变的情况（常量）                                 | 单线程环境，需要频繁修改字符串 | 多线程环境，需频繁修改字符串 |

##### 为什么String对象是不可变的？ :star:

> ①String类是final，不可被继承  （但是不是原因）
>
> ②String类中的成员变量char[]是final （这才是原因）
>
> ③这个char[]还是private的，并且没有提供setter接口

##### 为什么Java9后String的成员变量变成了byte[]数组?

Java8之前，String的成员变量为char[]数组，每个char占两个字节，所以很浪费空间

Java9：改为byte[]数组，并增加了一个**coder**字段

```java
public final class String implements java.io.Serializable, Comparable<String>, CharSequence {
    private final byte[] value;  // 用 byte[] 代替 char[]
    private final byte coder;    // 记录编码类型（LATIN1 或 UTF16）
}
```

> **`byte[]` 存储字符串数据**，相比 `char[]` **节省一半空间**（对于 ASCII 字符），但对于中文并没有区别
>
> **`coder` 记录编码类型**：
>
> - `coder == 0`：LATIN-1（ISO-8859-1，即 **单字节编码**，每个字符占 1 字节）
> - `coder == 1`：UTF-16（每个字符占 **2 字节**）
>
> **只用 `byte[]`，不会额外存 `char[]`**，比 Java 8 **节省 50% 内存（在 ASCII 场景下）**

##### 字符串拼接运算符+

Java中仅有的两个重载运算符`+`和`+=`，专门为 String 类重载

> 字符串对象通过`+`的字符串拼接方式，实际上是通过 `StringBuilder` 调用 `append()` 方法实现的，拼接完成之后调用 `toString()` 得到一个 `String` 对象
>
> 存在的问题：在**循环内**使用`+`进行字符串的拼接的话，由于编译器不会创建单个 `StringBuilder` 以复用，会导致创建过多的 `StringBuilder` 对象
>
> 在 JDK 9 中，字符串相加“+”改为用动态方法 `makeConcatWithConstants()` 来实现，通过提前分配空间从而减少了部分临时对象的创建

##### 字符串常量池

> **字符串常量池** 是 JVM 为了提升性能和减少内存消耗针对字符串（String 类）专门开辟的一块区域，主要目的是为了避免字符串的重复创建
>
> ```java
> // 在字符串常量池中创建字符串对象 ”ab“
> // 将字符串对象 ”ab“ 的引用赋值给 s1
> String s1 = "ab";
> // 直接返回字符串常量池中字符串对象 ”ab“，赋值给引用 bb
> String s2 = "ab";
> String s3 = new String("ab");
> ```
>
> 上面这串代码中，s1 == s2 但 != s3 
>
> s1和s2两个对象引用都是直接指向字符串常量池中的"ab"
>
> 因为**s3的创建过程**是这样的，JVM先判断字符串常量池中是否含有"ab"，如果没有则创建，有则直接取出
>
> 接着在堆中创建一个String对象，并使用常量池中的"ab"对其进行初始化，所以**s3并非指向常量池的中"ab"**

###### String.intern()方法

> 强制**返回字符串常量池中的引用**，如果常量池中没有则先创建
>
> ```java
> // s1 指向字符串常量池中的 "Java" 对象
> String s1 = "Java";
> // s2 也指向字符串常量池中的 "Java" 对象，和 s1 是同一个对象
> String s2 = s1.intern();
> // 在堆中创建一个新的 "Java" 对象，s3 指向它
> String s3 = new String("Java");
> // s4 指向字符串常量池中的 "Java" 对象，和 s1 是同一个对象
> String s4 = s3.intern();
> ```
>
> s1、s2、s4都指向了常量池中的"Java"字符串
>
> s3则指向堆中的一个对象

###### 常量折叠

> 编译器在编译的过程中，会对可以计算出结果的常量表达式，使用最终常量替换在源代码中
>
> 对于 `String str3 = "str" + "ing";` 编译器会给你优化成 `String str3 = "string";`
>
> 只对<u>**final修饰过的**基本数值类型变量和字符串**变量**</u>或者<u>基本数据类型常量和字符串**常量**</u>有效



#### 异常

##### 继承结构

所有异常的父类：Throwable

异常分为两种：Execption（异常）和Error（错误）

Exception分为两种：Checked 和 UnChecked

<img src="https://oss.javaguide.cn/github/javaguide/java/basis/types-of-exceptions-in-java.png" alt="Java 异常类层次结构图" style="zoom:67%;" />

##### Exception vs. Error

> **`Exception`** :程序本身可以处理的异常，通常是代码问题，通过try-catch块来处理
>
> **`Error`**： 属于JVM层面引发的错误，程序无法处理，比如`OutOfMemoryError`、类加载错误

##### Checked Exception vs. Unchecked Exception

区别在于是否能在**编译之前发现**

> Checked Exception 即**受检查异常**，Java 代码在编译过程中，如果受检查异常没有被 `catch`或者`throws` 关键字处理的话，就没办法通过编译。
>
> ​	例如在使用IO流时，没有try-catch一些IOException则无法通过编译，一般IDE也会提示
>
> Unchecked Exception 即**不受检查异常** ，Java 代码在编译过程中 ，我们即使不处理不受检查异常也**可以正常通过编译**
>
> ​	例如数组越界异常、类映射异常，这些都是在**运行时**才能发现的，所以`RuuntimeException`及其子类就都是Unchecked异常
>
> 

##### try-catch-finally

> `try`块：用于捕获异常，后可以跟多个`catch`块，如果没有`catch`，那么必须有`finally`块
>
> `catch`块：用于处理 try 捕获到的异常
>
> `finally` 块：无论是否捕获或处理异常，`finally` 块里的语句都会被执行
>
> 如果try或finally块中有return语句，那么finally块会在return之前先执行
>
> **不要在 finally 语句块中使用 return**：
>
> ​	当 try 语句和 finally 语句中都有 return 语句时，<u>try 语句块中的 return 语句会被忽略</u>。
>
> ​	这是因为 try 语句中的 return 返回值会先被暂存在一个本地变量中，当执行到 finally 语句中的 return 之后，这个本地变量的值就变为了 finally 语句中的 return 返回值
>
> ```java
> try {
>     a = 10;
>     return a; // a = 10 先被暂存为返回值
> } finally {
>     a = 20;
>     return a; // finally 的 return 覆盖了 try 的 return
> }
> // 最后return 20
> ```
>
> ​	如果try中有return，finally中没有，那么在执行try中的return前，会先执行finally中的语句，不过finally中就算改了return的变量值也不会影响try中的return，因为这个return值先被保存起来了
>
> ```java
> try {
>     a = 10;
>     return a; // return值先被保存起来了
> } finally {
>     a = 20; // 就算改了a也没问题
>     // 没有 return，不影响原返回值
> }
> ```
>
> 

###### try-with-resources代替try-catch-finally

不需要手动在finally块中关闭资源

```java
try (Scanner scanner = new Scanner(new File("test.txt"))) {
    while (scanner.hasNext()) {
        System.out.println(scanner.nextLine());
    }
} catch (FileNotFoundException fnfe) {
    fnfe.printStackTrace();
}
```



#### 泛型

泛型类、泛型接口、泛型方法

```java
// 泛型类
public class Generic<T>{

    private T key;

    public Generic(T key) {
        this.key = key;
    }

    public T getKey(){
        return key;
    }
}

// 泛型接口
public interface Generator<T> {
    public T method();
}

// 泛型方法
public static < E > void printArray( E[] inputArray )
{
    for ( E element : inputArray ){
        System.out.printf( "%s ", element );
    }
    System.out.println();
}
```

##### 泛型擦除

> Java的泛型是**伪泛型**
>
> Java在编译期间会把所有的泛型信息擦掉：
>
> ​	如果是泛型T，会擦除为Object
>
> ​	如果是T extends A，会擦除为A
>
> **原因**：为了保证引入泛型机制但不创建新的类型，减少虚拟机的运行开销
>
> **既然会泛型擦除，那为什么不直接使用Object代替？**
>
> ​	使用泛型可以在**编译期间**进行类型检测
>
> ​	使用Object类型需要**手动**进行强制类型转换
>
> ​	使用泛型可以**限定类型**，例如`T extends Comparable`

##### 桥方法

> **桥方法** 是 Java 编译器为泛型方法或泛型类的子类**自动生成**的一个辅助方法，目的是<u>继承泛型类时能保持多态</u>，保证子类方法仍然能正确覆写父类方法
>
> Java 泛型是基于`类型擦除` 实现的，在运行时泛型类型会被擦除，但Java需要**保证重写泛型方法**时仍然符合**多态**规则。由于泛型擦除后，方法签名可能不同，Java 编译器就会自动生成桥方法来适配方法调用
>
> ```java
> class Parent<T> {
>     public T getValue(T value) {
>         return value;
>     }
> }
> // 类继承了泛型类
> class Child extends Parent<String> {
>     @Override
>     public String getValue(String value) {
>         return value;
>     }
> }
> ```
>
> **编译之后**：因为类型擦除，Parent类的getValue()方法的参数为Object，而Child类的getValue()方法的参数则为String，这样就破坏了多态（方法签名不同）
>
> 所以编译器就给Child生成了一个**桥方法**
>
> ```java
> class Parent {
>     public Object getValue(Object value) {  // 泛型擦除后的方法
>         return value;
>     }
> }
> 
> class Child extends Parent {
>     public String getValue(String value) {  // 子类方法
>         return value;
>     }
> 
>     // Java 编译器自动生成的桥方法
>     public Object getValue(Object value) {
>         return getValue((String) value);
>     }
> }
> ```
>
> 



##### 通配符`？`

```java
<T>			   ==>    <?>
<T extends A>  ==>    <? extends A>
<T super A>    ==>    <? super A>
```

###### 无界通配符<?>

> 可以接受任何类型
>
> 例如：List<?>表示持有某种特定类型的List，但是不知道是哪种类型
>
> 所以**只能读，不能写**
>
> 即只能调用list.get()，返回Object对象
>
> 如果调用list.add("aaa")就会报错

###### 上界通配符<? extends A>

> 可以接受A类以及其子类
>
> **只能读，不能写**
>
> get的时候返回A类型的对象
>
> set的时候报错

###### 下届通配符<? super A>

> 可以接受A类以及其超类
>
> **只能写不能读**
>
> get()的时候不知道返回什么类型，所以报错
>
> set()的时候只接受A类型的对象

| **通配符**      | **适用场景**                    | **能否读取？**                | **能否写入？** |
| --------------- | ------------------------------- | ----------------------------- | -------------- |
| `<?>`           | 适用于**只读**的泛型集合        | 可以读取，但只能作为 `Object` | ❌              |
| `<? extends T>` | 适用于**读取为 `T` 类型**的方法 | 可以读取为`T`                 | ❌              |
| `<? super T>`   | 适用于**写入 `T` 类型**的方法   | 只能读取为 `Object`           | ✅ 可以写入 `T` |

##### 如何在运行是获取List<T>中的T到底是什么类型？

> ①通过list中元素的getClass()方法
>
> ```java
> List<String> list = new ArrayList<>();
> list.add("12");
> System.out.println(list.get(0).getClass());
> ```
>
> 问题：如果List中没有元素呢？
>
> ②通过`ParameterizedType`来获取T的具体类型
>
> ```java
> public interface ParameterizedType extends Type {
>  Type[] getActualTypeArguments();  // 获取泛型参数，如 T、K、V 等
>  Type getRawType();  // 获取原始类型，如 List<T> ==> List
>  Type getOwnerType(); // 获取外部类类型（如果是内部类）
> }
> ```
>
> 具体的获取方法：
>
> ​	通过反射获取该list的Field对象
>
> ​	getGenericType(）可以获取这个对象的泛型类型：List<java.lang.String>
>
> ​	只有泛型对象才会返回ParameterizedType，然后获取里面的actualType
>
> ```java
> // 获取字段 myList
> Type genericFieldType = Example.class.getDeclaredField("myList").getGenericType();
> System.out.print(genericFieldType);  // ==> java.util.List<java.lang.String>
> 
> // 检查是否是参数化类型：如果该对象是普通类，那么不会进入
> if (genericFieldType instanceof ParameterizedType) {
>  ParameterizedType parameterizedType = (ParameterizedType) genericFieldType;
> 
>  // 获取泛型类型参数
>  Type actualType = parameterizedType.getActualTypeArguments()[0];
>  System.out.println("List<T> 中的 T 是：" + actualType.getTypeName());
> }
> ```
>
> 



#### 反射机制

##### 反射是什么？

> **反射（Reflection）** 是 Java 语言的**动态特性**，允许在**运行时**获取类的信息（字段、方法、构造函数等），**并操作**这些成员
>
> **优点**：可以让代码更加灵活、为各种框架提供开箱即用的功能提供了便利
>
> **缺点**：让我们在运行时有了分析操作类的能力，这同样也增加了安全问题（有些类型在编译时检查不出来），同时性能差

##### 反射的核心类

Java 反射主要涉及 **`java.lang.Class`** 和 **`java.lang.reflect`** 包下的类：

| **类**           | **作用**                                         |
| ---------------- | ------------------------------------------------ |
| `Class<?>`       | 代表 Java 类，提供获取类信息的方法               |
| `Constructor<T>` | 代表类的构造方法                                 |
| `Field`          | 代表类的字段（成员变量）                         |
| `Method`         | 代表类的方法                                     |
| `Modifier`       | 提供方法用于判断修饰符（`public`、`private` 等） |

###### 1.获取 Class 对象的四种方式

> ① 如果知道具体的类是谁
>
> ​	Class alunbarClass = TargetObject**.class**;
>
> ② 如果知道类的全路径
>
> ​	Class alunbarClass1 = **Class.forName**("cn.javaguide.TargetObject");
>
> ③ 通过该类**对象实例**来获取
>
> ​	TargetObject **o** = new TargetObject(); 
>
> ​	Class alunbarClass2 = **o.getClass()**;
>
> ④ 通过类加载器`xxxClassLoader.loadClass()`传入类的全路径路径获取
>
>   ClassLoader.getSystemClassLoader().**loadClass**("cn.javaguide.TargetObject");
>
> 
>
> :star:反射的 **`Class.forName()`** 和 **`ClassLoader.loadClass()`** 的区别？
>
> ​	`Class.forName()`会同时完成类的**加载** 和 **初始化**
>
> ​	`Class.forName()`底层就是调用`ClassLoader.loadClass()`来完成类的加载的，接着默认会执行类的静态初始化
>
> ​	`ClassLoader.loadClass()`只完成了类的加载（即静态变量和静态代码块不会执行）

###### 2.获取类的成员变量Field

> 先获取该类的Class对象，再获取Field对象
>
> ```java
> Class<?> clazz = Class.forName("com.example.Person");
> 
> // 获取所有字段
> Field[] fields = clazz.getDeclaredFields();
> for (Field field : fields) {
>     System.out.println("字段：" + field.getName());
> }
> 
> // 访问私有字段
> Field privateField = clazz.getDeclaredField("name");
> privateField.setAccessible(true);  // 取消私有访问限制
> privateField.set(obj, "Tom");  // 修改私有字段
> ```
>
> 

###### 3.获取类的成员方法Method

> 先获取该类的Class对象，再获取方法Method对象，通过Method.invoke来调用方法
>
> ```java
> Class<?> clazz = Class.forName("com.example.Person");
> 
> // 获取所有方法
> Method[] methods = clazz.getDeclaredMethods();
> for (Method method : methods) {
>     System.out.println("方法：" + method.getName());
> }
> 
> // 调用方法
> Method method = clazz.getMethod("sayHello"); // 获取无参数方法
> method.invoke(obj); // 调用 sayHello 方法
> ```
>
> 



##### 为什么反射慢？如何优化？

> 慢的原因：
>
> 1. 方法调用有额外开销
>
>    - 反射调用方法需要**查找方法元数据**，然后执行 `invoke()`，比普通调用慢 **10~20 倍**
> 2. 安全检查（SecurityManager机制）
>
>    - 反射访问**私有成员**需要调用 `setAccessible(true)`，这会进行安全性检查
> 3. 无法进行 JIT（即时编译）优化
>
>    - **普通方法调用**可被 JIT **内联优化**，但**反射方法调用**由于动态性，无法优化
>
> 4. 需要类型转换
>    - 反射返回 `Object` 类型，通常需要**强制类型转换**，增加性能损耗



##### Spring 是如何使用反射实现依赖注入的？

> Spring 通过反射创建对象并设置属性，主要涉及：
>
> 1. **扫描 Bean**（`@ComponentScan`）
> 2. **创建对象**（通过 `Constructor.newInstance()`）
> 3. **注入依赖**（通过 `Field.set()` 或 `Method.invoke()`）





#### Unsafe类 :star:

`Unsafe` 是位于 `sun.misc` 包下的一个类，主要提供一些用于执行低级别、不安全操作的方法，如直接访问系统内存资源、自主管理内存资源等。这些方法都是`native`本地方法，即底层是有C或C++来实现

##### 创建

`Unsafe`为单例类，其创建需要通过`Unsafe.getUnsafe()`来获得单一实例，并且该方法是`@CallSensitive`的：

​	该方法会对调用者的`classLoader`进行检查，判断当前类是否由`Bootstrap classLoader`加载，如果不是的话那么就会抛出一个`SecurityException`异常，即只有**启动类加载器加载的类**才能够调用 Unsafe 类中的方法

```java
@CallerSensitive
public static Unsafe getUnsafe() {
    Class var0 = Reflection.getCallerClass();
    // 仅在引导类加载器`BootstrapClassLoader`加载时才合法
    if(!VM.isSystemDomainLoader(var0.getClassLoader())) {
        throw new SecurityException("Unsafe");
    } else {
        return theUnsafe;
    }
}
```

​	可以通过反射来创建，也可以通过命令行将调用者类所在的jar包追加到BootStrap路径中

##### 功能

> 1. 内存操作 :star:
> 2. 内存屏障 :star:
> 3. 对象操作
> 4. 数据操作
> 5. CAS 操作 :star:
> 6. 线程调度
> 7. Class 操作
> 8. 系统信息

###### 内存操作

Unsafe提供了几个直接操控内存的方法：

```java
//分配新的本地空间
public native long allocateMemory(long bytes);
//重新调整内存空间的大小
public native long reallocateMemory(long address, long bytes);
//将内存设置为指定值
public native void setMemory(Object o, long offset, long bytes, byte value);
//内存拷贝
public native void copyMemory(Object srcBase, long srcOffset,Object destBase, long destOffset,long bytes);
//清除内存
public native void freeMemory(long address);
```

注意：这些分配的内存属于**堆外内存**，直接由操作系统管理，而不是JVM，即不会自动进行GC，需要手动释放`freeMemory()`

如果没有手动释放内存的化，可能会导致**内存泄漏**

典型应用：`DirectByteBuffer`类就是使用Unsafe的内存操作在堆外内存中开辟了一块**通信缓冲池**： Netty的ByteBuf



###### 内存屏障

内存屏障就是通过**阻止**屏障两边的**指令重排序**从而避免编译器和硬件的不正确优化情况

Unsafe提供了三个不同的内存屏障方法：

```java
//内存屏障，禁止load操作重排序。屏障前的load操作不能被重排序到屏障后，屏障后的load操作不能被重排序到屏障前
public native void loadFence();
//内存屏障，禁止store操作重排序。屏障前的store操作不能被重排序到屏障后，屏障后的store操作不能被重排序到屏障前
public native void storeFence();
//内存屏障，禁止load、store操作重排序
public native void fullFence();
```

内存屏障可以看做对内存随机访问的操作中的一个**同步点**，使得**此点之前的所有读写操作**都执行后才可以开始执行此点之后的操作

在代码需要插入屏障的地方使用`unsafe.xxxFence()`即可



###### 对象操作

Unsafe 提供了全部 **8 种基础数据类型**以及**Object**的`put`和`get`方法，并且所有的`put`方法都可以越过访问权限，**直接修改内存**中的数据

```java
//在对象的指定偏移地址获取一个对象引用
public native Object getObject(Object o, long offset);
//在对象指定偏移地址写入一个对象引用
public native void putObject(Object o, long offset, Object x);
```

还提供了**volalite**读写和有序写入的方法

```java
//在对象的指定偏移地址处读取一个int值，支持volatile load语义
public native int getIntVolatile(Object o, long offset);
//在对象指定偏移地址处写入一个int，支持volatile store语义
public native void putIntVolatile(Object o, long offset, int x);

// 有序
public native void putOrderedInt(Object o, long offset, int x);
```



###### CAS操作

Unsafe提供了各种基本数据类型和Object的CAS操作

```java
public final native boolean compareAndSwapObject(Object o, long offset,  Object expected, Object update);

public final native boolean compareAndSwapInt(Object o, long offset, int expected,int update);

public final native boolean compareAndSwapLong(Object o, long offset, long expected, long update);
```

底层实现即为 CPU 指令 `cmpxchg`，该指令是一个原子操作





###### 线程调度

Unsafe类提供了一些线程调度的方法：

```java
//取消阻塞线程
public native void unpark(Object thread);
//阻塞线程
public native void park(boolean isAbsolute, long time);
//获得对象锁（可重入锁）

// 下面三个方法已被弃用
@Deprecated
public native void monitorEnter(Object o);
//释放对象锁
@Deprecated
public native void monitorExit(Object o);
//尝试获取对象锁
@Deprecated
public native boolean tryMonitorEnter(Object o);
```

方法 `park`、`unpark` 即可实现线程的**挂起**与**恢复**

应用：`AbstractQueuedSynchronizer`，就是通过调用`LockSupport.park()`和`LockSupport.unpark()`实现线程的阻塞和唤醒的，而 `LockSupport` 的 `park`、`unpark` 方法实际是调用 `Unsafe` 的 `park`、`unpark` 方式实现的

`park`和`unpark`都是native方法，每个 Java 线程都会关联一个 **`Parker` 对象**（C++ 层面的结构体）

`Parker` 的核心结构包含三部分：

- **计数器（counter）**：一个整数，用于记录 `unpark()` 的调用次数（解决「先唤醒后阻塞」的问题）。

- **互斥锁（mutex）**：保证多线程操作 `Parker` 时的线程安全。
- **条件变量（condition variable）**：依赖操作系统的同步原语（如 Linux 的 `pthread_cond_t`、Windows 的 `Event`），用于实现线程的阻塞与唤醒。





------

## Java IO :white_check_mark:

JavaIO四个最基础的**抽象类**：

​	输入：InputStream/Reader   所有输入流的基类，前者是字节输入流，后者是字符输入流

​	输出：OutputStream/Writer   所有输出流的基类，前者是字节输出流，后者是字符输出流

两种流：字节流、字符流

进阶：字节缓冲流、字符缓冲流 



##### 字节流

###### InputStream

从源头（通常是文件）读取数据（字节信息）到内存

> `read()`：返回输入流中**下一个字节**的数据
>
> ​				 返回的值介于 0 到 255 之间。如果未读取任何字节，则代码返回 `-1` ，表示文件结束
>
> `read(byte b[])` : 从输入流中读取一些字节**存储到字节数组 b 中**
>
> ​								 如果数组 `b` 的长度为零，则不读取；如果没有可用字节读取，返回 `-1`
>
> ​								 如果有可用字节读取，则最多读取的字节数最多等于 `b.length` ， 返回读取的字节数
>
> `read(byte b[], int off, int len)`：在`read(byte b[])` 方法的基础上增加了 `off` 参数（偏移量）和 `len` 参数（要读取的最大字节数）
>
> `skip(long n)`：忽略输入流中的 n 个字节 ,返回实际忽略的字节数
>
> `available()`：返回输入流中可以读取的字节数
>
> `close()`：关闭输入流释放相关的系统资源

一般完整的读一个文件，可以使用其子类`FileInputStream`，加上while循环来读，通过其返回的-1来表示读到文件末尾

```java
try (InputStream fis = new FileInputStream("input.txt")) {
    System.out.println("Number of remaining bytes:" + fis.available());  // avaliable方法获取文件字节总数
    int content;
    long skip = fis.skip(2); // skip 可以跳过前几个字节 
 	// 通过while循环来读文件 逐个字节读出
    while ((content = fis.read()) != -1) {
        System.out.print((char) content);
    }
} catch (IOException e) {
    e.printStackTrace();
}
```

一般会使用`BufferedInputStream`来包装FileInputStream，再进行读取

```java
// 新建一个 BufferedInputStream 对象
BufferedInputStream bufferedInputStream = new BufferedInputStream(new FileInputStream("input.txt"));
// 读取文件的内容并复制到 String 对象中
String result = new String(bufferedInputStream.readAllBytes());
```

如果要指定读出的数据为**其他基本数据类型**，可以使用`DataInputStream `来包装FileInputStream，在进行读取

```java
FileInputStream fileInputStream = new FileInputStream("input.txt");
//必须将fileInputStream作为构造参数才能使用
DataInputStream dataInputStream = new DataInputStream(fileInputStream);
//可以读取任意具体的类型数据
dataInputStream.readBoolean(); 
dataInputStream.readInt();
dataInputStream.readUTF();
```

如果要读的数据类型是一个对象，可以使用`ObjectInputStream`，它会从输入流中读取 Java 对象（反序列化）

```java
ObjectInputStream input = new ObjectInputStream(new FileInputStream("object.data"));
MyClass object = (MyClass) input.readObject();
```



###### OutputStream

将数据（字节信息）写入到目的地（通常是文件）

方法和InputStream是相反的

> `write(int b)`：将特定字节写入输出流
>
> `write(byte b[])` : 将数组`b` 写入到输出流，等价于 `write(b, 0, b.length)` 
>
> `write(byte[] b, int off, int len)` : 在`write(byte b[])` 方法的基础上增加了 `off` 参数（偏移量）和 `len` 参数（要读取的最大字节数）
>
> `flush()`：**刷新**此输出流并强制写出所有缓冲输出字节
>
> `close()`：关闭输出流释放相关的系统资源

`FileOutputStream`：将字节流写入文件中，先获取文本的字节流

```java
try (FileOutputStream output = new FileOutputStream("output.txt")) {
    byte[] array = "JavaGuide".getBytes(); // 获取文本字节流
    output.write(array);
} catch (IOException e) {
    e.printStackTrace();
}
```

通常也会配合 `BufferedOutputStream`包装`FileOutputStream`来使用

```java
FileOutputStream fileOutputStream = new FileOutputStream("output.txt");
BufferedOutputStream bos = new BufferedOutputStream(fileOutputStream)
```

同样也有`DataOutputStream`来写入其他基本数据类型，用`ObjectOutputStream`来写入对象类型



##### 字符流

使用字节流容易出现乱码，I/O 流就干脆提供了一个直接操作字符的接口，方便我们平时对字符进行流操作

###### Reader

`Reader`用于从源头（通常是文件）读取数据（字符信息）到内存中

> `read()` : 从输入流读取**一个字符**  【InputStream的read是读出一个字节】
>
> `read(char[] cbuf)` : 从输入流中读取一些字符，并将它们存储到字符数组 `cbuf`中
>
> `read(char[] cbuf, int off, int len)`：在`read(char[] cbuf)` 方法的基础上增加了 `off` 参数（偏移量）和 `len` 参数（要读取的最大字符数）。
>
> `skip(long n)`：忽略输入流中的 n 个字符 ,返回实际忽略的字符数。
>
> `close()` : 关闭输入流并释放相关的系统资源

`InputStreamReader` 是字节流转换为字符流的桥梁

`FileReader` 是基于该基础上的封装，可以直接操作字符文件



###### Writer

`Writer`用于将数据（字符信息）写入到目的地（通常是文件）

> `write(int c)` : 写入单个字符
>
> `write(char[] cbuf)`：写入字符数组 `cbuf`，等价于`write(cbuf, 0, cbuf.length)`
>
> `write(char[] cbuf, int off, int len)`：在`write(char[] cbuf)` 方法的基础上增加了 `off` 参数（偏移量）和 `len` 参数（要读取的最大字符数）
>
> `write(String str)`：写入字符串，等价于 `write(str, 0, str.length())` 
>
> `write(String str, int off, int len)`：在`write(String str)` 方法的基础上增加了 `off` 参数（偏移量）和 `len` 参数（要读取的最大字符数）
>
> `append(CharSequence csq)`：将指定的字符序列附加到指定的 `Writer` 对象并返回该 `Writer` 对象
>
> `append(char c)`：将指定的字符附加到指定的 `Writer` 对象并返回该 `Writer` 对象
>
> `flush()`：刷新此输出流并强制写出所有缓冲的输出字符
>
> `close()`：关闭输出流释放相关的系统资源

`OutputStreamWriter` 是字符流转换为字节流的桥梁

子类 `FileWriter` 是基于该基础上的封装，可以直接将字符写入到文件



##### 字节缓冲流

IO 操作是很消耗性能的，缓冲流将数据加载至缓冲区，一次性读取/写入多个字节，从而避免频繁的 IO 操作，提高流的传输效率

字节缓冲流这里采用了**装饰器模式**来增强 `InputStream` 和`OutputStream`子类对象的功能，

得到`BufferedInputStream`和`BufferedOutputStream`

字节流和字节缓冲流的性能差别主要体现在我们使用两者的时候都是调用 `write(int b)` 和 `read()` 这两个一次只读取一个字节的方法的时候。由于字节缓冲流内部有缓冲区（字节数组），因此，字节缓冲流会先将读取到的字节存放在缓存区，大幅减少 IO 次数，提高读取效率



##### 字符缓冲流

`BufferedReader` 和 `BuferedWriter`



##### 打印流 PrintStream

`System.out`就是一个PrintStream对象，System.out.print()调用的是PrintStream对象的`write()`方法



##### 随机访问流 RandomAccessFile

支持随意跳转到文件的任意位置进行读写

```java
public RandomAccessFile(File file, String mode) throws FileNotFoundException {
    this(file, mode, false);
}
```

其构造方法中有一个`mode`参数，表示文件的读写模式

> `r` : 只读模式。
>
> `rw`: 读写模式
>
> `rws`: 相对于 `rw`，`rws` 同步更新对“文件的内容”或“元数据”的修改到外部存储设备
>
> `rwd` : 相对于 `rw`，`rwd` 同步更新对“文件的内容”的修改到外部存储设备

RandomAccessFile中有一个**文件指针**用来表示**下一个**将要被写入或者读取的字节所处的**位置**

可以通过`seek(long pos)`方法来设置该指针的偏移量，可以通过`getFilePointer()`获取文件指针当前位置

###### 文件断点续传 :star:

`RandomAccessFile` 比较常见的一个应用就是实现大文件的 **断点续传** 

> **大文件如何上传？**
>
> 首先，客户端先计算大文件的**哈希值（SHA-2）**，并将大文件按照一定的分片规则，分成大小相等的**分片**
>
> 客户端初始化一个分片上传任务，告诉服务端一共有多少个分片，以及原始文件的哈希值
>
> 客户端每个分片发送前，都会先计算其**哈希值**，并将这个哈希值同分片一同发送给服务端
>
> 接着，按照一定的策略发送各分片，串行并行都可以
>
> 服务器接收到每个分片后，重新计算其哈希值，并与客户端发送的哈希值比较，一致就认为该分片有效，不一致则通知客户端重新发送给分片
>
> 服务端收到所有分片后，对分片进行合并，得到大文件，并且计算其哈希值是否和客户端发送的哈希值一致

> 前端可以使用`Blob.slice()`对文件进行分片
>
> 后端可以使用`RandomAccessFile`进行分片的合并

###### 文件秒传 :star:

> 客户端上传文件时，服务端判断该文件是否上传过，可以立马返回上传成功
>
> **如何判断文件上传过？** 肯定不能通过文件名
>
> 客户端计算文件的摘要，发送给服务端请求是否存在
>
> 服务端需要维护一个文件和对应哈希值的表来快速判断是否存在
>
> **如果发现要上传的文件已经有部分分片上传到服务器了，服务器可以返回上传过的分片序号给前端，前端继续上传其他分片**



##### Java IO中涉及到的设计模式

###### 装饰器模式 :star:

BufferedInputStream就是对InputStream的装饰

BufferedOutputStream就是对OutputStream的装饰

###### 适配器模式  :star:

InputStreamReader和OutputStreamReader就是适配器，它们是字符流和字节流的桥梁

InputStreamReader就继承了Reader抽象类，又持有一个InputStream成员变量

###### 工厂模式

**NIO** 中大量用到了工厂模式

InputStream is = **Files.newInputStream**(Paths.get(generatorLogoPath))

###### 观察者模式

**NIO** 中的文件目录监听服务使用到了观察者模式

文件目录监听服务基于 `WatchService` 接口和 `Watchable` 接口

`WatchService`是观察者 `Watchable`是被观察者

`Watchable`的`register()`方法可以用于将对象注册到观察者上，并绑定事件监听的方法

`WatchService`中通过一个**守护进程**daemon Thread来**定期轮询**文件的变化



##### Java三种IO模型 :star:

###### BIO（Blocking IO，同步阻塞IO）

> 同步阻塞 IO 模型中，应用程序发起 read 调用后，会一直阻塞，直到内核把数据拷贝到用户空间
>
> <img src="https://oss.javaguide.cn/p3-juejin/6a9e704af49b4380bb686f0c96d33b81~tplv-k3u1fbpfcp-watermark.png" alt="图源：《深入拆解Tomcat & Jetty》" style="zoom: 67%;" />

###### NIO（Non-BLocking IO，同步非阻塞IO / IO多路复用）

> 同步非阻塞IO只在拷贝数据（从内核空间拷贝到用户空间）的时候阻塞，但是应用程序需要**轮询**调用`read`去查询数据是否准备就绪
>
> <img src="https://oss.javaguide.cn/p3-juejin/bb174e22dbe04bb79fe3fc126aed0c61~tplv-k3u1fbpfcp-watermark.png" alt="图源：《深入拆解Tomcat & Jetty》" style="zoom:67%;" />
>
> 轮询很耗CPU资源，所以出现了**IO多路复用**：
>
> <img src="https://oss.javaguide.cn/github/javaguide/java/io/88ff862764024c3b8567367df11df6ab~tplv-k3u1fbpfcp-watermark.png" alt="img" style="zoom:67%;" />
>
> 是一种**同时监听多个 I/O 事件**的技术，使得单个线程可以同时管理多个 I/O 连接，提高系统的并发能力
>
> 线程首先发起 `select` 调用，询问内核数据是否准备就绪，等内核把数据准备好了后通知用户线程，用户线程再发起`read`调用(将数据从内核空间拷贝到用户空间还是阻塞的)。IO多路复减少了多次无效的系统调用read
>
> 多路复用的关键：`Selector`，允许 **单个线程** 监视多个通道（Channel）上的事件，当某个通道准备好数据时，Selector 会通知应用程序来拷贝数据
>
> <img src="https://oss.javaguide.cn/github/javaguide/java/nio/channel-buffer-selector.png" alt="Buffer、Channel和Selector三者之间的关系" style="zoom: 80%;" />

###### AIO（Asynchronized IO，异步IO）

> 异步 IO 是基于事件和回调机制实现的，应用程序调用read后可以做其他事情，等待操作系统的通知

###### 三者的对比

![BIO、NIO 和 AIO 对比](https://oss.javaguide.cn/github/javaguide/java/nio/bio-aio-nio.png)



##### NIO详解 :star:

三个重要的组件：Selector、Buffer、Channel

<img src="https://oss.javaguide.cn/github/javaguide/java/nio/channel-buffer-selector.png" alt="Buffer、Channel和Selector三者之间的关系" style="zoom:67%;" />

###### Buffer 缓冲区

> NIO 读写数据都是通过缓冲区进行操作的
>
> ​	用户程序读操作的时候将 Channel 中的数据填充到 Buffer 中，从Buffer中读
>
> ​	用户程序写操作时将数据写入Buffer，再把 Buffer 中的数据写入到 Channel 中

Buffer类的**四个成员变量**：

> ```java
> public abstract class Buffer {
>     // Invariants: mark <= position <= limit <= capacity
>     private int mark = -1;
>     private int position = 0;
>     private int limit;
>     private int capacity;
> }
> ```
>
> 容量（`capacity`）：Buffer可以存储的最大数据量，Buffer创建时设置且不可改变
>
> 界限（`limit`）：Buffer 中可以读/写数据的边界
>
> ​	写模式下，`limit` 代表**最多**能写入的数据，`limit`小于等于`capacity`，一般等于 `capacity`
>
> ​	读模式下，`limit` 等于 Buffer 中实际写入的数据大小，也就是缓冲区种能读取的范围
>
> 位置（`position`）：下一个可以被读写的数据的位置（索引）
>
> ​	写模式下，每写入一个字节，position会后移一位
>
> ​	从写操作模式到读操作模式切换的时候，`position` 都会归零，这样就可以从头开始读
>
> 标记（`mark`）：Buffer允许将位置**直接定位**到该标记处，注意必须在有效范围内
>
> 关系：**0 <= mark <= position <= limit <= capacity** 

**Buffer两种模式**：读模式和写模式

> Buffer 被创建之后**默认是写模式**
>
> * `flip()`可以从写模式切换为读模式，此时limit会被设置为position，表示可读数据的范围，position会被重置为0，表示从头开始读
> * `clear()`、`compact()`可以从读模式切换为写模式，此时会清空缓冲区，position设置为0，limit设置为capacity
>
> 例如创建一个Buffer后，往其中写入一些数据，再切换为读模式
>
> ![position 、limit 和 capacity 之前的关系](https://oss.javaguide.cn/github/javaguide/java/nio/NIOBufferClassAttributes.png)

**Buffer对象的创建**：不能用new，只能用静态方法`allocate()`或`allocateDirect()`

```java
// 分配堆内存
public static ByteBuffer allocate(int capacity);
// 分配直接内存
public static ByteBuffer allocateDirect(int capacity);
```



###### Channel 通道

Channel 是一个**双向的**、可读可写的数据传输通道，NIO 通过 Channel 来实现数据的输入输出

它是一个抽象的概念，可以代表文件或socket与Buffer之间的连接

Channel与流的最大区别在于：Channel是全双工的

<img src="https://oss.javaguide.cn/github/javaguide/java/nio/channel-buffer.png" alt="Channel 和 Buffer之间的关系" style="zoom:50%;" />

Channel的子类有：FileChannel（文件访问通道）、SocketChannel（TCP连接通道）、DatagramChannel（UDP连接通道）



###### Selector 选择器

允许一个线程处理多个 Channel，是基于**事件驱动**的 I/O 多路复用模型

**工作原理**：

> 通过 Selector **注册通道的事件**，Selector 会不断地**轮询**注册在其上的 Channel。
>
> 当事件发生时，比如某个 Channel 上面有新的 TCP 连接接入、读和写事件，这个 Channel 就处于就绪状态，会被 Selector 轮询出来。
>
> Selector 会将相关的 Channel 加入到**就绪集合**中。通过 SelectionKey 可以获取就绪 Channel 的集合，然后对这些就绪的 Channel 进行相应的 I/O 操作

Selector监听的事件类型：

> `SelectionKey.OP_ACCEPT`：表示通道接受连接的事件，这通常用于 `ServerSocketChannel`。
>
> `SelectionKey.OP_CONNECT`：表示通道完成连接的事件，这通常用于 `SocketChannel`。
>
> `SelectionKey.OP_READ`：表示通道准备好进行读取的事件，即有数据可读。
>
> `SelectionKey.OP_WRITE`：表示通道准备好进行写入的事件，即可以写入数据。

Selector实例的创建：不能通过new，只能通过静态方法`open()`

Selector有三个**SelectionKey**：

> **所有**的 `SelectionKey` 集合：代表了注册在该 Selector 上的 `Channel`，这个集合可以通过 `keys()` 方法返回
>
> **被选择**的 `SelectionKey` 集合：代表了所有可通过 `select()` 方法获取的、需要进行 `IO` 处理的 Channel，这个集合可以通过 `selectedKeys()` 返回
>
> **被取消**的 `SelectionKey` 集合：代表了所有被取消注册关系的 `Channel`，在下一次执行 `select()` 方法时，这些 `Channel` 对应的 `SelectionKey` 会被彻底删除，程序通常无须直接访问该集合，也没有暴露访问的方法



###### IO多路复用：select  poll  epoll

常见的 I/O 多路复用机制包括 select、poll 和 epoll 等

| 特性           | `select`             | `poll`         | `epoll`              |
| -------------- | -------------------- | -------------- | -------------------- |
| 文件描述符限制 | 受 `FD_SETSIZE` 限制 | 无限制         | 无限制               |
| 时间复杂度     | O(n)                 | O(n)           | O(1)                 |
| 数据复制       | 需要                 | 需要           | 不需要               |
| 工作方式       | 线性扫描             | 线性扫描       | 事件通知             |
| 内核支持       | 所有 UNIX 系统       | 所有 UNIX 系统 | Linux 2.6 及以上版本 |
| 适用场景       | 少量连接             | 中等连接       | 大量并发连接         |

select：

> 将已连接的 Socket 都放到⼀个**⽂件描述符集合fd_set**，然后调⽤ select 函数将 fd_set 集合**拷⻉到内核⾥**，让内核来检查是否有⽹络事件产⽣，检查的⽅式很粗暴，就是通过遍历 fd_set 的⽅式，当检查到有事件产⽣后，将此 Socket 标记为可读或可写， 接着**再把整个 fd_set 拷⻉回⽤户态⾥**，然后⽤户态还需要再通过遍历的⽅法找到可读或可写的 Socket，再对其处理
>
> 特点：
>
> ​	监听多个文件描述符（FD）
>
> ​	使用 **固定大小的数组**（一般 `FD_SET_SIZE=1024`）来存储监听的 FD 【受FD_SETSIZE限制】
>
> ​	每次调用 `select()`，都需要**重新传递所有 FD 集合**，从用户态复制到内核态，并让内核**轮询**所有 FD 之后，还需要重新把FD集合拷贝会用户态，用户程序再遍历一遍，效率较低，O(2N)

poll：

> 与 `select` 类似，只是改为用 **链表** 存储 FD，因此**没有最大连接数限制**
>
> 仍然是**轮询**所有 FD，性能仍然是 **O(N)**，但比 `select` 兼容性更好

epoll：

> 使用**红黑树**和**链表**来管理FD，只需要从用户态**拷贝一次**到内核态（即红黑树和链表都在内核里），基于回调的方式，当某个FD就绪时，通知应用程序
>
> **事件驱动**，不需要轮询所有 FD，只有发生事件的 FD 才会被**通知**，O(1)
>
> epoll 方式是将用户 socket 对应的 fd 注册进 epoll，然后 epoll 帮你监听哪些 socket 上有消息到达，这样就避免了大量的无用操作

在Redis中就是用epoll来实现IO多路复用的

<img src="https://cdn.tobebetterjavaer.com/stutymore/redis-20240918114125.png" alt="有盐先生：IO 多路复用" style="zoom:67%;" />

###### epoll具体原理 :star:

> 第⼀点，epoll 在内核⾥使⽤**红⿊树来跟踪进程所有待检测的⽂件描述字**，把需要监控的 socket 通过 epoll_ctl() 函数加⼊内核中的红⿊树⾥，红⿊树是个⾼效的数据结构，增删查⼀般时间复杂度是 O(logn) ，通过对这棵⿊红树进⾏操作，这样就不需要像 select/poll 每次操作时都传⼊整个 socket 集合，只需要传⼊⼀个待检测的 socket，**减少了内核和⽤户空间⼤量的数据拷⻉和内存分配**。
>
> 第⼆点， epoll 使⽤事件驱动的机制，内核⾥**维护了⼀个链表来记录就绪事件**，当某个 socket 有事件发⽣时，通过回调函数，内核会将其加⼊到这个就绪事件列表中，当⽤户调⽤ epoll_wait() 函数时，只会返回有事件发⽣的⽂件描述符的个数，不需要像 select/poll 那样轮询扫描整个 socket 集合，⼤⼤提⾼了检测的效率。
>
> <img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/os-cca76ac4-cfb4-4374-8fc6-256cd4d3893f.png" alt="epoll接口作用-来源参考[3]" style="zoom:67%;" />

> ①用户通过`epoll_create()`创建一个**epoll文件描述符**，用于后续的epoll操作
>
> ​	该方法中创建了一个 `eventpoll` 对象，并初始化其等待队列`wq(wati_queue)`、就绪队列`rdllist`、红黑树`rbr`
>
> ②`epoll_ctl()`函数用于增加，删除，修改epoll事件，epoll事件会存储于内核epoll结构体红黑树中
>
> ​	使用红黑树来提高epoll事件的增删改查效率
>
> ③`epoll_wait()`用于监听事件，它轮询就绪队列`rdllist`，一旦就绪队列中有就绪的epoll时间，就会调用其回调函数
>
> O(1)的原因：当某个epoll事件就绪时，就会<u>从红黑树中移到就绪队列</u>中，所以只需要遍历就绪队列就可以，不需要遍历多有的epoll事件，避免了无效的轮询

水平触发 vs. 边缘触发

> **水平触发LT**：epoll默认情况下就是LT
>
> * 当文件描述符就绪时，`epoll_wait` 函数会立即返回，并且会返回**所有处于就绪状态的文件描述符**
> * 只要文件描述符上的事件就绪状态**持续存在**，就会重复触发通知（如果读取数据后缓冲区仍有数据，`epoll` 会再次通知）
>
> **边缘触发ET**：将socket添加到epoll描述符的时候使用了`EPOLLET`标志, epoll进入ET工作模式
>
> * 当文件描述符就绪时，`epoll_wait` 函数只会返回一次，并且只返回该文件描述符上<u>自上次 epoll_wait 调用后</u>发生的就绪事件
> * 只有从无到有的“边缘变化”时才触发事件
> * 仅在文件描述符的事件就绪状态**发生变化时**触发一次（例如，从无数据变为有数据时）

##### 什么情况下select和epoll的没有差别？
> 1. 连接数不多
> 2. select数组中都是就绪事件，epoll就绪队列中也全是就绪事件，这样就都得遍历整个数组



## Java 集合 :white_check_mark:

两个接口：`Collection` 和 `Map`

`Collection`下分为`List` `Set` `Queue`三大类

![Java 集合框架概览](https://oss.javaguide.cn/github/javaguide/java/collection/java-collection-hierarchy.png)

各种类的底层数据结构、实现、扩容方式都要掌握



##### List： ArrayList 和 LinkedList 和 Vector  :star:

| 特性             | `ArrayList`                                                  | `LinkedList`            | `Vector`                                                   |
| ---------------- | ------------------------------------------------------------ | ----------------------- | ---------------------------------------------------------- |
| **底层数据结构** | 数组                                                         | 双向不循环链表          | 数组                                                       |
| **随机访问**     | 支持随机访问 O(1)                                            | 不支持，需遍历链表 O(n) | 支持随机访问 O(1)                                          |
| **中间插入**     | 需要移动元素 O(n)                                            | 遍历链表 O(n)           | 需要移动元素 O(n)                                          |
| **头/尾部插入**  | 头部O(n) 需要移动元素<br />尾部O(1) 或者 O(n) 可能触发扩容   | O(1)                    | 头部O(n) 需要移动元素<br />尾部O(1) 或者 O(n) 可能触发扩容 |
| **线程安全**     | 不安全                                                       | 不安全                  | **安全**（使用 `synchronized` 关键字）                     |
| **扩容策略**     | **1.5** 倍左右                                               | 无需扩容                | **2 **倍扩容                                               |
| **其他**         | 可以插入`null元素`<br />性能更好<br />实现了 `RandomAccess` 接口 | 支持`null`              |                                                            |

###### Vector vs. Stack

> Stack是**继承**于Vector的，两者都是**线程安全**的，都使用synchronized关键字
>
> Vector是列表、Stack是后进先出

###### ArrayList的序列化

序列化：writeObject

```java
private void writeObject(ObjectOutputStream s) throws IOException {
    // 将当前 ArrayList 的结构进行序列化
    int expectedModCount = modCount;
    s.defaultWriteObject(); // 序列化非 transient 字段
    // 序列化数组的大小
    s.writeInt(size);
    // 序列化每个元素
    for (int i = 0; i < size; i++) {
        s.writeObject(elementData[i]);
    }
    // 检查是否在序列化期间发生了并发修改
    if (modCount != expectedModCount) {
        throw new ConcurrentModificationException();
    }
}
```

反序列化：readObject

```java
private void readObject(ObjectInputStream s) throws IOException, ClassNotFoundException {
    // 读取非 transient 字段
    s.defaultReadObject();
    
    // 读取 size
    int size = s.readInt();
    
    // 初始化 elementData 数组
    ensureCapacity(size);
    
    // 逐个读取元素并存入数组
    for (int i = 0; i < size; i++) {
        elementData[i] = s.readObject();
    }
    
    this.size = size;
}
```





#####  Set ： HashSet 和 LinkedHashSet 和 TreeSet 

Set特性：元素唯一且无需

| **特性**            | **HashSet**              | **LinkedHashSet**            | **TreeSet**                  |
| ------------------- | ------------------------ | ---------------------------- | ---------------------------- |
| **底层数据结构**    | `HashMap`                | `LinkedHashMap` + 双向链表   | `TreeMap`（红黑树）          |
| **元素顺序**        | 无序                     | 插入顺序                     | 自然排序 / 自定义 Comparator |
| **查找速度**        | **O(1)**                 | **O(1)**（稍慢于 `HashSet`） | **O(log n)**                 |
| **插入/删除速度**   | **O(1)**                 | **O(1)**                     | **O(log n)**                 |
| **是否允许 `null`** | ✅                        | ✅                            | ❌                            |
| **是否支持排序**    | ❌ 无序                   | ❌ 按插入顺序FIFO             | ✅ 自然顺序或自定义           |
| **线程安全**        | 不安全                   | 不安全                       | 不安全                       |
| **适用场景**        | 只关心唯一性，不关注顺序 | 既要唯一性，又要保持插入顺序 | 需要自动排序的集合           |
| **其他**            |                          | 是`HashSet`的子类            | 实现了`SortedSet`接口        |

###### HashSet

HashSet底层直接调用了HashMap提供的api，例如`add`调用了HashMap的`put`操作

HashSet检查重复的方法：通过`put`方法以及返回值来判断是否有重复元素

```java
public boolean add(E e) {
	return map.put(e, PRESENT)==null;
}
```









##### Queue：Queue 和 Deque 

`Queue` 是单端队列，只能从一端插入元素，另一端删除元素，FIFO

`Deque` 是双端队列，在队列的两端均可以插入或删除元素，`Deque`实现了`Queue`

###### ArrayDeque 与 LinkedList 的区别

`ArrayDeque`和`LinkedList`都实现了`Deque`

`PriorityQueue`只实现了`Queue`

| **特性**            | ArrayDeque                             | LinkedList                | PriorityQueue                            |
| ------------------- | -------------------------------------- | ------------------------- | ---------------------------------------- |
| **底层数据结构**    | 数组 + 双指针                          | 双向链表                  | 二叉堆                                   |
| **随机访问速度**    | O(1）                                  | O(n)                      |                                          |
| **插入/删除速度**   | 头/尾 O(1)，中间 O(n)，均摊O(1)        | 头/尾 O(1)，中间 O(1)     | O(logN)                                  |
| **扩容策略**        | 2 倍扩容                               | 无需扩容                  |                                          |
| **线程安全**        | 不安全                                 | 不安全                    | 不安全                                   |
| **是否允许 `null`** | 不允许 `null`                          | 允许 `null`               | 不允许`null`，不允许`non-Comparable`对象 |
| **适用场景**        | 队列（FIFO）、栈（LIFO）、高效双端访问 | 队列、栈、频繁的插入/删除 | O(1)操作获取最值                         |

###### BlockingQueue接口

`阻塞队列`：当队列没有元素时一直阻塞，直到有元素；当队列满了要加入元素时也会阻塞直到队列有空位

一般用于消息队列

<img src="https://oss.javaguide.cn/github/javaguide/java/collection/blocking-queue.png" alt="BlockingQueue" style="zoom:50%;" />

其实现类有：`ArrayBlockingQueue`、`LinkedBlockingQueue`、`PriorityBlockingQueue`、`SynchronousQueue`、`DelayQueue`

锁是否分离： `ArrayBlockingQueue`中的锁是没有分离的，即生产和消费用的是同一个锁；`LinkedBlockingQueue`中的锁是分离的，即生产用的是`putLock`，消费是`takeLock`，这样可以防止生产者和消费者线程之间的锁争夺





##### Map：HashMap 和 HashTable  :star:

|                | HashMap                                                      | HashTable                          |
| -------------- | ------------------------------------------------------------ | ---------------------------------- |
| 底层数据结构   | 数组 + 链表 + 红黑树(解决冲突)<br />当链表长度大于8且数组长度大于64，链表会转换成**红黑树**，否则**数组扩容** | 数组 + 链表                        |
| 线程安全       | 不安全                                                       | 安全（基本方法都有`synchronized`） |
| 效率           | 高                                                           | 低                                 |
| `null`的支持   | 支持`null`作为key和value<br />其中`null`只能作为一个key<br />`null`可以作为多个value | key和value都不支持                 |
| 初始容量       | 16                                                           | 11                                 |
| 扩容           | 2n                                                           | 2n+1                               |
| 最大容量       | 1<<30                                                        | Integer.MAX_VALUE - 8              |
| 哈希值计算方式 | 对哈希值进行了高位和低位的**混合扰动**处理以减少冲突         | 直接使用键的 `hashCode()` 值       |
| 使用场景       |                                                              | 已不推荐使用                       |

###### HashMap特别重要 :star:

`hash()`函数

> HashMap的映射：Index = **hash() % length()** = hash() & (length - 1) 
>
> HashMap的hash()扰动函数：对hashCode的结果进行处理
>
> ```java
> static final int hash(Object key) {
>     int h = key.hashCode();  // key.hashCode()：返回key的hashcode
>     // ^：按位异或  >>>:无符号右移，忽略符号位，空位都以0补齐
>     return (key == null) ? 0 : h ^ (h >>> 16);
> }
> ```
> 为什么异或和右移可以减少哈希冲突？
>
> ​	如果直接拿key的hashcode去%length，即直接&length-1，也就是直接取其低位（例如length=15就是取低4位）
>
> ​	如果右移16位，那么高16位和低16位就可以一起参与到哈希的计算中



> 关于HashMap的**红黑树**
>
> ​	当某条链表的长度大于**8**(`TREEIFY_THREHOLD`)时，这是如果数组长度小于64，那么直接扩容数组
>
> ​	如果此时数组长度大于**64**(`MIN_TREEIFY_CAPACITY`)，那么将链表转换为红黑树
>
> ​	查询从O(n) ==> O(logN)
>
> <img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/collection-8.png" alt="三分恶面渣逆袭：JDK 8 HashMap 数据结构示意图" style="zoom:67%;" />
>
> ​	如果删除元素时，红黑树的节点数小于**6**(`UNTREEIFY_THRESHOLD`)，那么变回链表
>
> 官方对`TREEIFY_THRESHOLD = 8`和`UNTREEIFY_THRESHOLD = 6`的解释：
>
> > 红黑树节点的大小大概是普通节点大小的两倍，所以转红黑树，牺牲了空间换时间，更多的是一种兜底的策略，保证极端情况下的查找效率
> >
> > **阈值为什么要选 8 呢？**和统计学有关。理想情况下，使用随机哈希码，链表里的节点符合泊松分布，出现节点个数的概率是递减的，节点个数为 8 的情况，发生概率仅为`0.00000006`
> >
> > **至于红黑树转回链表的阈值为什么是 6，而不是 8？**是因为如果这个阈值也设置成 8，假如发生碰撞，节点增减刚好在 8 附近，会发生链表和红黑树的不断转换，导致资源浪费

> 为什么不用**二叉树**？
>
> 二叉树是最基本的树结构，每个节点最多有两个子节点，但是二叉树容易出现极端情况，比如插入的数据是有序的，那么二叉树就会<u>退化成链表</u>，查询效率就会变成 O(n)
>
> 为什么不用**平衡二叉树**？
>
> 平衡二叉树比红黑树的<u>要求更高</u>，每个节点的左右子树的高度最多相差 1，这种高度的平衡保证了极佳的查找效率，但在进行插入和删除操作时，可能需要频繁地进行<u>旋转</u>来维持树的平衡，维护成本更高
>
> 为什么用**红黑树**？
>
> 红黑树是一种折中的方案，查找、插入、删除的时间复杂度都是 `O(log n)`

> 为什么HashMap的数组长度要为**2的幂次**？
>
> ​	因为一个元素经过散列后得到hashCode，再通过`hashCode%(length-1)`知道它应该存放到数组哪一个位置
>
> ​	如果length为2的n次，那么`hashCode % length`等价于`hashCode & (length-1)`
>
> ​	我们知道&运算符直接对二进制数进行运算，效率高
>
> ​	第二个原因时，2倍扩容可以让扩容后的元素分布**均匀**，因为每个元素要么留在原位index，要么移动到index+oldCapacity处
>
> 所以初始化HashMap时传入一个不是2的幂次的数，也是会初始化位2的幂次容量。这里就用到了`tableSizeFor(n)`
>
> ```java
> static final int tableSizeFor(int cap) {
>  int n = cap - 1;
>  n |= n >>> 1;
>  n |= n >>> 2;
>  n |= n >>> 4;
>  n |= n >>> 8;
>  n |= n >>> 16;
>  return (n < 0) ? 1 : (n >= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;
> }
> ```
>
> 例如n=17，会得到32。最大容量`MAXIMUM_CAPACITY` = 2^30
>
> 这个方法使用了二进制运算，目标就是锁定住n的最高位1，并将该位置之后都置为1，从而得到一个大于n的(2次幂-1)的数
>
> - `n |= n >>> 1`：将**最高位 1** 向右传播 1 位（例如：`1000` → `1100`）
> - `n |= n >>> 2`：将已传播的范围(最高2位)再向右扩展 2 位（例如：`1100` → `1111`）
> - `n |= n >>> 4`：再将最高四位扩展 4 位
> - `n |= n >>> 8`：再扩展 8 位
> - `n |= n >>> 16`：再扩展 16 位

> HashMap在插入新元素时，是插入到链表的尾端，即**尾插法**(JDK1.8)
>
> 因为JDK1.7使用的**头插法**会出现在多线程并发扩容的情况下导致形成环形链表而导致**死循环**

HashMap的`put()`的具体流程：

> 计算hash() => 找到key对应的位置 => 插入 => 判断是否需要扩容
>
> ​	如果是树节点，调用红黑树的`putTreeVal(val)`
>
> ​	如果是链表节点，遍历到链表**尾部**插入（JDK1.8） / **头部**插入（JDK1.7）
>
> <img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/collection-13.jpg" alt="三分恶面渣逆袭：HashMap插入数据流程图" style="zoom: 50%;" />

`resize()`扩容后，元素重新哈希的过程：`rehash()`

> ①如果当前桶中只有一个元素，那么直接通过**键的哈希值**与**数组大小**取模锁定新的索引位置：`e.hash & (newCap - 1)`
>
> ②如果当前桶是一个链表，通过**旧键的哈希值**与**旧的数组大小**取模 `(key.hash & oldCap)` 来作为判断条件：
>
> * 如果为0，元素保留在**原索引**的位置
> * 如果为1，元素移动到**原索引 + 旧数组大小**的位置
>   * 例如原本容量为16，那么5（0000 <u>0101</u>）会映射到5，21（0001 <u>0101</u>）也会映射到 5
>   * 此时扩容为32，那么5（000<u>**0** 0101</u>）还留在原位，21（000<u>**1** 0101</u>）会映射到 21 即 5 + 16
>   * 所以只需要判断**第5位**是0是1，就也可以知道这个key是留在**原位**，还是**原索引+原数组大小**的位置
>   * 而如何知道第5位是0是1，只需要用`key.hash & oldCap`就知道了
>   * 这样可以减少重复计算哈希的开销
>
> ③如果当前桶是红黑树，那么会调用 `split()` 方法分裂树节点，以保证树的平衡

Overall：JDK8的HashMap相比JDK7做了什么优化？

> ①底层数据结构由数组 + 链表改成了数组 + 链表或红黑树的结构
>
> ②链表的插入方式由头插法改为了**尾插法**。头插法在扩容后容易改变原来链表的顺序，也会导致死循环
>
> ③**扩容的时机**由**插入时**判断改为**插入后判断**
>
> ④**哈希扰动算法**也进行了优化

###### HashMap 和 TreeMap

TreeMap主要比HashMap多实现了`NavigableMap`接口和`SortedMap` 接口

​	实现 `NavigableMap` 接口让 `TreeMap` 有了对集合内元素的**搜索**的能力，基于**红黑树**数据结构

​	实现`SortedMap`接口让 `TreeMap` 有了对集合中的元素**根据键排序**的能力



###### HashMap的for循环遍历

HashMap的for遍历是以什么顺序？如果要按照元素插入的顺序应该用什么？

> HashMapfor循环遍历的顺序是**底层数组中元素存储的顺序**
>
> ​	如果节点是链表，那么以链表的顺序遍历
>
> ​	如果节点是红黑树，那么以**中序遍历**的顺序
>
> ```java
> for (Map.Entry<String, Integer> entry : map.entrySet()) {
> System.out.println(entry.getKey() + " -> " + entry.getValue());
> }
> for(String key: map.keySet()) {
> 
> }
> for(String value: map.valueSet()) {
> 
> }
> ```
>
> 如果要按照**插入顺序**，可以使用 LinkedHashMap
>
> 如果要对key进行**排序**，可以使用 TreeMap





##### 扩容机制 :star:

###### 1.ArrayList

> 底层：Object[]
>
> 默认长度：**10**
>
> 最大容量：**Integer.MAX_VALUE-8**
>
> 三种构造函数：无参、有参（int capacity）、有参（Collection<T> )
>
> ​	无参构造函数：默认创建长度为10的数组
>
> ​	capacity：创建长度为capacity的数组
>
> ​	Collection：将Collection中的元素拷贝到Object[]中
>
> 扩容：**1.5倍**
>
> ```java
> // 1.先扩容1.5倍左右
> int newCapacity = oldCapacity + (oldCapacity >> 1);
> 
> // 2.再判断扩容后够不够最小容量要求：如果不够，那么直接扩容为最小容量要求
> if (newCapacity - minCapacity < 0) 
>  newCapacity = minCapacity;
> 
> // 3.如果新容量大于 MAX_ARRAY_SIZE 执行 hugeCapacity()方法
> // 比较 minCapacity 和 MAX_ARRAY_SIZE，
> // 		①如果 minCapacity 大于最大容量，则新容量则为 Integer.MAX_VALUE
> //  	②否则，新容量大小则为 MAX_ARRAY_SIZE 即为 Integer.MAX_VALUE - 8
> if (newCapacity - MAX_ARRAY_SIZE > 0)
>  newCapacity = hugeCapacity(minCapacity);
> 
> // 4.最后拷贝数组
> elementData = Arrays.copyOf(elementData, newCapacity);
> ```
>
> `>>`除以2，偶数的话是1.5倍，奇数的话小于1.5倍



###### 2.HashMap

> 初始容量16，数组扩容为2n
>
> 最大容量：**1 << 30** (2的30次方，即最大扩容30-4=26次)
>
> 当链表长度大于8时，如果此时数组长度还是小于64，那么数组扩容
>
> 如果此时数组长度大于64，那么将链表转化为红黑树
>
> 扩容的阈值是`capacity * loadFactor`，capacity 为容量，loadFactor 为负载因子，默认为 0.75



###### 3.HashTable

> 初始容量11，扩容2n+1
>
> 最大容量：**Integer.MAX_VALUE - 8**



###### 4.PriorityQueue

> 初始容量11，扩容1.5n



##### fail-fast 和 fail-safe 是什么

> `fail-fast`(快速失败)：在修改集合的时候，记录一个`modCount`来记录**修改次数**
>
> 最后跟预期修改次数`expectedModCount`进行比较，如果不相等，说明存在并发问题
>
> 从而实现**快速失败**，由此保证在避免在异常时执行非必要的复杂代码
>
> 抛出 `ConcurrentModificationException`
>
> 迭代器在遍历元素时也使用modCount，每次遍历完一个元素，modCount会加一
>
> 每当迭代器使用 `hashNext()/next()` 遍历下一个元素之前，都会检测 modCount 变量是否为 expectedmodCount 值，是的话就返回遍历；否则抛出异常，终止遍历。
>



> `fail-safe`(安全失败): 在修改集合之前先时复制出一份**快照**，基于这份快照完成添加或者删除操作后
>
> 例如`CopyOnWriteArrayList`，通过`Arrays.copyOf`得到一个数组的快照，基于这个快照完成添加操作后，修改底层`array`变量指向的引用地址由此完成写时复制
>





------

## JVM :white_check_mark:

重点：类的初始化、垃圾回收、Java内存划分

##### Java内存区域

###### JDK1.7

![Java 运行时数据区域（JDK1.7）](https://oss.javaguide.cn/github/javaguide/java/jvm/java-runtime-data-areas-jdk1.7.png)

###### JDK1.8

![Java 运行时数据区域（JDK1.8 ）](https://oss.javaguide.cn/github/javaguide/java/jvm/java-runtime-data-areas-jdk1.8.png)

详情：https://javaguide.cn/java/jvm/memory-area.html



###### 线程私有：PC、虚拟机栈、本地方法栈

**程序计数器PC**

> 存放了当前线程的执行的下一条执行的位置，通过改变程序计数器来依次读取指令，从而实现代码的流程控制
>
> 多线程的情况下用来记录每个线程执行到哪，从而在线程上下文切换后能回到上一次执行的位置

**虚拟机栈**：

> 线程调用**除了native方法之外的所有方法**都需要通过虚拟机栈来实现
>
> 每次调用一个方法，都会往栈中压入一个**栈帧**，每次方法结束，弹出这个栈帧
>
> 栈帧包括：
>
> * 局部变量表：存放各种基本数据类型和引用类型
> * 操作数栈：存放方法中产生的中间结果、临时变量
> * 动态链接：当前类的常量池引用，用来实现其他方法的调用等
> * 方法返回地址

**本地方法栈**：

> 服务于本地方法native的调用
>
> 当本地方法被调用时，会在本地方法栈中压入一个栈帧

###### 线程共享：堆、方法区（元空间）、直接内存

**堆**：

> “几乎”所有的对象都在堆中分配
>
> 堆是垃圾收集器管理的主要区域，因此也被称作 **GC 堆**，细分为：新生代、老年代和<u>永久代/元空间(方法区)</u>
>
> 【<u>永久代</u>在<u>JDK1.7</u>中存在于<u>堆</u>中，<u>元空间</u>在<u>JDK1.8</u>中被移到了<u>本地内存</u>中】
>
> 堆中含有**字符串常量池**：
>
> ​	一个固定大小的`HashTable` ，容量为 `StringTableSize`（可以通过 `-XX:StringTableSize` 参数来设置），保存的是字符串（key）和 字符串对象的引用（value）的映射关系，字符串对象的引用指向堆中的字符串对象

**方法区**：

《Java虚拟机规范》只给出了方法区的定义，并没有给出具体的实现，由各大厂商去实现

方法区 == 永久代/元空间

可以把**方法区**看为接口，而**永久代/元空间**是具体的实现（分别是JDK1.7和JDK1.8）

> 方法区会存储已被虚拟机加载的 **类信息、字段信息、方法信息、常量、静态变量、即时编译器编译后的代码缓存**
>
> - 类信息：包括类的结构信息、类的访问修饰符、父类与接口等信息
> - 常量池：存储类和接口中的常量，包括字面值常量、符号引用，以及运行时常量池
> - 静态变量：存储类的静态变量，这些变量在类初始化的时候被赋值
> - 方法字节码：存储类的方法字节码，即编译后的代码
> - 符号引用：存储类和方法的符号引用，是一种直接引用不同于直接引用的引用类型
> - 运行时常量池：存储着在类文件中的常量池数据，在类加载后在方法区生成该运行时常量池
> - 常量池缓存：用于提升类加载的效率，将常用的常量缓存起来方便使用
>
> **JDK1.7永久代**的参数：
>
> ```java
> -XX:PermSize=N //方法区 (永久代) 初始大小
> -XX:MaxPermSize=N //方法区 (永久代) 最大大小
> ```
>
> JDK1.8中用**元空间**代替了JDK1.7的永久代
>
> ```java
> -XX:MetaspaceSize=N //设置 Metaspace 的初始（和最小大小）
> -XX:MaxMetaspaceSize=N //设置 Metaspace 的最大大小
> ```
>
> 元空间使用的是**本地内存**，受本机可用内存的限制，而不受JVM的限制
>
> * 元空间溢出的几率下降
> * 元空间存放加载的**类的元信息**，所以加载的类的个数不再由`MaxPermSize`限制，而由系统实际内存大小限制

**运行时常量池**：

> .Class文件中的常量池`constant_pool`：每个类都有用于存放编译期生成的各种**字面量**和**符号引用**的 **常量池表**
>
> * **字面量**：整数（1、132）、浮点数（1.554f）和字符串字面量（"abc"）
> * **符号引用**：类符号引用、字段符号引用、方法符号引用、接口方法符号
>
> 这些常量池表就存储在运行时常量池中

**直接内存**：

> 直接内存是一种特殊的内存缓冲区，并不在 Java 堆或方法区中分配的，而是通过 JNI 的方式在本地内存上分配的



###### 内存溢出 vs. 内存泄漏

> **内存溢出**，俗称 OOM，是指当程序请求分配内存时，由于没有足够的内存空间，从而抛出 OutOfMemoryError
>
> > `java.lang.OutOfMemoryError: GC Overhead Limit Exceeded`：当 JVM 花太多时间执行垃圾回收并且只能回收很少的堆空间时，就会发生此错误
> >
> > `java.lang.OutOfMemoryError: Java heap space` :假如在创建新的对象时, 堆内存中的空间不足以存放新创建的对象, 就会引发此错误
> >
> > `java.lang.OutOfMemoryError: MetaSpace`:元空间溢出
>
> **内存泄漏**是指程序在使用完内存后，未能及时释放，导致占用的内存无法再被使用。随着时间的推移，内存泄漏会导致可用内存逐渐减少，最终导致内存溢出。
>
> 
>
> ###### 有没有处理过内存泄漏/OOM问题？
>
> 有，在项目中，由于 ThreadLocal 没有及时清理导致出现了内存泄漏问题
>
> 具体是如何发现并解决的：[参考](https://javabetter.cn/sidebar/sanfene/jvm.html#_20-%E6%9C%89%E6%B2%A1%E6%9C%89%E5%A4%84%E7%90%86%E8%BF%87%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E9%97%AE%E9%A2%98)
>
> 使用`jstat -gcutil [pid] 5000 10` 每隔 5 秒输出 GC 信息，输出 10 次，查看 **Young GC** 和 **Full GC** 次数
>
> 一般来说，如果发现 `Full GC` 次数太多，就很大概率存在内存泄漏了
>
> **Full GC 频繁**说明老年代内存压力大，而**内存泄漏**会导致无用对象被错误地保留在内存中，迟迟无法被回收，**最终撑满老年代**，导致 JVM 不断触发 Full GC
>
> 解决：
>
> ①配置OOM时生成 `dump` 文件：
>
> ```
> -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=<path-to-dump-file>
> ```
>
> ②也可以手动生成 `dump` 文件：
>
> ```
> // 会输出进程 10025 的堆快照信息，保存到文件 heap.hprof 中
> jmap -dump:format=b,file=heap.hprof 10025
> ```
>
> ③生成 `dump` 文件后借助**可视化工具**分析哪个对象非常多，基本就能定位到问题根源了
>
> 图形化工具分析：如 JDK 自带的 **VisualVM**，从菜单 > 文件 > 装入 dump 文件
>
> 观察内存占用最多的对象，找到内存泄漏的源头
>
> ④发现时ThreadLocal<UserContext>导致的，线程池中的线程会复用，所以每个线程的ThreadLocalMap是不会清空的，导致Map越来越大



##### Java内存模型（JMM，Java Memory Model）

解决多线程之间的通讯问题

###### 指令重排序

> Java 源代码会经历 **编译器优化重排 —> 指令并行重排 —> 内存系统重排** 的过程，最终才变成操作系统可执行的指令序列
>
> **内存屏障**（Memory Barrier）是一种 CPU 指令，用来禁止处理器指令发生重排序（像屏障一样），从而保障指令执行的有序性
>
> Java提供了 `happens-before` 来解决指令重排序的问题

###### JMM

> **主内存是**所有线程的共享内存，可以用来线程的通信，**所有线程创建的实例对象**都存放在主内存中，不管该实例对象是成员变量，还是局部变量，类信息、常量、静态变量都是放在主内存中
>
> **本地内存**：每个线程都有一个私有的本地内存，本地内存存储了该线程以读 / 写共享变量的**副本**

<img src="https://oss.javaguide.cn/github/javaguide/java/concurrent/jmm.png" alt="JMM(Java 内存模型)" style="zoom:67%;" />

> 主内存与工作内存直接的具体交互协议（一个变量如何从主内存拷贝到工作内存，如何从工作内存同步到主内存？）
>
> JMM定义了8个同步操作：
>
> **lock（锁定）**: 作用于**主内存**中的变量，将它标记为**一个线程独享变量**
>
> **unlock（解锁）**: 作用于**主内存**中的变量，解除变量的锁定状态，被解除锁定状态的变量才能被其他线程锁定
>
> 
>
> **read（读取）**：作用于主内存的变量，它把一个变量的值从主内存传输到线程的工作内存中，以便随后的 load 动作使用
>
> **load（载入）**：把 read 操作从主内存中得到的变量值放入工作内存的变量的副本中
>
> 
>
> **use(使用)**：把工作内存中的一个变量的值传给执行引擎，每当虚拟机遇到一个使用变量的指令时都会使用该指令
>
> **assign（赋值）**：作用于工作内存的变量，它把一个从执行引擎接收到的值赋给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作
>
> 
>
> **store（存储）**：作用于工作内存的变量，它把工作内存中一个变量的值传送到主内存中，以便随后的 write 操作使用
>
> **write（写入）**：作用于主内存的变量，它把 store 操作从工作内存中得到的变量的值放入主内存的变量中

###### Java内存区域 与 Java内存模型（JMM）的区别

>  **Java 内存区域和内存模型是完全不一样的两个东西**：
>
> - JVM 内存区域是 **JVM 在运行 Java 程序时划分的不同内存区域**，主要用于 **管理对象实例、方法调用、线程运行等**，比如堆主要用于存放对象实例
> - Java 内存模型和 **Java 的并发编程**相关，抽象了**线程和主内存之间的关系**，就比如说线程之间的共享变量必须存储在主内存中

###### happens-before：

> 如果**一个操作 happens-before 另一个操作**，那么**第一个操作的执行结果将对第二个操作可见**，并且第一个操作的执行顺序排在第二个操作之前
>
> 两个操作之间存在 happens-before 关系，并不意味着 Java 平台的具体实现必须要按照 happens-before 关系指定的顺序来执行。如果重排序之后的执行结果，与按 happens-before 关系来执行的结果一致，那么 JMM 也允许这样的重排序
>
> 对于<u>会改变程序执行结果的重排序</u>，JMM 要求编译器和处理器必须禁止这种重排序
>
> happens-before的规则：
>
> 1. **程序顺序规则**：一个线程内，按照代码顺序，书写在前面的操作 happens-before 于书写在后面的操作
> 2. **解锁规则**：解锁操作 happens-before 加锁操作
> 3. **volatile 变量规则**：对一个 volatile 变量的**写操作** happens-before 于后面对这个 volatile 变量的**读操作**
> 4. **传递规则**：如果 A happens-before B，且 B happens-before C，那么 A happens-before C
> 5. **线程启动规则**：Thread 对象的 `start()` 方法 happens-before 于此线程的每一个动作
>
> 如果两个操作不满足上述任意一个 happens-before 规则，那么这两个操作就没有顺序的保障，JVM 可以对这两个操作进行重排序

###### as-if-serial：

> As-If-Serial 规则允许 CPU 和编译器优化代码顺序，但前提是不会改变单线程的执行结果。**它只适用于单线程**



##### 一个Java对象的创建过程 :star:

先加载 后创建

> ###### Step1:类加载检查
>
> 虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否**能在运行时常量池中定位到这个类的符号引用**，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程。
>
> ###### Step2:分配内存
>
> 在**类加载检查**通过后，接下来虚拟机将为新生对象**分配内存**。对象**所需的内存大小**在类加载完成后便可确定，为对象分配空间的任务等同于把一块确定大小的内存从 Java 堆中划分出来。
>
> 这个内存大小记录在Class对象中
>
> **分配方式**有 **“指针碰撞”** 和 **“空闲列表”** 两种，选择哪种分配方式由 Java 堆是否规整决定，而 Java 堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定
>
> **内存分配的两种方式**：
>
> - **指针碰撞**：
>   - 适用场合：堆内存规整（即没有内存碎片）的情况下。<u>新生代</u>
>   - 原理：用过的内存全部整合到一边，没有用过的内存放在另一边，中间有一个分界指针，只需要向着没用过的内存方向将该指针移动对象内存大小位置即可。
>   - 使用该分配方式的 GC 收集器：Serial, ParNew
> - **空闲列表**：
>   - 适用场合：堆内存不规整的情况下。<u>老年代</u>
>   - 原理：虚拟机会维护一个列表，该列表中会记录哪些内存块是可用的，在分配的时候，找一块儿足够大的内存块儿来划分给对象实例，最后更新列表记录。
>   - 使用该分配方式的 GC 收集器：CMS
>
> 选择以上两种方式中的哪一种，取决于 **Java 堆内存是否规整**。而 Java 堆内存是否规整，取决于 GC 收集器的算法是"标记-清除"，还是"标记-整理"（也称作"标记-压缩"），值得注意的是，复制算法内存也是规整的。
>
> **内存分配并发问题**：
>
> ​	在创建对象的时候有一个很重要的问题，就是线程安全，因为在实际开发过程中，创建对象是很频繁的事情，作为虚拟机来说，必须要保证线程是安全的，通常来讲，虚拟机采用两种方式来保证线程安全：
>
> - **CAS+失败重试：** 虚拟机采用 CAS 配上失败重试的方式保证更新操作的原子性
> - **TLAB：** 为每一个线程预先在 `Eden` 区分配一块儿内存，JVM 在给线程中的对象分配内存时，首先在 TLAB 分配，当对象大于 TLAB 中的剩余内存或 TLAB 的内存已用尽时，再采用上述的 CAS 进行内存分配
>
> ###### Step3:初始化零值
>
> 内存分配完成后，虚拟机需要将分配到的内存空间都初始化为**零值**（**不包括对象头**），这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值
>
> ###### Step4:设置对象头
>
> 初始化零值完成之后，**虚拟机要对对象进行必要的设置**，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的**哈希码**、对象的 GC 分代**年龄**等信息。 **这些信息存放在对象头中。** 另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式
>
> ###### Step5:执行 init 方法
>
> 在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从 Java 程序的视角来看，对象创建才刚开始，`<init>` 方法还没有执行，<u>所有的字段都还为零。</u>
>
> 所以一般来说，执行 new 指令之后会接着执行 `<init>` 方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来

###### 创建对象分配的内存块

分为3部分：**对象头**、**实例数据**、对齐填充（Padding）

> **对象头**：
>
> * **标记字段**（Mark Word）：用于存储对象自身的运行时数据， 如**HashCode**、**GC 分代年龄**、锁状态标志、线程持有的锁、偏向线程 ID、偏向时间戳等等（<u>8字节</u>）
>
> * **类型指针**（Klass pointer）：对象指向它的**类的元数据**的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例
>
>   类型指针可能会被压缩，以节省内存空间，<u>压缩前为8个字节，压缩后为4个字节</u> `UseCompressedOops = true`
>
> * 对象是数组类型，还会有一个额外的**数组长度字段**（<u>4字节</u>，所以数组最大长度为 2³¹ - 1 = 2,147,483,647）
>
> **实例数据**：根据声明顺序，存储各个字段的值（int 4字节、long 8字节、char 2字节、... 、引用指针 8字节/4字节(压缩）)
>
> ​				 为了提高访问效率，进行了对齐（保证每次读取的内容都是完整的字段，不含有某个字段的一部分）
>
> **对齐填充**：JVM 的内存模型要求**对象的起始地址**是 **8 字节对齐**，所以一个对象的内存块必须为**8的倍数**
>
> ​		原因：CPU 进行内存访问时，一次寻址的指针大小是 8 字节，正好是 L1 Cache 中每**行的大小**
>

<img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/jvm-12.png" alt="三分恶面渣逆袭：对象的存储布局" style="zoom: 67%;" />



###### 程序是如何访问这个创建在堆上的对象的？==> 对象指针

> 程序运行的栈中，有`reference`来指向这块内存，具体有两种方式：句柄、直接指针
>
> **句柄**：堆中维护一个句柄池，reference指向**句柄池**
>
> ​	相当于间接访问，好处就是如果堆中的内存进行调整，只需要修改对应的句柄池就行
>
> <img src="https://oss.javaguide.cn/github/javaguide/java/jvm/access-location-of-object-handle.png" alt="对象的访问定位-使用句柄" style="zoom: 67%;" />
>
> **直接引用**：reference直接指向堆中对象
>
> ​	如果对象内存块移动，需要更新引用的值，如果一个对象有多个引用那会很麻烦
>
> <img src="https://oss.javaguide.cn/github/javaguide/java/jvm/access-location-of-object-handle-direct-pointer.png" alt="对象的访问定位-直接指针" style="zoom:67%;" />
>
> 一个对象引用8字节，压缩后是4字节



##### GC垃圾回收机制 :star:

> 针对 HotSpot VM 的实现，它里面的 GC 其实准确分类只有两大种：
>
> **部分收集** (Partial GC)：
>
> - 新生代收集（Minor GC / Young GC）：只对新生代进行垃圾收集
> - 老年代收集（Major GC / Old GC）：只对老年代进行垃圾收集。需要注意的是 Major GC 在有的语境中也用于指代整堆收集
> - 混合收集（Mixed GC）：对整个新生代和部分老年代进行垃圾收集
>
> **整堆收集** (Full GC)：收集**整个 Java 堆和方法区**

###### 什么是内存担保？

> **内存担保**：
>
> <u>JDK1.6</u>之前：
>
> 每次在进行Minor GC前，会先看**老年代中剩余的空间是否能够装下新生代中所有的对象**，如果可以，那么此次Minor GC才是安全的，可以执行；如果不可以，那么之间进行一次Full GC
> 
> 当然也可以通过设置`-XX:HandlePromotionFailure`（是否**允许担保失败**）参数为`允许`来强制执行非安全的Minor GC，此时会通过判断**历次晋升为老生代的对象的平均大小**是否小于老生代剩余空间，如果是，那么就可以执行本次非安全Minor GC，否则不能执行，还是执行Full GC
>
> JDK1.6之后：
>
> 只要**老年代的连续空间**大于**新生代对象总大小**或者**历次晋升的平均大小**，就会进行 Minor GC，否则进行Full GC

###### 关于年龄机制：

堆分为三部分：**新生代**（Eden、Survivor[S0、S1]）、**老生代**、**永久代**

默认 Eden ： S0 ： S1 的大小比例是 8∶1 ：1

一般创建一个对象会进入Eden，不够如果是**大对象**会直接进入老生代

每个对象的对象头中都有一个age(4bit，最大为1111，即15)

对象在 Survivor 中每熬过一次 MinorGC，年龄就增加 1 岁，当对象的age增加到15，就进入老生代

> `15`这个阈值并不是一成不变的，它可以通过`-XX:MaxTenuringThreshold`来设置
>
> 并且这个值也不是固定的，是动态的，具体机制如下：
>
> ​	Hotspot 遍历所有对象时，按照年龄**从小到大**对其所占用的大小进行累积，当这个累计值`total`大于 survivor 区的 50% ，那么此时的**age**会作为新的阈值（前提是这个age<`MaxTenuringThreshold`）

###### 关于死亡对象/垃圾：

> 如何判断一个对象是否可以被GC
>
> * **引用计数法**：给对象中添加一个引用计数器
>
>   * 每当有一个地方引用它，计数器就加 1
>   * 当引用失效，计数器就减 1
>   * 任何时候计数器为 0 的对象就是不可能再被使用的
>
>   问题：无法解决**循环引用**：两个对象互相引用，但不再被其他对象引用，它们的引用计数都不为零，因此不会被回收
>
> * **可达性分析算法**：通过一系列的称为 **“GC Roots”** 的对象作为起点，从这些节点开始向下搜索，节点所走过的路径称为**引用链**，当一个对象到 GC Roots 没有任何引用链相连的话，则证明此对象是不可用的，需要被回收
>
>   * GC Roots有：<u>虚拟机栈中的引用、本地方法栈中的引用、类静态变量、常量池常量</u>
>   
>     ![img](面经.assets/1719111821599-650b1691-2737-453b-ba4b-26b065a96e88.png)
>

==并不是不可达的对象就一定得被回收==

> 要真正宣告一个对象死亡，至少要经历**两次标记过程**；
>
> 1.可达性分析法中不可达的对象被**第一次标记**并且进行一次筛选，筛选的条件是此对象是否有必要执行 `finalize` 方法
>
> (1)确定对象是否需要执行 `finalize()`，没有则直接回收
>
> - 如果对象 **重写** 了 `finalize()` 方法，则 GC 认为它需要执行 `finalize()`，否则直接回收
> - `finalize()` 只有 第一次 被 GC 标记时才会执行，第二次标记时会直接回收
>
> (2)将对象放入 `F-Queue`
>
> - 如果需要执行 `finalize()`，对象会被放入一个称为 **F-Queue（Finalization Queue）** 的队列，等待 `Finalizer` 线程执行
> - 这个线程是 JVM 内部的一个守护线程，专门负责执行 `finalize()` 方法
>
> (3)执行 `finalize()` 方法
>
> - `Finalizer` 线程调用对象的 `finalize()` 方法，允许对象在回收前进行一些清理操作（如释放资源、关闭流等）
> - 注意： `finalize()` 方法执行时，可能会使对象重新变成可达（如 `this` 被赋值给某个静态变量），此时对象会逃脱第一次回收
>
> (4)重新判断对象是否仍然不可达
>
> - 如果 `finalize()` 执行后，**对象重新被引用**，从而复活，GC 会认为它存活，不会立即回收
> - 如果 `finalize()` 执行后，对象仍然是不可达的，GC 在下一轮垃圾回收时会**真正回收该对象**
>
> 2.被判定为需要执行的对象将会被放在一个队列中进行**第二次标记**，除非这个对象与引用链上的任何一个对象建立关联，否则就会被真的回收。
>
> 
>
> 什么时候会重载`finalize()`方法？
>
> ​	释放对象的文件资源、网络连接资源等
>
> ​	打印出对象合适被回收
>
> ​	对对象进行复活（重新引用该对象，使得这个对象不会被回收）

###### 关于废弃的常量：

假如在字符串常量池中存在字符串 "abc"，如果当前没有任何 String 对象引用该字符串常量的话，就说明常量 "abc" 就是废弃常量，如果这时发生内存回收的话而且有必要的话，"abc" 就会被系统清理出常量池了

###### 关于无用的类：

**同时**满足下面 3 个条件才能算是 **“无用的类”**：

- 该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例
- 加载该类的 `ClassLoader` 已经被回收，ClassLoader被回收了就无法获得这个类的Class对象了
- 该类对应的 `java.lang.Class` 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法



##### Java四种引用类型总结

强引用 > 软引用 > 弱引用 > 虚引用

> **1．强引用（StrongReference）**—— 生活必须品、使用最普遍的引用
>
> 如果一个对象具有强引用，那么**垃圾回收器绝不会回收它**
>
> 当内存空间不足，Java 虚拟机宁愿抛出 OutOfMemoryError 错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足问题
>
> **2．软引用（SoftReference）** —— 可有可无的生活用品
>
> 如果一个对象只具有软引用，如果内存空间足够，垃圾回收器就不会回收它，如果内存空间不足了，就会回收这些对象的内存，只要垃圾回收器没有回收它，该对象就可以被程序使用
>
> 软引用可用来实现**内存敏感的高速缓存**
>
> **3．弱引用（WeakReference）**—— 可有可无的生活用品
>
> 弱引用与软引用的区别在于：只具有弱引用的对象**拥有更短暂的生命周期**
>
> 在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程， 因此**不一定会很快发现**那些只具有弱引用的对象。
>
> **4．虚引用（PhantomReference）** —— 形同虚设
>
> 虚引用并**不会决定**对象的生命周期。如果一个对象仅持有虚引用，那么它就**和没有任何引用一样**，在任何时候都可能被垃圾回收
>
> **虚引用主要用来跟踪对象被垃圾回收的活动**
>
> **虚引用与软引用和弱引用的一个区别在于：** 虚引用**必须**和引用队列（ReferenceQueue）联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。程序如果发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动

###### 面试官拷打：弱引用每次gc都会被回收，那不是跟垃圾一样吗？

| 方面                                 | 弱引用 `WeakReference<T>`                                  | 垃圾对象（unreachable）              |
| ------------------------------------ | ---------------------------------------------------------- | ------------------------------------ |
| **是否还被引用？**                   | ✅ 是，有“弱引用”指向它                                     | ❌ 没有任何引用指向它                 |
| **是否可以“观察”它是否被回收？**     | ✅ 可以，比如通过 `WeakReference.get()` 或 `ReferenceQueue` | ❌ 不行，垃圾对象一旦不可达就不可用了 |
| **能不能使用这个引用再次访问对象？** | ✅ 在被回收前，可以通过 `.get()` 获取对象                   | ❌ 完全没有引用了，无法访问           |
| **用途**                             | 控制对象的生命周期，可做缓存、监听器等                     | 单纯的垃圾，没有用途                 |
| **回收时机**                         | 只要没有强引用，GC 时就会回收                              | 一旦不可达，GC 就会回收              |
| **程序是否知道它被回收？**           | ✅ 可以监听（如 `ReferenceQueue`）                          | ❌ 不知道，只能等它消失               |

可以用在：

**缓存系统**：弱引用常用于实现缓存，特别是当希望缓存项能够在内存压力下自动释放时。如果缓存的大小不受控制，可能会导致内存溢出。使用弱引用来维护缓存，可以让JVM在需要更多内存时自动清理这些缓存对象。

```java
import Java.lang.ref.WeakReference;
import Java.util.HashMap;
import Java.util.Map;

public class CacheExample {

    private Map<String, WeakReference<MyHeavyObject>> cache = new HashMap<>();

    public MyHeavyObject get(String key) {
        WeakReference<MyHeavyObject> ref = cache.get(key);
        if (ref != null) {
            return ref.get();
        } else {
            MyHeavyObject obj = new MyHeavyObject();
            cache.put(key, new WeakReference<>(obj));
            return obj;
        }
    }

    // 假设MyHeavyObject是一个占用大量内存的对象
    private static class MyHeavyObject {
        private byte[] largeData = new byte[1024 * 1024 * 10]; // 10MB data
    }
}
```

##### 垃圾回收算法 :star:

> 1.**标记-清除法**
>
> ​	分为“**标记**（Mark）”和“**清除**（Sweep）”阶段：
>
> ​	首先标记出所有不需要回收的对象，在标记完成后统一回收掉所有没有被标记的对象
>
> ​	两个阶段的效率都很差、产生**内存碎片**
>
> 2.**复制算法**
>
> ​	将内存分为**大小相同的两块**，每次使用其中的一块
>
> ​	当这一块的内存使用完后，就将还**存活的对象**复制到另一块去，然后再把使用的空间一次**清理掉**
>
> ​	虚拟机将内存分为一块较大的 **Eden 空间**和两块较小的 **Survivor 空间**，每次分配内存只使用 Eden 和其中一块 Survivor。发生垃圾收集时，将 Eden 和 Survivor 中仍然存活的对象一次性复制到另外一块 Survivor 空间上，然后直接清理掉 Eden 和已用过的那块 Survivor 空间
>
> <img src="面经.assets/image-20250308155428675.png" alt="image-20250308155428675" style="zoom: 25%;" />
>
> ​	不适合老年代存活对象多的，因为每次都要复制很多存活对象到另一块，开销大
>
> ​	使用于新生代，因为新生代存活的对象少
>
> 3.**标记-整理算法**
>
> ​	根据老年代的特点提出的一种标记算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象回收，而是让**所有存活的对象向一端移动**，然后直接**清理掉端边界以外的内存**
>
> ​	把存活对象全部移到最前面，把剩下的空间都清掉
>
> <img src="https://oss.javaguide.cn/github/javaguide/java/jvm/mark-and-compact-garbage-collection-algorithm.png" alt="标记-整理算法" style="zoom:80%;" />
>
> ​	移动这一步效率不高，不过解决了老年代的问题
>
> 4.**分代收集算法**
>
> ​	对于新生代、老生代，分别采用不同的垃圾回收算法
>
> ​	新生代中每次收集都会有大量对象死去，可以选择“复制”算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集
>
> ​	老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进行垃圾收集

###### 垃圾收集器：垃圾回收算法的具体实现

两大类：分代收集器（CMS）＆ 分区收集器（G1、ZGC）

> **==CMS垃圾收集器==：**Concurrent Mark Sweep，并发**标记清除**   —— JDK1.5引入，JDK9过时，JDK14移除
>
> ​	以获取最短回收停顿时间为目标的收集器
>
> **初始标记：** 短暂停顿STW，标记<u>直接与 GC Roots 相连</u>的对象（根对象）
>
> **并发标记：** 同时开启 GC 和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以 GC 线程无法保证可达性分析的实时性。所以<u>这个算法里会跟踪记录这些发生引用更新的地方</u>
>
> **重新标记：** 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短
>
> ​	在并发标记期间，如果对象的引用发生变化，CMS 需要额外的记录并发期间的对象变动
>
> ​	重新标记阶段会STW扫描这些记录下来的变动的对象，更新其标记
>
> **并发清除：** 开启用户线程，同时 GC 线程开始对未标记的区域做清扫
>
> <img src="https://cdn.tobebetterjavaer.com/stutymore/gc-collector-20231228211056.png" alt="小潘：CMS" style="zoom:80%;" />
>
> 优点：并发标记、低停顿
>
> 缺点：标记清除容易产生内存碎片、对CPU资源敏感、无法处理浮动垃圾



> **==G1垃圾收集器==**： Garbage-First，HotSpot的默认垃圾收集器，采用**标记整理**算法 —— JDK1.7引入，JDK9默认
>
> **初始标记**： 短暂停顿（Stop-The-World，STW），标记从 GC Roots 可直接引用的对象，即标记所有**直接可达**的活跃对象
>
> **并发标记**：与应用并发运行，标记**所有可达**对象。 这一阶段可能持续较长时间，取决于堆的大小和对象的数量
>
> **最终标记**： 短暂停顿（STW），处理并发标记阶段结束后残留的少量未处理的引用变更
>
> ​	在并发标记开始时，就保存了对象的**初始快照**，因此最终标记时，只需补充标记存活对象，无需重新扫描整个堆
>
> ​	Snapshot-At-The-Beginning（SATB）算法
>
> **筛选回收**：根据标记结果，选择回收**价值高**的区域，**复制**存活对象到新区域，回收旧区域内存。这一阶段包含一个或多个停顿（STW），具体取决于回收的复杂度
>
> ![G1 收集器](https://oss.javaguide.cn/github/javaguide/java/jvm/g1-garbage-collector.png)
>
> **G1 收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的 Region**，所以称为G-First
>
> <u>回收价值（回收收益分数） = 可回收垃圾大小 / 预估回收时间</u>
>
> 优点：利用多核CPU来缩短STW时间、建立可预测的停顿时间模型
>
> ​		  主要针对配备多颗处理器及大容量内存的机器



> ==**ZGC垃圾收集器**==： 采用**标记-复制**算法 —— JDK11引入
>
> 将垃圾收集的停顿时间控制在 10ms 以内，即使在 TB 级别的堆内存下也能保持较低的停顿时间
>
> 它通过并发标记和重定位来避免大部分 Stop-The-World 停顿，主要依赖**指针染色**来管理对象状态



###### CMS vs. G1

| 对比项           | **CMS（Concurrent Mark-Sweep）**                            | **G1（Garbage-First）**                                      |
| ---------------- | ----------------------------------------------------------- | ------------------------------------------------------------ |
| **设计目标**     | 低延迟，减少 STW 停顿                                       | 低延迟 + 高吞吐，减少全局停顿，适用于大堆内存                |
| **内存划分**     | 新生代 + 老年代（**分代**收集）                             | **分区**，统一堆划分为多个 Region                            |
| **GC 算法**      | **标记-清除**，不压缩                                       | **标记-整理**，会压缩                                        |
| **回收流程**     | 初始标记（STW） → 并发标记 → 重新标记（STW） → **并发清除** | 初始标记（STW） → 并发标记 → 最终标记（STW） → **筛选回收（STW）** |
| **碎片问题**     | 清除垃圾但不整理空间，易碎片化                              | 采用压缩整理，避免碎片化                                     |
| **Full GC 触发** | 并发回收失败、碎片化导致无法分配大对象                      | 老年代空间不足                                               |
| **Full GC 影响** | 触发后使用 `Serial Old`，单线程执行，停顿时间长             | 采用 `G1 Serial Old`，全堆整理，性能相对更好                 |
| **适用场景**     | 响应时间敏感的 Web 服务、小内存（<6GB）                     | 大内存（>6GB），需要低延迟和高吞吐的应用                     |
| **未来趋势**     | JDK 9+ 已不推荐，G1 替代 CMS                                | JDK 9+ 默认垃圾收集器                                        |



###### 什么是STW（Stop the World）

> JVM 进行垃圾回收的过程中，会涉及到对象的移动，**为了保证对象引用在移动过程中不被修改**，必须暂停所有的用户线程，像这样的停顿，我们称之为`Stop The World`。简称 STW
>
> 具体过程：
>
> JVM 会使用一个名为**安全点**（Safe Point）的机制来确保线程能够被安全地暂停，其过程包括四个步骤：
>
> - JVM 发出暂停信号：设置一个**全局标志位**，表示“要停下来“
> - 线程执行到安全点后，判断全局标志位是否为1，是的话挂起自身并等待垃圾收集完成；
> - 垃圾回收器完成 GC 操作；
> - 线程恢复执行
>
> **安全点**通常位于方法调用、循环跳转、异常处理等位置，以**保证线程暂停时数据的一致性**

###### 什么是三色标记法？

在**重新标记**阶段采用的是三色标记法

> 三色：
>
> 1. 白色（White）：尚未访问的对象。垃圾回收结束后，仍然为白色的对象会被认为是不可达的对象，可以回收
> 2. 灰色（Gray）：已经访问到但**未标记完**其引用的对象。灰色对象是需要进一步处理的
> 3. 黑色（Black）：已经访问到并且其**所有引用对象都已经标记过**。黑色对象是完全处理过的，不需要再处理

三色标记法的工作流程：

> ①初始标记（Initial Marking）：从 GC Roots 开始，标记所有直接可达的对象为**灰色**
>
> ②并发标记（Concurrent Marking）：在此阶段，标记**所有灰色对象引用的对象为灰色**，然后将**灰色对象自身标记为黑色**这个过程是并发的，和应用线程同时进行。
>
> 此阶段的一个问题是，应用线程可能在并发标记期间修改对象的引用关系，导致一些对象的标记状态不准确。
>
> ③重新标记（Remarking）：重新标记阶段的目标是处理并发标记阶段遗漏的引用变化。为了确保所有存活对象都被正确标记，remark 需要在 STW 暂停期间执行。
>
> ④使用写屏障（Write Barrier）：来捕捉并发标记阶段应用线程对对象引用的更新。通过遍历这些更新的引用来修正标记状态，确保遗漏的对象不会被错误地回收
>
> ![Java全栈架构师：三色标记法](https://cdn.tobebetterjavaer.com/stutymore/jvm-20240816132235.png)





##### 类文件（.class） 结构

```
// u4表示4个字节
ClassFile {
    u4             magic; //Class 文件的标志
    u2             minor_version;//Class 的小版本号
    u2             major_version;//Class 的大版本号
    u2             constant_pool_count;//常量池的数量
    cp_info        constant_pool[constant_pool_count-1];//常量池
    u2             access_flags;//Class 的访问标记
    u2             this_class;//当前类
    u2             super_class;//父类
    u2             interfaces_count;//接口数量
    u2             interfaces[interfaces_count];//一个类可以实现多个接口
    u2             fields_count;//字段数量
    field_info     fields[fields_count];//一个类可以有多个字段
    u2             methods_count;//方法数量
    method_info    methods[methods_count];//一个类可以有个多个方法
    u2             attributes_count;//此类的属性表中的属性数
    attribute_info attributes[attributes_count];//属性表集合
}
```

1.**魔数**（magic number）：4个字节，固定为0xCAFEBABE，可以记忆为咖啡宝贝

  版本号是jdk版本号，使用jdk编译生成的class文件，高版本的jdk可以运行低版本jdk编译的class文件，反之不行

2.常量池数组的长度为什么是`constant_pool_count - 1`?

```
	u2             constant_pool_count;//常量池的数量
    cp_info        constant_pool[constant_pool_count-1];//常量池
```

因为：**常量池计数器是从 1 开始计数的，将第 0 项常量空出来是有特殊考虑的，索引值为 0 代表“不引用任何一个常量池项”**）

常量池数组中，每一项都是一个**表**，存放的是不同类型的常量。每个表的第一项都是一个u1类型的标志位-tag

|               类型               | 标志（tag） |          描述          |
| :------------------------------: | :---------: | :--------------------: |
|        CONSTANT_utf8_info        |      1      |   UTF-8 编码的字符串   |
|      CONSTANT_Integer_info       |      3      |       整形字面量       |
|       CONSTANT_Float_info        |      4      |      浮点型字面量      |
|        CONSTANT_Long_info        |      5      |      长整型字面量      |
|       CONSTANT_Double_info       |      6      |   双精度浮点型字面量   |
|       CONSTANT_Class_info        |      7      |   类或接口的符号引用   |
|       CONSTANT_String_info       |      8      |    字符串类型字面量    |
|      CONSTANT_FieldRef_info      |      9      |     字段的符号引用     |
|     CONSTANT_MethodRef_info      |     10      |   类中方法的符号引用   |
| CONSTANT_InterfaceMethodRef_info |     11      |  接口中方法的符号引用  |
|    CONSTANT_NameAndType_info     |     12      |  字段或方法的符号引用  |
|     CONSTANT_MethodType_info     |     16      |      标志方法类型      |
|    CONSTANT_MethodHandle_info    |     15      |      表示方法句柄      |
|   CONSTANT_InvokeDynamic_info    |     18      | 表示一个动态方法调用点 |



3.类索引、父类索引、接口索引

**类索引**确定这个类的全限定名

**父类索引**确定这个类的父类的全限定名。除了 `java.lang.Object` 之外，所有的 Java 类都有父类，因此除了 `java.lang.Object` 外，所有 Java 类的父类索引都不为 0

**接口索引**集合用来描述这个类实现了哪些接口



4.字段Fields集合

```
    u2             fields_count;//字段数量
    field_info     fields[fields_count];//一个类可以有多个字段
```

![字段表的结构 ](面经.assets/字段表的结构.png)

**access_flags:** 字段的作用域（`public` ,`private`,`protected`修饰符），是实例变量还是类变量（`static`修饰符）,可否被序列化（transient 修饰符）,可变性（final）,可见性（volatile 修饰符，是否强制从主内存读写）。

**name_index:** 对常量池的引用，表示的字段的名称；

**descriptor_index:** 对常量池的引用，表示字段和方法的描述符；

**attributes_count:** 一个字段还会拥有一些额外的属性，attributes_count 存放属性的个数；

**attributes[attributes_count]:** 存放具体属性具体内容。



5.方法methods集合

method_info跟field_info结构体一模一样

![方法表的结构](面经.assets/方法表的结构.png)

##### 类加载的过程 :star:

类加载是一个对象能被创建的前提，如果判断此类还未加载，应该先加载，才能使用（即创建该类的对象实例）

> 加载 => 链接 => 初始化 ( => 使用 => 卸载)
>
> 其中：**链接** 拆分为：验证 => 准备 => 解析
>
> <img src="https://oss.javaguide.cn/github/javaguide/java/jvm/lifecycle-of-a-class.png" alt="一个类的完整生命周期" style="zoom:67%;" />
>
> ①**加载**：
>
> 1. 通过 **类全限定名** 查找类的 **字节码文件（.class 文件）** 并读取其二进制数据
> 2. 将该数据转化为 **方法区中的 **运行时数据结构 【方法区就是永久代/元空间】
> 3. 在方法区中创建一个 `Class` 对象来表示这个类的元信息
>
> ②**验证**： 确保被加载的类文件(.class)符合 JVM 规范，保证安全性
>
> 1. 文件格式验证（Class 文件格式检查）
> 2. 元数据验证（字节码语义检查）
> 3. 字节码验证（程序语义检查）
> 4. 符号引用验证（类的正确性检查）
>
> ③**准备**：为类的**静态（static ）变量**分配内存，并初始化**默认值**
>
> ​	注意：如果静态变量不是`final`的，那么初始化为0（不进行赋值操作）
>
> ​			 如果静态变量是`final`的，那么会进行赋值操作
>
> ```java
> public static int value=111;  		// ==> value在准备阶段的值为0，等到初始化阶段才是111
> public static final int value=111; // ==> value在准备阶段的值为111
> ```
>
> ④**解析**：将常量池中的**符号引用**替换为**直接引用**
>
> ⑤**初始化**：真正执行类初始化的java代码，包括执行静态变量的赋值、执行静态代码块
>
> ```java
> class Example {
>     static int a = 10;  // 在初始化阶段才赋值
>     static { // 在初始化阶段执行
>         System.out.println("Class Example initialized");
>     }
> }
> ```
>
> ⑥**使用**：创建该类的实例对象
>
> ⑦**卸载**：即该类的 **Class 对象（类初始化阶段创建的）**被 GC
>
> 条件：[无用的类]
>
> 1. 该类的**所有的实例对象**都已被 GC，也就是说堆不存在该类的实例对象
>
> 2. 该**类的类加载器**的实例已被 GC
>
> 3. 该类的Class对象**没有**在其他任何地方被引用
>
> 

###### 虚拟机严格规定必须对类进行【初始化】的条件：

> 1. 当遇到 `new`、 `getstatic`、`putstatic`或`invokestatic`这 4 条字节码指令时,即：
>    1. 创建一个类的实例对象
>    2. 访问类的静态变量(不是静态常量，因为静态常量在**准备阶段**就被赋值)
>    3. 给类的静态变量赋值
>    4. 调用类的静态方法
> 2. 使用 `java.lang.reflect` 包的方法对类进行反射调用时如 `Class.forName("...")`, `newInstance()` 等等。如果类没初始化，需要触发其初始化
> 3. 初始化一个类，如果其**父类还未初始化**，则先触发该父类的初始化
> 4. 当虚拟机启动时，用户需要定义一个要执行的主类 (包含 `main` 方法的那个类)，虚拟机会先初始化这个类
> 5. `MethodHandle` 和 `VarHandle` 可以看作是轻量级的反射调用机制，而要想使用这 2 个调用，就必须先使用 `findStaticVarHandle` 来初始化要调用的类
> 6. 当一个接口被 default 关键字修饰时，如果有这个接口的**实现类**发生了初始化，那该接口要在其之前被初始化。因为defalut方法中有方法实现

###### 类加载由**类加载器**完成

> 类加载器是一个负责加载类的对象，用于**加载类的字节码(.class文件)到JVM中，在内存中生成一个Class对象**
>
> 每个 Class对象 有一个引用指向加载它的 `ClassLoader`
>
> ```java
> class Class<T> {
>   ...
>   private final ClassLoader classLoader;
> 
>  @CallerSensitive
>   public ClassLoader getClassLoader() {
>     //...
>   }
>   ...
> }
> ```
>
> BUT：**数组类**不是通过 `ClassLoader` 创建的（数组类没有对应的二进制字节流），是由 **JVM直接生成** 的
>
> ​		数组类的类加载器是它的元素的类加载器，如果它的元素是基本数据类型，那么该数组类就没有类加载器
>
> 在创建一个类的实例对象的第一步就是判断该类是否已经加载，如果还没加载则需要先加载，如果已经加载过了，直接返回这个类加载器中的Class对象即可（ClassLoader中维护了所有由它加载过的类的Class对象）
>
> ```java
> public abstract class ClassLoader {
>   ...
>       private final ClassLoader parent;
>   // 由这个类加载器加载的类
>   private final Vector<Class<?>> classes = new Vector<>();
>   // 由VM调用，用此类加载器记录每个已加载类。
>   void addClass(Class<?> c) {
>         classes.addElement(c);
>   }
>   ...
> }
> ```
>
> Java内置的3个类加载器：
>
> * **`BootstrapClassLoader`(启动类加载器)**：最顶层的加载类，由 **C++**实现，通常表示为 null，并且没有父级加载器，主要用来加载 JDK 内部的核心类库以及被 `-Xbootclasspath`参数指定的路径下的所有类
> * **`ExtensionClassLoader`(扩展类加载器)**：主要负责加载 `%JRE_HOME%/lib/ext` 目录下的 jar 包和类以及被 `java.ext.dirs` 系统变量所指定的路径下的所有类
> * **`AppClassLoader`(应用程序类加载器)**：面向我们用户的加载器，负责加载当前应用 classpath 下的所有 jar 包和类
>
> 每个ClassLoader都可以通过其`ClassLoader parent`获得其父加载器，BootstrapClassLoader的父加载器为`null`
>
> 关系链：用户自定义类加载器 => AppClassLoader => ExtensionClassLoader => BootstrapClassLoader => null
>
> 所有加载器类都继承于抽象类ClassLoader

###### 类加载器——双亲委派模型 :star:

> `ClassLoader` 实例会在试图亲自查找类或资源**之前**，将搜索类或资源的任务**委托给其父类加载器**
>
> 【基于上面提到的所有的类加载器都有一个parent类加载器】
>
> :star:过程如下： `loadClass()`  —> 先尝试调用 `parent.loadClass()`，再调用`this.findClass()`
>
> ​	每当一个类加载器接收到加载请求时，它会**先将请求转发给父类加载器**（自底向上）
>
> ​	如果在父类加载器中找到了，直接返回对应的**Class对象**
>
> ​	如果在父类加载器**没有找到**所请求的类的情况下，会先**尝试去加载**
>
> ​	如果父类**也无法加载**这个类，那么就由子类去尝试加载（自顶向下）
>
> ​	如果最终子类加载器也无法加载这个类，那么它会抛出一个 `ClassNotFoundException` 异常
>
> <img src="https://oss.javaguide.cn/github/javaguide/java/jvm/class-loader-parents-delegation-model.png" alt="类加载器层次关系图" style="zoom: 67%;" />
>
> 如何打破双亲委派模型？==> 重写`loadClass()`
>
> ​	Tomcat 服务器为了能够**优先加载 Web 应用目录下的类**，然后再加载其他目录下的类，就自定义了类加载器 `WebAppClassLoader` 来打破双亲委托机制
>
> ​	热部署框架：在不重启服务器的情况下更新应用程序代码，需要替换旧版本的类，但旧版本的类可能由父加载器加载。自定义类加载器，优先加载新的类版本
>
> 为什么使用双亲委派模型？
>
> **①避免类的重复加载**：父加载器加载的类，子加载器无需重复加载
>
> **②保证核心类库的安全性**：如 `java.lang.*` 只能由 Bootstrap ClassLoader 加载，防止被篡改

###### Java判断两个类是否是同各类的条件

> JVM 不仅要看**类的全名**是否相同，还要看**加载此类的类加载器**是否一样。
>
> 只有两者都相同的情况，才认为两个类是相同的。

###### Tomcat的类加载器

> Tomcat 基于双亲委派模型进行了一些扩展，主要的类加载器有：
>
> - Bootstrap ClassLoader：加载 **Java 的核心类**库
>
> - Catalina ClassLoader：加载 **Tomcat 的核心类**库
>
> - Shared ClassLoader：加载**共享类库**，允许多个 Web 应用共享某些类库
>
> - WebApp ClassLoader：加载 **Web 应用程序的类**库，支持多应用隔离和优先加载应用自定义的类库
>
>   （<u>破坏了双亲委派模型</u>：先自己加载，加载不了再去找父加载器）
>
> <img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/jvm-48.png" alt="Tomcat类加载器" style="zoom: 67%;" />

###### 如何实现热部署？

> 第一步，使用文件监控机制，如 Java NIO 的 WatchService 来监控类文件或配置文件的变化。当监控到文件变更时，触发热部署流程
>
> 第二步，创建一个自定义类加载器，继承`java.lang.ClassLoader`，并重写`findClass()`方法，用来加载新的类文件



##### 对象逃逸

> <u>大部分对象都创建在**堆**上</u>，但并不是所有的对象都创建在堆上
>
> 是否创建在堆上，取决于对象的**逃逸分析**：
>
> **逃逸分析**：用于分析对象是否会**逃离当前方法或线程的作用域**。如果一个对象不会逃逸到方法外部，JVM 可以优化内存分配
>
> ​	①**栈上分配**：将这个对象创建在栈中，这样就不需要GC，减少GC压力
>
> ​	②**标量替代**：如果这个对象可以被拆解成独立变量，直接用变量替代对象
>
> ```java
> public void scalarReplace() {
>      Point p = new Point(3, 4); // 可能被替换为两个 int 变量
>      int sum = p.x + p.y;
>      System.out.println(sum);
> }
> 
> // 编译为：标量替代
> public void scalarReplace() {
>      int x = 3;
>      int y = 4;
>      int sum = x + y;
>      System.out.println(sum);
> }
> ```
>
> **逃逸行为**：
>
> ①方法逃逸：如果一个对象在**方法外部可访问**（如返回值、赋值给全局变量等），它必须在**堆上分配**
>
> ```java
> class EscapeExample {
>  static Object globalObj; // 逃逸到全局变量
> 
>  public static void methodEscape() {
>      globalObj = new Object(); // 在方法中创建的对象逃逸出当前方法
>  }
> 
>  public Object getObbject() {
>      Object obj = new Object();
>      return obj;  // 逃逸
>  }
> }
> ```
>
> ②线程逃逸：如果一个对象可以被**多个线程访问**，JVM 无法确定对象的生命周期，因此它必须分配在**堆**上
>
> ```java
> class ThreadEscapeExample {
>  public void threadEscape() {
>      new Thread(() -> {
>          Object obj = new Object(); // 逃逸到线程外部
>          System.out.println(obj);
>      }).start();
>  }
> }
> ```
>
> **逃逸技术的好处**：
>
> 第一，如果确定一个对象不会逃逸，那么就可以考虑栈上分配，对象占用的内存随着栈帧出栈后销毁，这样一来，垃圾收集的压力就降低很多。
>
> 第二，线程同步需要加锁，加锁就要占用系统资源，如果逃逸分析能够确定一个对象不会逃逸出线程，那么这个对象就不用加锁，从而减少线程同步的开销。
>
> 第三，如果对象的字段在方法中独立使用，JVM 可以将对象分解为**标量变量**，避免对象分配



##### JVM调优

jstack

jmap

jconsole可视化工具

VisualVM工具







------

## 并发 :white_check_mark:

重点：java线程、线程池、悲观锁(synchronized、ReenterLock)、乐观锁（CAS、原子类）、volatile、ThreadLocal、AQS

##### 并发编程三特性（什么是线程安全?）

> ###### 原子性
>
> 一次操作或者多次操作，要么所有的操作全部都得到执行并且不会受到任何因素的干扰而中断，要么都不执行。
>
> 可以通过synchronized和各种锁来实现原子性
>
> ###### 可见性
>
> 当一个线程对共享变量进行了修改，那么另外的线程都是立即可以看到修改后的最新值
>
> 可以通过volatile来实现
>
> ###### 有序性
>
> 要确保线程不会因为死锁、饥饿、活锁等问题导致无法继续执行
>
> 由于指令重排序问题，代码的执行顺序未必就是编写代码时候的顺序
>
> volatle可以禁止指令重排序，内存屏障也可以

并行：多核CPU，多个线程**同时**运行

并发：单核CPU，多个线程在**同一时间段**内交替运行



##### Java线程基础

> JDK1.2之前，java线程是使用**绿色线程**实现的，是一种用户级线程， JVM 自己模拟了多线程的运行，而不依赖于操作系统
>
> JDK1.2之后，Java 线程改为基于**原生线程**（Native Threads）实现，也就是说 JVM 直接使用操作系统原生的内核级线程（内核线程）来实现 Java 线程，由操作系统内核进行线程的调度和管理
>
> <img src="https://oss.javaguide.cn/github/javaguide/java/concurrent/three-types-of-thread-models.png" alt="常见的三种线程模型" style="zoom:67%;" />
>
> Windows和Linux系统下，Java的线程模型为**一对一**

###### Thread.sleep() vs. Object.wait() vs. Thread.join()

> sleep()**方法没有释放锁**，而 wait()方法**释放了锁** 
>
> sleep()随时可以调用，wait()需要持有对象锁才可以调用，调用后释放锁
>
> sleep()方法执行完成后，线程会**自动苏醒**，或者也可以使用 `sleep(long timeout)` 超时后线程会自动苏醒
>
> wait() 方法被调用后，线程**不会自动苏醒**，需要别的线程调用同一个对象上的 `notify()`或者 `notifyAll()` 方法来唤醒
>
> sleep()是 `Thread` 类的**静态本地方法**：Thread.sleep(ms)
>
> wait() 则是 `Object` 类的本地方法
>
> join()方法是让当前线程等待指定线程（调用 `join()` 的线程）执行完成后再继续执行，线程会进入等待态

###### Thread.interrupt() vs. Thread.stop()

> `interrupt()` 方法用于通知线程停止，但不会直接终止线程，需要线程自行处理中断标志，常与 `isInterrupted()` 或 `Thread.interrupted()` 配合使用
>
> `stop()` 方法用来强制停止线程，目前已经处于**废弃**状态，因为 stop 方法可能会在不一致的状态下释放锁，破坏对象的一致性
>
> ###### 如何真正停止一个线程？
>
> 每个线程都一个与之关联的布尔属性来表示其中断状态，中断状态的初始值为false，当一个线程被其它线程调用`Thread.interrupt()`方法中断时，会根据实际情况做出响应。
>
> - 如果该线程正在执行低级别的可中断方法（如`Thread.sleep()`、`Thread.join()`或`Object.wait()`），则会解除阻塞并**抛出`InterruptedException`异常**。
> - 否则`Thread.interrupt()`仅设置线程的中断状态，在该被中断的线程中稍后可通过轮询中断状态来决定是否要停止当前正在执行的任务
>
> 可以先调用sleep，再调用interrupt
>
> 

###### Thread.yeild()

> `yield()` 方法的目的是让当前线程让出 CPU 使用权，回到就绪状态。但是线程调度器可能会忽略
>
> **只是“建议”操作系统调度器考虑让出 CPU**，并不强制

###### Java线程的状态图

> <img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/javathread-7.png" alt="三分恶面渣逆袭：Java线程状态变化" style="zoom:67%;" />
>
> wait、sleep、join三个方法只要带有等待时间，线程就都会进入超时等待态
>
> 只有Thread.join()和Object.wait()和LockSupport.park()才会进入等待态
>
> 进入等待/超时等待态的线程，都能被notify()/notifyAll()唤醒进入就绪态
>
> 线程等待锁的过程中会进入阻塞态，获得锁后就进入就绪态
>
> ###### 阻塞态和等待态的区别
>
> - BLOCKED是锁竞争失败后被被动触发的状态，WAITING是人为的主动触发的状态
> - BLCKED的唤醒时自动触发的，而WAITING状态是必须要通过特定的方法来主动唤醒

###### 单核 CPU 上运行多个线程效率一定会高吗？

> 不一定，看任务的类型：
>
> **CPU 密集型**：CPU 密集型的线程主要进行计算和逻辑处理，需要占用大量的 CPU 资源 :x:
>
> **IO 密集型**：IO 密集型的线程主要进行输入输出操作，如读写文件、网络通信等，需要等待 IO 设备的响应，而不占用太多的 CPU 资源 :heavy_check_mark:
>
> 对于单核 CPU 来说，如果任务是 CPU 密集型的，那么开很多线程会影响效率；
>
> 如果任务是 IO 密集型的，那么开很多线程会提高效率

###### 虚拟线程 —— JDK21一个重量级更新

> 虚拟线程（Virtual Thread）是 **JDK** （而不是 OS） 实现的**轻量级线程**，由 JVM 调度。
>
> 多个虚拟线程共享同一个平台线程，一个平台线程对应一个操作系统线程，虚拟线程的数量可以远大于操作系统线程的数量
>
> ![虚拟线程、平台线程和系统内核线程的关系](https://oss.javaguide.cn/github/javaguide/java/new-features/virtual-threads-platform-threads-kernel-threads-relationship.png)
>
> 图中的Platform Thread（平台线程）就是我们平常使用的Thread，跟内核级线程是一对一的。
>
> 而多个虚拟线程由一个平台线程来管理，一个平台线程可以在不同的时间执行不同的虚拟线程，当某个虚拟线程被阻塞或等待时，平台线程可以切换到执行另一个虚拟线程
>
> **创建**：
>
> `Thread.ofVirtual().start()`
>
> `Thread.startVirtualThread(customThread)`
>
> 
>
> 在密集 IO 的场景，虚拟线程可以大幅提高线程的执行效率，减少线程资源的创建以及上下文切换

###### 守护线程 Daemon Thread

> 守护线程是一种**在后台运行**的线程，通常用于执行一些后台任务，例如GC、日志记录等
>
> 守护线程 vs. 用户线程
>
> ​	它的生命周期**依赖于用户线程**，当**所有用户线程结束时，JVM 也会自动退出**，不会等待守护线程完成

###### Java创建线程的方法

> ①new Thread().start()  并重写run()方法
>
> ②实现Runnable接口，重写run()方法，作为Thread(Runnable r)参数传入
>
> ③实现Callable接口，重写call()方法，作为Thread(Callable c)参数传入

###### 启动一个Java程序后，至少有哪几个线程？

> ①main()函数线程，是程序的入口
>
> ②GC线程，用来垃圾回收
>
> ③JIT线程，用来即时编译
>
> 具体可以在main()函数中打印当前的线程信息
>
> ```
> Thread: Monitor Ctrl-Break (ID=5)
> Thread: Reference Handler (ID=2)   ==> 这个线程是用来处理引用对象的
> Thread: main (ID=1) ==> 主线程 ID=1
> Thread: Signal Dispatcher (ID=4) ==> 信号调度线程，处理来自操作系统的信号，将它们转发给 JVM 进行进一步处理
> Thread: Finalizer (ID=3)  ==> 终结器线程，负责调用对象的 finalize 方法，在被GC之前先调用finalize方法
> ```
>
> 

###### 单例类对象的创建如何保证线程安全？

> 懒汉式：第一次使用的时候才创建
>
> ```java
> class LazySingleton {
>     private static volatile LazySingleton instance;
> 
>     private LazySingleton() {}
> 
>     public static LazySingleton getInstance() {
>         if (instance == null) { // 第一次检查
>             synchronized (LazySingleton.class) {
>                 if (instance == null) { // 第二次检查
>                     instance = new LazySingleton();
>                 }
>             }
>         }
>         return instance;
>     }
> }
> ```
>
> 饿汉式：类加载的时候创建
>
> ```java
> class Singleton {
>     private static final Singleton instance = new Singleton();
> 
>     private Singleton() {
>     }
> 
>     public static Singleton getInstance() {
>         return instance;
>     }
> }
> ```
>
> 





##### 并发容器

###### Map => ConcurrentHashMap :star:

> `HashMap`: 非线程安全
>
> `Hashtable`: 线程安全，它通过对**每个方法加锁**来确保在并发情况下不会发生数据不一致的问题。因此性能较差，已不推荐使用
>
> `ConcurrentHashMap`: 线程安全，性能更优，适合高并发场景。它的线程安全性方式不同于 `Hashtable`。它通过**分段锁**（Segment Locking）或**桶级锁**来提高并发性能，即允许多个线程同时**读写不同的部分**，而不是对整个 map 进行全局加锁

ConcurrentHashMap实现线程安全的方法：

> **JDK1.7**：
>
> `ConcurrentHashMap` 对整个桶数组进行了**分割分段**(`Segment`，分段锁)，每一把锁只锁容器其中一部分数据，多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率
>
> ​										**Segment 数组 + HashEntry 数组 + 链表**
>
> `Segment`继承了`ReentrantLock`，Segment数组不可扩容，默认就为16，只能有16个线程并发写
>
> <img src="https://oss.javaguide.cn/github/javaguide/java/collection/java7_concurrenthashmap.png" alt="Java7 ConcurrentHashMap 存储结构" style="zoom:80%;" />
>
> **JDK1.8**：
>
> 摒弃了 `Segment` 的概念，而是直接用 `Node` 数组+链表/红黑树的数据结构来实现，并发控制使用 `synchronized` 和 CAS 来操作
>
> 每次只锁一个链表or红黑树的首节点，所以并发度为Node数组的大小
>
> ​																**Node 数组 + 链表 / 红黑树**
>
> <img src="https://oss.javaguide.cn/github/javaguide/java/collection/java8_concurrenthashmap.png" alt="Java8 ConcurrentHashMap 存储结构" style="zoom:80%;" />
>
>  Node 节点中，value 和 next 都是 **volatile** 的，这样就可以保证对 value 或 next 的更新会被其他线程立即看到

ConcurrentHashMap不支持`null`，存在二义性

ConcurrentHashMap 效率比 Hashtable 高的原因：

> **①** Hashtable 在任何时刻**只允许一个线程**访问整个 Map，是通过对整个 Map 加锁来实现线程安全的
>
> ​    `get` 和 `put` 方法，是直接在方法上加的 synchronized 关键字
>
> **②** 而 ConcurrentHashMap 在 JDK 8 中是采用 **CAS + synchronized** 实现的，仅在必要时加锁
>
> ​	`put` 的时候优**先使用 CAS 尝试**插入，如果**失败再使用 synchronized** 代码块加锁。
>
> ​	`get` 的时候是**完全无锁**的，因为 **key 和 value 是 volatile 变量 修饰的**，保证了内存可见



###### ArrayList => CopyOnWriteArrayList

> **写时复制**：当需要修改 `CopyOnWriteArrayList` 的内容时，不会直接修改原数组，而是会先创建底层数组的副本，**对副本数组进行加锁**（ReentrantLock）后修改，修改完之后再将修改后的数组赋值回去（即数组对象指向该副本），这样就可以保证写操作不会影响读操作了
>
> 允许并发读，读操作是无锁的，内部使用 **volatile** 变量来修饰数组 array，以保证读操作的内存可见性

> 也可以使用 `Collections.synchronizedList()` 方法，它可以返回一个线程安全的 List
>
> ```java
> SynchronizedList list = Collections.synchronizedList(new ArrayList());
> ```
>
> 内部是通过synchronized 关键字加锁来实现的



###### Queue => BlockingQueue （阻塞队列）

阻塞队列使用 ReentrantLock + Condition 来确保并发安全

BlockingQueue 的实现类有很多，比如说 ArrayBlockingQueue、PriorityBlockingQueue 等

| 实现类                    | 数据结构                      | 是否有界                           | 特点                                       |
| ------------------------- | ----------------------------- | ---------------------------------- | ------------------------------------------ |
| **ArrayBlockingQueue**    | 数组                          | ✅ 有界                             | 基于数组，固定容量，FIFO                   |
| **LinkedBlockingQueue**   | 链表                          | ✅ 可有界（默认 Integer.MAX_VALUE） | 基于链表，吞吐量比 ArrayBlockingQueue 高   |
| **PriorityBlockingQueue** | 堆（优先队列）                | ❌ 无界                             | 元素按优先级排序（非 FIFO）                |
| **DelayQueue**            | 优先队列（基于 Delayed 接口） | ❌ 无界                             | 元素到期后才能被取出                       |
| **SynchronousQueue**      | 无缓冲                        | ✅ 容量为 0                         | 必须一对一交换数据，适用于高吞吐的任务提交 |
| **LinkedTransferQueue**   | 链表                          | ❌ 无界                             | 支持 tryTransfer()，数据立即交给消费者     |



##### 线程池  :star:

###### Executor三大构件

> **任务**：Runnable / Callable
>
> **执行器**： Executor <-  ExecutorService  <- ThreadPoolExecutor
>
> **结果**：Future <- FutureTask
>
> <img src="https://javaguide.cn/assets/Executor%E6%A1%86%E6%9E%B6%E7%9A%84%E4%BD%BF%E7%94%A8%E7%A4%BA%E6%84%8F%E5%9B%BE-8GKgMC9g.png" alt="Executor 框架的使用示意图" style="zoom:80%;" />

###### ThreadPoolExecutor参数

> ```java
> public ThreadPoolExecutor (
>        int corePoolSize,//线程池的核心线程数量
>        int maximumPoolSize,//线程池的最大线程数
>        long keepAliveTime,//当线程数大于核心线程数时，多余的空闲线程存活的最长时间
>        TimeUnit unit,//时间单位
>        BlockingQueue<Runnable> workQueue,//任务队列，用来储存等待执行任务的队列
>        ThreadFactory threadFactory,//线程工厂，用来创建线程，一般默认即可
>        RejectedExecutionHandler handler//拒绝策略，当提交的任务过多而不能及时处理时，我们可以定制策略来处理任务
>    ) {
>        if (corePoolSize < 0 ||
>            maximumPoolSize <= 0 ||
>            maximumPoolSize < corePoolSize ||
>            keepAliveTime < 0)
>            throw new IllegalArgumentException();
>        if (workQueue == null || threadFactory == null || handler == null)
>            throw new NullPointerException();
>        this.corePoolSize = corePoolSize;
>        this.maximumPoolSize = maximumPoolSize;
>        this.workQueue = workQueue;
>        this.keepAliveTime = unit.toNanos(keepAliveTime);
>        this.threadFactory = threadFactory;
>        this.handler = handler;
> }
> ```
>
> **core**PoolSize：当任务队列没满时，线程池最多可以同时运行的线程数
>
> **maximum**PoolSize：当任务队列满时，线程池最多可以同时运行的线程数
>
> workQueue：任务队列，当线程池中有corePoolSize个线程在运行时，新进来的线程就加入workQueue中
>
> keepAliveTime：任务队列中任务最长等待时间，unit为其单位
>
> handler：**拒绝策略**，当线程池满了，任务队列也满了时，进来的线程的处理方式 :star:
>
> * AbortPolicy：抛出`RejectedExecutionException`来拒绝新任务
> * DiscardPolicy：直接丢弃
> * DiscardOldestPolicy：丢弃最早的未被处理的线程
> * CallerRunsPolicy：由当前线程执行任务
> * 也可以自定义
>
> <img src="https://oss.javaguide.cn/github/javaguide/java/concurrent/relationship-between-thread-pool-parameters.png" alt="线程池各个参数的关系" style="zoom: 40%;" />
>
> ![图解线程池实现原理](https://oss.javaguide.cn/github/javaguide/java/concurrent/thread-pool-principle.png)

###### 线程池的任务队列有哪几种阻塞队列？

有界队列 ArrayBlockingQueue；无界队列 LinkedBlockingQueue；优先级队列 PriorityBlockingQueue；延迟队列 DelayQueue；同步队列 SynchronousQueue

<img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/javathread-69.png" alt="三分恶面渣逆袭：线程池常用阻塞队列" style="zoom: 67%;" />

> ArrayBlockingQueue：一个**有界**的先进先出的阻塞队列，底层是一个数组，适合固定大小的线程池
>
> LinkedBlockingQueue：底层是**链表**，如果不指定大小，默认大小是 `Integer.MAX_VALUE`，几乎相当于一个**无界**队列
>
> PriorityBlockingQueue：一个支持**优先级排序**的**无界**阻塞队列。任务按照其自然顺序或 Comparator 来排序
>
> DelayQueue：类似于 PriorityBlockingQueue，由**二叉堆**实现的**无界**优先级阻塞队列，`newScheduledThreadPool()`
>
> SynchronousQueue：每个插入操作必须等待另一个线程的移除操作，同样，任何一个移除操作都必须等待另一个线程的插入操作，`Executors.newCachedThreadPool()`



###### 为什么线程池的创建推荐用ThreadPoolExecutor，而不是工具类Executors？

###### or 为什么不推荐使用内置线程池？

> 因为Executors返回的四种内置的预定义的线程池都存在风险：
>
> ```java
> // 无界队列 LinkedBlockingQueue
> public static ExecutorService newFixedThreadPool(int nThreads) {
>  	return new ThreadPoolExecutor(nThreads, nThreads,0L, TimeUnit.MILLISECONDS,
>                                new LinkedBlockingQueue<Runnable>());
> }
> // 无界队列 LinkedBlockingQueue
> public static ExecutorService newSingleThreadExecutor() {
>      return new FinalizableDelegatedExecutorService (
>          new ThreadPoolExecutor(1, 1,0L, TimeUnit.MILLISECONDS,
>                                 new LinkedBlockingQueue<Runnable>()
>                                )
>  );
> }
> // 同步队列 SynchronousQueue，没有容量，最大线程数是 Integer.MAX_VALUE`
> public static ExecutorService newCachedThreadPool() {
>  	return new ThreadPoolExecutor(0, Integer.MAX_VALUE,60L, TimeUnit.SECONDS
>                                ,new SynchronousQueue<Runnable>());
> }
> // DelayedWorkQueue（延迟阻塞队列）
> public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) {
>  	return new ScheduledThreadPoolExecutor(corePoolSize);
> }
> ```
>
> - `FixedThreadPool` 和 `SingleThreadExecutor`：使用的是无界的 `LinkedBlockingQueue`，任务队列最大长度为 `Integer.MAX_VALUE`,可能堆积大量的请求，从而导致 OOM。
> - `CachedThreadPool`：使用的是同步队列 `SynchronousQueue`, 允许创建的线程数量为 `Integer.MAX_VALUE` ，如果任务数量过多且执行速度较慢，可能会创建大量的线程，从而导致 OOM
> - `ScheduledThreadPool` ：使用的无界的延迟阻塞队列`DelayedWorkQueue`，任务队列最大长度为 `Integer.MAX_VALUE`，可能堆积大量的请求，从而导致 OOM
>
> 这四种内置线程池的任务队列最大长度都为Integer.MAX_VALUE，都容易导致OOM

###### 四种内置的线程池

> 固定大小的线程池 `Executors.newFixedThreadPool(int nThreads);`，适合用于任务数量确定，且对线程数有明确要求的场景。例如，IO 密集型任务、数据库连接池等。其 corePoolSize == maximumPoolSize = nThreads，使用LinkedBlockingQueue
>
> 单线程线程池 `Executors.newSingleThreadExecutor();`，适用于需要按顺序执行任务的场景。例如，日志记录、文件处理等。其 corePoolSize == maximumPoolSize = 1， 使用 LinkedBlockingQueue 作为阻塞队列
>
> 缓存线程池 `Executors.newCachedThreadPool();`，适用于短时间内任务量波动较大的场景。例如，短时间内有大量的文件处理任务或网络请求。其 corePoolSize = 0，maximumPoolSize = Integer.MAX_VALUE，使用SynchronousQueue ，根据SynchronousQueue的特性，每次插入需要等待另一个线程移除
>
> 定时任务线程池 `Executors.newScheduledThreadPool(int corePoolSize);`，适用于需要定时执行任务的场景。例如，定时发送邮件、定时备份数据等



###### 为什么推荐用Executor.submit()来创建线程，而不是new Thread().start()呢？

> 避免**this逃逸问题**：在对象构造过程中，`this` 引用被外部访问，导致该对象在完全初始化之前被其他线程使用，可能引发线程安全问题或者导致使用未完全初始化的对象
>
> ```java
> public class ThisEscape {
>     private final int value;
> 
>     public ThisEscape() {
>         new Thread(() -> {
>             System.out.println("Value: " + ThisEscape.this.value);
>         }).start(); // 这里可能访问到尚未初始化的 value
> 
>         value = 42; // 这行代码可能在新线程执行后才执行
>     }
> 
>     public static void main(String[] args) {
>         new ThisEscape();
>     }
> }
> ```
>
> 

###### Runnable vs. Callable

> ①返回值：Runnable接口不会返回结果，Callable接口会
>
> ②抛出异常：Runnable接口不会抛出检查异常，Callable接口会
>
> 工具类 `Executors` 可以实现将 `Runnable` 对象转换成 `Callable` 对象

###### submit()  vs. execute()

> ①返回值：submit()有返回值，返回一个 `Future` 对象，调用get()
>
> ​				 execute()用于提交不需要返回值的任务，一般用于提交Runnable
>
> ②异常处理：submit方法可以通过返回的Future对象处理任务执行过程中抛出的异常
>
> ​					execute() 方法的异常处理需要通过自定义的 ThreadFactory的UncaughtExceptionHandler

###### shutdown() vs. shutdownNow()

> shutdown()：关闭线程池，线程池的状态变为 `SHUTDOWN`。线程池不再接受新任务了，但是队列里的**任务得执行完毕**
>
> shutdownNow()：关闭线程池，线程池的状态变为 `STOP`。线程池会**终止当前正在运行的任务**，并停止处理排队的任务并返回正在等待执行的 List
>
> Terminated状态必须是先Tidying（线程池空闲），才能到达Terminated状态
>
> ![三分恶面渣逆袭：线程池状态切换图](https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/javathread-78.png)

###### 线程池中线程异常后，销毁还是复用？

> **使用`execute()`提交任务**：当任务通过`execute()`提交到线程池并在执行过程中抛出异常时，如果这个异常没有在任务内被捕获，那么该异常会导致当前线程终止，并且<u>异常会被打印到控制台或日志文件中</u>。线程池会检测到这种线程终止，并**创建一个新线程来替换它**，从而保持配置的线程数不变
>
> **使用`submit()`提交任务**：对于通过`submit()`提交的任务，如果在任务执行中发生异常，这个<u>异常不会直接打印出来</u>。相反，异常会被封装在由`submit()`返回的`Future`对象中。当调用`Future.get()`方法时，可以捕获到一个`ExecutionException`。在这种情况下，线程不会因为异常而终止，它会继续存在于线程池中，准备执行后续的任务
>
> 简单来说：
>
> 使用`execute()`时，未捕获异常导致线程终止，线程池创建**新线程替代**
>
> 使用`submit()`时，异常被封装在`Future`中，线程继续**复用**



##### 悲观锁 :star:

> 悲观锁总是假设最坏的情况，认为共享资源每次被访问的时候就会出现问题，所以每次在获取资源操作的时候都会上锁
>
> **共享资源每次只给一个线程使用**，其它线程阻塞，用完后再把资源转让给其它线程
>
> `sychronized`和`ReentrantLook`都是悲观锁

###### synchronized

> **使用方式**：
>
> 1. 修饰**实例方法**   `synchronized void method() {    //业务代码 }`
> 2. 修饰**静态方法**   `synchronized static void method() {    //业务代码 }`
> 3. 修饰**代码块**      `synchronized(object or 类.class) {    //业务代码 }`
>
> 修饰静态方法或者synchronzied(Object.class)都是给类上锁
>
> 修饰实例方法或者synchronzied(this)都是给对象上锁
>
> :bell:**构造方法**本身是线程安全，不可以加synchronized，但是构造方法内可以使用synchronized

**底层实现**： :star:

> 1.`synchronized` 修饰**代码块**的实现使用的是 `monitorenter` 和 `monitorexit` 指令，其中 `monitorenter` 指令指向同步代码块的开始位置，`monitorexit` 指令则指明同步代码块的结束位置
>
> * 执行`monitorenter`时，会尝试获取对象的锁
>   * 如果锁的计数器为 0 则表示锁可以被获取，获取后将锁计数器设为 1 也就是加 1
>   * 如果获取对象锁失败，那当前线程就要阻塞等待，直到锁被另外一个线程释放为止
> * 执行 `monitorexit` 指令后，将锁计数器设为 0，表明锁被释放，其他线程可以尝试获取锁
>
> 每个对象的内存分块的**对象头**中就有一个`Mark Word`，其中就有**锁状态标记**以及**monitor对象的指针**
>
> <img src="https://cdn.tobebetterjavaer.com/stutymore/javathread-20250209115813.png" alt="博客园Zebt：Java 对象头" style="zoom:67%;" />
>
> synchronized 依赖对象头的 Mark Word 进行状态管理，支持无锁、偏向锁、轻量级锁，以及重量级锁
>
> Monitor对象结构如下：
>
> ```cpp
> ObjectMonitor() {
>     _count        = 0;     // 当前线程获取该锁的次数
>     _owner        = NULL;  // 指向持有ObjectMonitor对象的线程ID
>     _WaitSet      = NULL;  // 处于wait状态的线程，会被加入到_WaitSet
>     _cxq          = NULL ;
>     _EntryList    = NULL ;  // 处于等待锁block状态的线程，会被加入到该列表
> }
> ```
>
> 
>
> 2.`synchronized` 修饰的**方法**并没有 `monitorenter` 指令和 `monitorexit` 指令，取而代之的是 `ACC_SYNCHRONIZED` 标识，该标识指明了该方法是一个同步方法。JVM 通过该 `ACC_SYNCHRONIZED` 访问标志来辨别一个方法是否声明为同步方法，从而执行相应的同步调用。
>
> * 如果修饰的是**实例方法**，JVM 会尝试获取实例**对象的锁**
> * 如果修饰的是**静态方法**，JVM 会尝试获取当前 **class 的锁**
>
> 不过，两者的本质都是对**对象监视器 monitor** 的获取

**synchornized锁升级机制**：

> 没有线程竞争时，就使用低开销的“偏向锁”，此时没有额外的 CAS 操作
>
> 轻度竞争时，使用“轻量级锁”，采用 CAS 自旋，避免线程阻塞
>
> 只有在重度竞争时，才使用“重量级锁”，由 Monitor 机制实现，需要线程阻塞
>
> <img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/javathread-34.png" alt="三分恶面渣逆袭：Mark Word变化" style="zoom:67%;" />
>
> **四种锁状态**：
>
> ①无锁状态：对象未被任何线程锁定
>
> ​	此时来了一个线程A，尝试获取对象的锁...
>
> ②偏向锁：当线程A**第一次**获取锁时，会进入偏向模式。对象头中的 Mark Word 会记录线程 ID，锁类型标志改为01，偏向锁标志改为1，后续同一线程（A）再次获取锁时，可以直接进入 synchronized 加锁的代码，无需额外加锁
>
> ​	在A未释放锁时，来了线程B，尝试获取对象的锁...
>
> ③轻量级锁：当没其他线程同线程B竞争锁时，JVM 会采用轻量级锁来避免线程阻塞，未持有锁的线程（B）通过**CAS自旋**等待锁释放
>
> ​	将对象的MarkWord存储到线程的**虚拟机栈**上，然后通过CAS将对象的MarkWord的内容设置为指向Displaced Mark Word的指针，如果设置成功则获取锁
>
> ④重量级锁：如果该线程（B）自旋超过一定的次数，或者一个线程持有锁，一个自旋，又有第三个线程（C）进入 synchronized 加锁的代码时，轻量级锁就会升级为重量级锁。此时 JVM 会在**操作系统层面**创建一个互斥锁——Mutex，所有进一步尝试获取该锁的线程将会被**阻塞**，直到锁被释放
>
> ![三分恶面渣逆袭：锁升级简略过程](https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/javathread-36.png)

**synchronized的优化**：

> **锁升级**：如上
>
> **锁消除**：JIT 可以在运行时进行代码分析，如果发现某些锁操作不可能被多个线程同时访问，就会对这些锁进行消除，从而减少上锁开销

###### ReentrantLock可重入锁

> ```java
> public class ReentrantLock implements Lock, java.io.Serializable {}
> ```
>
> ReentrantLock加锁和释放锁等操作主要依赖于它的**成员变量`Sync`**来实现
>
> `Sync`是ReentrantLock的一个**内部类**，它实现了`AbstractQueuedSynchronizer`接口
>
> 使用CAS尝试获得锁，如果失败，加入AQS等待队列中
>
> `Sync`由两种类型，分别为**公平锁**和**非公平锁**，由创建ReentrantLock是传入的`fair`参数决定，默认为**非公平**
>
> ```java
> public ReentrantLock(boolean fair) {
>     sync = fair ? new FairSync() : new NonfairSync();
> }
> ```
>
> > **公平锁** : 先到先得，性能较差，因为公平锁为了保证时间上的绝对顺序，上下文切换更频繁
> >
> > **非公平锁**：锁被释放之后，后申请的线程可能会先获取到锁，是随机或者按照其他优先级排序的。性能更好，但可能会导致某些线程永远无法获取到锁
>
> 
>
> **使用**：
>
> ```java
> private Lock lock = new ReentrantLock();
> lock.lock();
> try {
>    // 需要同步的操作
> } finally {
>     lock.unlock();
> }
> ```
>
> ReenterLock如何实现可重入：
>
> ​	ReenterLock中的**volatile**变量`state`表示资源的状态，0为空闲，1为被占有
>
> ​	如果一个线程要请求资源，先判断`state`是否为0，是的话直接加锁请求请求成功，否则就需要等待
>
> ​	如果一个线程已经对资源加锁，那么它可以重复获取这个锁，并且每次获取**`state`都得加1**
>
> ​	所以一个线程必须**释放与获取的次数相同**的锁，才能让 `state` 的值回到 0
>
> ReetrantLock尝试申请独占锁：
>
> ```java
> // ReentrantLock
> final boolean nonfairTryAcquire(int acquires) {
>     final Thread current = Thread.currentThread();
>     // 1、获取 AQS 中的 state 状态
>     int c = getState();
>     // 2、如果 state 为 0，证明锁没有被其他线程占用
>     if (c == 0) {
>         // 2.1、通过 CAS 对 state 进行更新
>         if (compareAndSetState(0, acquires)) {
>             // 2.2、如果 CAS 更新成功，就将锁的持有者设置为当前线程
>             setExclusiveOwnerThread(current);
>             return true;
>         }
>     }
>     // 3、如果当前线程和锁的持有线程相同，说明发生了「锁的重入」
>     else if (current == getExclusiveOwnerThread()) {
>         int nextc = c + acquires;
>         if (nextc < 0) // overflow
>             throw new Error("Maximum lock count exceeded");
>         // 3.1、将锁的重入次数加 1
>         setState(nextc); // 这里直接setState就可以 不需要compareAndSetState
>         return true;
>     }
>     // 4、如果锁被其他线程占用，就返回 false，表示获取锁失败
>     return false;
> }
> ```
>
> ReenterLock尝试释放独占锁
>
> ```java
> // ReentrantLock
> protected final boolean tryRelease(int releases) {
>     int c = getState() - releases;
>     // 1、判断持有锁的线程是否为当前线程
>     if (Thread.currentThread() != getExclusiveOwnerThread())
>         throw new IllegalMonitorStateException();
>     boolean free = false;
>     // 2、如果 state 为 0，则表明当前线程已经没有重入次数。因此将 free 更新为 true，表明该线程会释放锁。
>     if (c == 0) {
>         free = true;
>         // 3、更新持有资源的线程为 null
>         setExclusiveOwnerThread(null);
>     }
>     // 4、更新 state 值
>     setState(c);
>     return free;
> }
> ```
>
> ​	只有state=0，free才能更新为true，并返回，表示完全释放成功
>
> ​	如果state!=0，那么free仍未false，更新state值

**ReetrantLock vs. synchronized**  :star:

①两者都是**可重入锁**

②synchronized 依赖于 **JVM** ； ReentrantLock 依赖于 **AQS** 提供的API

 底层：对象头中的monitor对象；Unsafe的CAS操作和park/unpark操作（Parker对象）

③synchronized 自动加锁释放锁；ReentrantLock 需要手动`lock()`和`unlock()`

④synchronized 是**非公平锁**；ReentrantLock 既可以是公平锁，也可以是非公平锁（通过构造函数的fair参数决定）

⑤synchronized是**不可中断锁** ； ReentrantLock是**可中断锁**（通过lock.lockInterruptibly()操作来实现）

⑥ReenterLock实现了一些高级功能：

* 等待**可中断**：等待锁的过程中可以执行其他事情，不需要一直阻塞到获得锁， `lock.lockInterruptibly()`
* 可实现选择性通知（锁可以绑定多个条件）：通过`Condition`接口
* 支持超时：**`tryLock(timeout)`**，超时时间内获得锁，否则失败，不会一直等待

> **可中断锁**：获取锁的过程中可以被中断，不需要一直等到获取锁之后 才能进行其他逻辑处理。`ReentrantLock` 就属于是可中断锁
>
> **不可中断锁**：一旦线程申请了锁，就只能等到拿到锁以后才能进行其他的逻辑处理。 `synchronized` 就属于是不可中断锁

并发量大的情况下，选择ReentrantLock：

- ReentrantLock 提供了超时和公平锁等特性，可以应对更复杂的并发场景
- ReentrantLock 允许更细粒度的锁控制，能有效减少锁竞争
- ReentrantLock 支持条件变量 Condition，可以实现比 synchronized 更友好的线程间通信机制

##### 乐观锁 :star:

> 乐观锁总是假设最好的情况，认为共享资源每次被访问的时候不会出现问题，线程可以不停地执行，无需加锁也无需等待，只是在提交修改的时候去验证对应的资源（也就是数据）是否被其它线程修改了
>
> 实现：**版本号** or **CAS（Compare and Swap）**

###### 版本号机制

> 一般是在数据表中加上一个数据版本号 `version` 字段，表示数据被修改的次数。当数据被修改时，`version` 值会加一
>
> 当线程 A 要更新数据值，在读取原始数据的同时也会读取 `version` 值，在提交更新时，若刚才读取到的 `version` 值与当前数据库中的 `version` 值相等才更新（表示数据没有被其他线程修改），否则重试更新操作，直到更新成功

###### CAS：Compare And Swap :star:

> 用一个**预期值**和**要更新的变量值**进行比较，两值**相等**才会进行更新，否则重试直到成功
>
> CAS是一个**原子操作**，涉及到三个操作数：
>
> - **V**：要更新的变量值(Var)，即要执行更新操作的前一刻，这个变量现在的值是多少
> - **E**：预期值(Expected)，一般就是更新变量之前变量的原始值
> - **N**：拟写入的新值(New)
>
> 只有当更新的那一刻变量的值`V`和更新变量之前变量的原始值`E`相等，表明没有被其他线程修改，才可以进行更新否则，将E更新为V，重新进行
>
> **问题**：
>
> ① CAS 经常会用到**自旋操作**来进行**重试**，如果长时间不成功，会给 CPU 带来非常大的执行开销  ==> 多次失败阻塞
>
> ② ABA问题 ==> 加版本号 或 使用 AtomicStampedReference
>
> ③ 只能操作一个变量 ==> 可以将多个变量封装为一个对象，使用 AtomicReference 进行 CAS 更新
>
> Java的CAS底层由`Unsafe`类来实现：
>
> :bell:`Unsafe`类提供了`compareAndSwapObject`、`compareAndSwapInt`、`compareAndSwapLong`方法来实现的对`Object`、`int`、`long`类型的 CAS 操作

###### 什么是ABA问题？如何解决？

> **ABA问题**是指在并发编程中，某个线程在执行**CAS**操作时，可能会遇到一个已经被其他线程修改过的值，而在该线程判断时**看起来值没有变化**，导致错误的结果
>
> 在使用CAS操作时，系统会检查**某个内存位置的值**是否与**预期值**相等。如果相等，则修改该内存位置的值。如果不等，则CAS操作失败，并且可以重试这个操作。问题出在，如果在判断值相等后，另一个线程已经修改过该值，然后又将其恢复成原来的值，这样CAS操作就会“误认为”值没有变化，导致错误地执行修改操作

> ABA 问题的解决思路是在变量前面追加上**版本号或者时间戳**
>
> AtomicStampedReference类就是用来解决ABA问题的，其`compareAndSet()` 方法就是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果**全部相等**，则以原子方式将该引用和该标志的值设置为给定的更新值
>
> ```java
> public boolean compareAndSet(V expectedReference, V newReference,
>                        int expectedStamp, int newStamp);
> ```

###### Atomic原子类

> Atomic原子类依赖CAS来实现，底层通过Unsafe类来进行CAS操作
>
> AtomicInteger、AtomicLong、AtomicBoolean、...、 AtomicIntegerArray、AtomicLongArray
>
> 分析`AtomicInteger`:
>
> ```java
> //获取当前的值
> public final int get() 
> //获取当前的值，并设置新的值
> public final int getAndSet(int newValue)
> //获取当前的值，并自增
> public final int getAndIncrement()
> //获取当前的值，并自减
> public final int getAndDecrement() 
> //获取当前的值，并加上预期的值
> public final int getAndAdd(int delta) 
> //如果输入的数值等于预期值，则以原子方式将该值设置为输入值（update）
> boolean compareAndSet(int expect, int update)
> //最终设置为newValue, lazySet 提供了一种比 set 方法更弱的语义，可能导致其他线程在之后的一小段时间内还是可以读到旧的值，但可能更高效
> public final void lazySet(int newValue)
> ```
>
> AtomicInteger有三个成员变量：
>
> * 一个Unsafe对象
> * 一个**value**值
> * 一个值偏移量**valueOffset**，因为unsafe是通过offset来定位value字段所在的位置的，直接修改对应内存
>
> ```java
> // 获取 Unsafe 实例
> private static final Unsafe unsafe = Unsafe.getUnsafe();
> private volatile int value;
> private static final long valueOffset; // 不变的
> static {
>     try {
>         // 获取“value”字段在AtomicInteger类中的内存偏移量
>         valueOffset = unsafe.objectFieldOffset
>             (AtomicInteger.class.getDeclaredField("value"));
>     } catch (Exception ex) { throw new Error(ex); }
> }
> ```
>
> 对于`compareAndSet(e, u)`方法，通过unsafe.compareAndSwapInt()来实现
>
> ```java
> public final boolean compareAndSet(int expect, int update) {
>     return unsafe.compareAndSwapInt(this, valueOffset, expect, update);
> }
> ```
>
> 对于getAndAdd(val)方法，通过unsafe.getAndAddInt()来实现
>
> 而unsafe的getAndInt方法则是通过不断地进行CAS操作，直到变量值修改成功（自旋）
>
> ```java
> // 原子地将当前值减 1 并返回减之前的值（旧值）
> public final int getAndAdd(int delta) {
>     return unsafe.getAndAddInt(this, valueOffset, delta);
> }
> 
> 
> // 原子地获取并增加整数值
> public final int getAndAddInt(Object o, long offset, int delta) {
>     int v;
>     do {
>         // 以 volatile 方式获取对象 o 在内存偏移量 offset 处的整数值
>         v = getIntVolatile(o, offset);
>     } while (!compareAndSwapInt(o, offset, v, v + delta));
>     // 返回旧值
>     return v;
> }
> ```
>
> 



###### 为什么 LongAdder 在高并发场景下会比 AtomicInteger 和 AtomicLong 的性能更好？

> LongAdder、AtomicInteger、AtomicLong都是**atomic包**下的**原子变量类**，基于乐观锁**CAS**实现的
>
> LongAdder性能好主要是因为它通过内部使用多个变量（桶）来分担竞争，从而减少了性能瓶颈。是一种以**空间换时间**的思想，其工作原理如下：
>
> * `LongAdder` 采用了**分桶**的方式来减少竞争，它内部维护了一个由多个 `Cell`（桶）组成的数组。每个线程在操作时，通常会将数据写到自己的桶中，而不是直接写入到单一的变量。
> * 只有在最终需要获取总和时，`LongAdder` 才会汇总各个桶的值。这样，多个线程可以并行地更新自己的桶，减少了锁竞争，提高了性能。
>
> `AtomicLong` 和 `AtomicInteger`在低并发场景下，它们可以提供较好的性能，但在高并发时，由于CAS的冲突和重试，性能可能会下降。也可能会引发“**ABA 问题**”



##### volatile关键字 :star:

###### 作用1：保证变量可见性

> `volatile` 关键字可以保证变量的可见性，这个变量是共享且不稳定的，每次使用它都直接到**主存中进行读取**
>
> Java中每个线程有自己的**共享变量缓存**，对于普通变量，线程一般都会到缓存中读取变量值
>
> 而volatile变量可以禁用缓存，每次都到主存中读取变量值
>
> <img src="https://oss.javaguide.cn/github/javaguide/java/concurrent/jmm2.png" alt="JMM(Java 内存模型)强制在主存中进行读取" style="zoom: 67%;" />
>
> **写操作**：
>
> 每次对一个volatile变量修改后，会插入一个**写屏障指令**，强制将其值写入主内存中
>
> ```
> StoreStore;   // 保证写入之前的操作不会重排
> volatile_write(); // 写入 volatile 变量
> StoreLoad;    // 保证写入后，其他线程立即可见
> ```
>
> <img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/javathread-28.png" alt="三分恶面渣逆袭：volatile写插入内存屏障后生成的指令序列示意图" style="zoom: 67%;" />
>
> **读操作**：
>
> 当线程对 volatile 变量进行读操作时，JVM 会插入一个**读屏障指令**，这个指令会强制让本地内存中的变量值失效，从而重新从主内存中读取最新的值
>
> <img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/javathread-29.png" alt="三分恶面渣逆袭：volatile写插入内存屏障后生成的指令序列示意图" style="zoom:67%;" />

###### 作用2：防止指令重排序

> 变量声明为 `volatile` 的变量在进行读写操作的时候，会通过插入特定的 **内存屏障** 的方式来**禁止指令重排序**
>
> 对 `volatile` 变量的写入不会被重排序到它之前的代码
>
> - StoreStore 屏障 可以禁止普通写操作与 volatile 写操作的重排
> - StoreLoad 屏障 会禁止 volatile 写与 volatile 读重排
> - LoadLoad 屏障 会禁止 volatile 读与后续普通读操作重排
> - LoadStore 屏障 会禁止 volatile 读与后续普通写操作重排
>
> `Unsafe`类也提供了内存屏障的实现，可以用它来达到`volatile`的效果

###### volatile修饰基本数据类型变量和引用变量

> 当 `volatile` 用于基本数据类型时，能确保该变量的读写操作是直接从主内存中读取或写入的
>
> 当 `volatile` 用于引用类型时，能确保引用本身的可见性，即**确保引用指向的对象地址是最新的**
>
> ​	BUT：不能确保引用对象内部是线程安全的，需要借助锁 ==> 可以看CopyOnWriteArrayList的读写操作

###### volatile vs. synchronized

> 1.volatile只能保证变量的可见性，但不能保证数据的**原子性**（可以保证对该变量的操作不进行指令重排序）
>
>   synchronized都可以保证
>
> 2.volatile性能肯定比synchronized关键字要好
>
> 3.volatile关键字只能用于变量；而 synchronized关键字可以修饰方法以及代码块



##### ThreadLocal变量 :star:

`ThreadLocal` 类允许每个线程绑定自己的值，让每个线程都有自己的**专属本地变量**

###### 应用：

可以使用 `ThreadLocal` 存储数据库连接对象，每个线程有自己独立的数据库连接，从而避免了多线程竞争同一数据库连接的问题

在分布式系统中，`ThreadLocal` 可以存储 `TraceId`，用于链路跟踪

###### 项目中的应用：

在我的项目中，每个 HTTP 请求由一个线程处理，可以用 `ThreadLocal` 绑定用户信息UserContext

因为在服务层和持久层也要用到用户信息，就可以在**控制层拦截请求**把**用户信息**存入 ThreadLocal

###### 底层数据结构

> 每个线程Thread都有一个ThreadLocalMap，其中key为ThreadLocal的**弱引用**，value为ThreadLocal变量的**强引用**
>
> <img src="https://javaguide.cn/assets/2-CFHd4NU8.png" alt="img" style="zoom:67%;" />
>
> 每个线程在读写ThreadLocal变量时都是在自己的map中进行，从而实现**线程隔离**
>
> ![img](https://javaguide.cn/assets/6-DAaW6e2T.png)
>
> `ThreadLocal.get(key)`方法：从当前Thread的ThreadLocalMap中调用get获取
>
> `ThreadLocal.set(key, value)`方法：也是调用ThreadLocalMap的set方法

###### ThreadLocalMap的Hash算法、hash冲突以及扩容机制

> ThreadLocalMap 虽然是一个Map，但没有实现 Map 接口，是一个简单的**线性探测哈希表**
>
> **Hash算法**：
>
> ```java
> int i = key.threadLocalHashCode & (len-1)
> ```
>
> 关键：**key.threadLocalHashCode**
>
> ThreadLocal中维护了一个`nextHashCode`变量，用来赋值给下一个ThreadLocal变量的hash值
>
> 而nextHashCode的更新逻辑为：当前ThreadLocal的hash值 + `HASH_INCREMENT`
>
> `HASH_INCREMENT `也是ThreadLocal中的变量，固定为**0x61c88647**
>
> 这个值为黄金分割数/斐波那契数，它可以使哈希分布非常均匀
>
> 
>
> **Hash冲突**：开放定址法
>
> * 通过`hash`计算后的槽位对应的`Entry`数据为空，直接插入
> * 对应槽位不为空，并且该槽位key与当前的key相同，那么覆盖
> * 对应槽位不为空，并且key不一样，那么往后遍历找到**槽为空**的或者**key过期**的或者**key一样**的，覆盖
>
> ThreadLocalMap 设计的目的是存储线程私有数据，数据量并不大，不需要用到链表或者红黑树，减少维护成本
>
> 
>
> **扩容**：当前散列数组中`Entry`的数量已经达到了列表的扩容阈值`(threhold = len * 2/3)`，调用`rehash()`
>
> ```java
> private void rehash() {
>     // 清理被 GC 回收的 key
>     expungeStaleEntries();
> 
>     //扩容
>     if (size >= threshold - threshold / 4)
>         resize();
> }
> ```
>
> ​	在`rehash()`中，不会立即扩容，而是先GC，如果填充率还是大于3/4再扩容
>
> ​	先进行**探测式清除**过期key，清除之后，如果`Entry`的数量还是大于 `threshold * 3/4`，那么进行扩容
>
> 
>
> **ThreadLocalMap vs. HashMap**
>
> 相同点：初始容量为16，扩容是2倍扩容，最大容量为2^30，
>
> 不同点：阈值不同：ThreadLocalMap的扩容阈值是`len * 2/3`，而HashMap的扩容阈值为`len * 3/4`
>
> ​			 扩容机制不同，ThreadLocalMap先GC再扩容，HashMap直接扩容

###### ThreadLocal内存泄漏问题

原因：

> `ThreadLocalMap` 的 key 和 value 引用机制：
>
> - **key 是弱引用**：`ThreadLocalMap` 中的 key 是 `ThreadLocal` 的弱引用 (`WeakReference<ThreadLocal<?>>`)。 这意味着，如果 `ThreadLocal` 实例不再被任何强引用指向，垃圾回收器会在下次 GC 时回收该实例，导致 `ThreadLocalMap` 中对应的 key 变为 null
> - **value 是强引用**：`ThreadLocalMap` 的 value 是某个局部变量的强引用，即使 key 被回收（变为 null），value 仍然存在于 `ThreadLocalMap` 中，被强引用，不会被回收
>
> 当 `ThreadLocal` 实例失去强引用后，其对应的 value 仍然存在于 `ThreadLocalMap` 中，因为 `Entry` 对象强引用了它。如果线程持续存活（例如线程池中的线程），`ThreadLocalMap` 也会一直存在，导致 key 为 null 的 entry 无法被垃圾回收，机会造成内存泄漏
>
> 引用关系如下：
>
> - ThreadLocal 强引用 -> ThreadLocal 对象。
> - Thread 强引用 -> ThreadLocalMap。
> - ThreadLocalMap[i] 强引用了 -> Entry。
> - Entry.key 弱引用 -> ThreadLocal 对象。
> - Entry.value 强引用 -> 线程的局部变量对象

如何避免？

> 在使用完 `ThreadLocal` 后，务必调用 `remove()` 方法。 这是最安全和最推荐的做法。
>
> `remove()` 方法会从 `ThreadLocalMap` 中显式地移除所有key为 null 的 entry
>
> 对key为null的entry进行`get()`和`set()`也会将其清除

###### 过期Key的清除

> **探测式清理**：
>
> ​	遍历散列数组，从开始位置向后探测清理过期数据，将过期数据的`Entry`设置为`null`，沿途中碰到未过期的数据则将此数据`rehash`后重新在`table`数组中定位，如果定位的位置已经有了数据，则会将未过期的数据放到**最靠近此位置**的`Entry=null`的桶中，使`rehash`后的`Entry`数据距离正确的桶的位置更近一些
>
> ​	经过一轮探测式清理后，key过期的数据会被清理掉，没过期的数据经过`rehash`重定位后所处的桶位置理论上更接近`i= key.hashCode & (tab.len - 1)`的位置

###### InheritableThreadLocal

> 用于共享父子线程的ThreadLocal变量，在父线程new Thread()子线程时，会在Thread的构造函数中调用**init**方法，其中就会将父线程的InheritableThreadLocal拷贝给子线程
>
> BUT：在线程池中线程是复用的，不会重新继承 `ThreadLocal` 变量，所以会导致数据错乱
>
> ==> 终极版：**TransmitiableThreadLocal**，并且还可以跨线程池传递值
>
> * 拦截 `ThreadLocal` 变量
> * 在任务提交前保存变量值
> * 在任务执行时恢复变量值
> * 执行完后清理变量，防止数据污染

###### 为什么ThreadLocal的key要设置为弱引用？

> ThreadLocal 的 key 是弱引用，是为了让 GC 能及时回收不再使用的 ThreadLocal 实例，进而清理掉对应的数据，减少泄漏的风险。当外部不再持有 ThreadLocal 实例的强引用时，GC 可以自动回收这个 ThreadLocal 实例
>
> 这样在 `ThreadLocalMap` 中就能发现 key == null 的 entry，在下一次访问（get/set/remove）时会触发清理逻辑，把这些 entry 清除，进而释放 value





##### AQS （AbstractQueuedSynchronizer） :star:

AQS，抽象队列同步器，主要用来实现各种锁和同步器，例如ReentrantLock中的Sync类是基于AQS实现的

> 作用：它提供了一个通用框架，用于实现各种同步器，例如 **可重入锁**（`ReentrantLock`）、**信号量**（`Semaphore`）和 **倒计时器**（`CountDownLatch`）。通过封装底层的线程同步机制，AQS 将复杂的线程管理逻辑隐藏起来，使开发者只需专注于具体的同步逻辑

AQS底层依赖与Unsafe类的CAS操作以及park和unpark等方法

###### CLH队列

自旋锁存在的问题：自旋锁通过线程不断对一个原子变量执行 `CAS`操作来尝试获取锁。在高并发场景下，多个线程会同时竞争同一个原子变量，容易造成某个线程的 `CAS` 操作长时间失败，从而导致 **“饥饿”问题**

> CLH队列用来优化自旋锁，它是一个**隐式的单向队列**（即不存在队列实例，仅存在节点之间的关联关系）
>
> **具体实现**：
>
> CLH 只维护一个 **原子变量** `tail`，所有新加入的线程都会**在 `tail` 后追加**，形成一个 **FIFO 隐式队列**
>
> 每个线程通过 `ThreadLocal` 存储自己的 **当前节点** 和 **前驱节点**，不断轮询前驱节点的状态，只有当前驱释放锁时，线程才能继续执行
>
> <img src="https://oss.javaguide.cn/github/javaguide/open-source-project/clh-lock-queue-structure.png" alt="CLH 锁的队列结构" style="zoom:67%;" />
>
> CLH对自旋锁进行了改进：
>
> - 每个线程会作为一个节点加入到队列中，并通过**自旋监控前一个线程节点的状态**，而不是直接竞争共享变量
> - 线程按顺序排队，确保**公平性**，从而避免了 “饥饿” 问题
>
> （感觉有点像Kafka的分布式锁，也是监控前一个节点是否结束从而获得锁）

###### AQS原理——CLH队列变体

> 改为**双向链表**结构：
>
> **一个节点表示一个线程**，它保存着线程的引用（thread）、 当前节点在队列中的状态（waitStatus）、前驱节点（prev）、后继节点（next）
>
> 加入`next`指针的作用：当前节点释放锁时可以直接唤醒后继节点，减少后续节点的轮询
>
> <img src="https://oss.javaguide.cn/p3-juejin/40cb932a64694262993907ebda6a0bfe~tplv-k3u1fbpfcp-zoom-1.png" alt="img" style="zoom: 50%;" />
>
> **AQS vs. CLH**
>
> ①AQS是双向链表，CLH是单向链表
>
> ②AQS是**自旋+阻塞**，而CLH单纯只有自旋
>
> ​	AQS：线程获取锁失败，会先短暂自旋尝试获取锁，如果一直失败，就阻塞等待前继节点的唤醒
>
> ​	CLH：线程获取锁失败会一直自旋尝试
>
> 
>
> AQS内部维护一个**volatile**的`state`变量，表示资源的状态，0为未被占有，1为被占有
>
> 一个线程申请对资源加锁前，先判断state是否为0，否则就将自己加入到同步等待队列中

###### Node节点的waitStatus状态含义

| Node 节点状态 | 值   | 含义                                                         |
| ------------- | ---- | ------------------------------------------------------------ |
| `CANCELLED`   | 1    | 表示线程已经取消获取锁。线程在等待获取资源时被中断、等待资源超时会更新为该状态 |
| `SIGNAL`      | -1   | 表示后继节点需要当前节点唤醒。在当前线程节点释放锁之后，需要对后继节点进行唤醒<br />一个线程在短暂自旋尝试获取锁失败后，就会阻塞，等待前继节点唤醒，此时就要将前继节点状态设置为`SINGNAL` |
| `CONDITION`   | -2   | 表示节点在等待 Condition。当其他线程调用了 Condition 的 `signal()` 方法后，节点会从**等待队列**转移到**同步队列**中等待获取资源 |
| `PROPAGATE`   | -3   | 用于共享模式，在共享模式下，前继节点不仅会唤醒后继节点，同时也可能会唤醒后继节点的后继节点 |
|               | 0    | 加入队列的新节点的初始状态                                   |

如果 `waitStatus > 0` ，表明节点的状态已经取消等待获取资源

如果 `waitStatus < 0` ，表明节点的处于有效的等待状态

因此在 AQS 的源码中，经常使用 `> 0` 、 `< 0` 来对 `waitStatus` 进行判断



###### AQS两种资源共享方式：独占 & 共享

`Exclusive`（独占，只有一个线程能执行，如`ReentrantLock`）

`Share`（共享，多个线程可同时执行，如`Semaphore`/`CountDownLatch`）

> AQS抽象类中提供了5个模板方法，用来自定义一个同步锁
>
> ```java
> //独占方式。尝试获取资源，成功则返回true，失败则返回false。
> protected boolean tryAcquire(int)
> //独占方式。尝试释放资源，成功则返回true，失败则返回false。
> protected boolean tryRelease(int)
> //共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。
> protected int tryAcquireShared(int)
> //共享方式。尝试释放资源，成功则返回true，失败则返回false。
> protected boolean tryReleaseShared(int)
> //该线程是否正在独占资源。只有用到condition才需要去实现它。
> protected boolean isHeldExclusively()
> ```
>
> [源码](https://javaguide.cn/java/concurrent/aqs.html#aqs-%E8%B5%84%E6%BA%90%E8%8E%B7%E5%8F%96%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-%E7%8B%AC%E5%8D%A0%E6%A8%A1%E5%BC%8F)



###### Semaphore

信号量，用于控制同时访问某个资源的线程数量

首先需要初始化一个 Semaphore 对象，指定<u>许可证数量</u>，表示最多允许多少个线程同时访问资源。

然后在每个线程访问资源前，调用 `acquire()` 方法获取许可证，如果没有可用许可证，则阻塞等待

访问完资源后，要调用 `release()` 方法释放许可证

```java
class SemaphoreExample {
    private static final int THREAD_COUNT = 5;
    private static final Semaphore semaphore = new Semaphore(2); // 最多允许 2 个线程访问

    public static void main(String[] args) {
        for (int i = 0; i < THREAD_COUNT; i++) {
            new Thread(() -> {
                try {
                    semaphore.acquire(); // 获取许可（如果没有可用许可，则阻塞）
                    System.out.println(Thread.currentThread().getName() + " 访问资源...");
                    Thread.sleep(2000); // 模拟任务执行
                } catch (InterruptedException e) {
                    e.printStackTrace();
                } finally {
                    semaphore.release(); // 释放许可
                }
            }).start();
        }
    }
}
```



###### CountDownLatch

计数器，通过一个倒计时计数器来控制多个线程的执行顺序，从而实现等待多个线程完成后再继续执行

在下面这段代码中，初始化了一个`count=3`的CountDownLatch，每个线程完成后，会调用`latch.countDown()`来时计数器减一

`latch.await()`会阻塞当前线程，直到计数器为0，才唤醒，从而实现等待所有线程执行完毕再继续执行其他线程的功能

```java
class CountDownLatchExample {
    public static void main(String[] args) throws InterruptedException {
        int threadCount = 3;
        CountDownLatch latch = new CountDownLatch(threadCount);

        for (int i = 0; i < threadCount; i++) {
            new Thread(() -> {
                try {
                    Thread.sleep((long) (Math.random() * 1000)); // 模拟任务执行
                    System.out.println(Thread.currentThread().getName() + " 执行完毕");
                } catch (InterruptedException e) {
                    e.printStackTrace();
                } finally {
                    latch.countDown(); // 线程完成后，计数器 -1
                }
            }).start();
        }

        latch.await(); // 主线程等待
        System.out.println("所有子线程执行完毕，主线程继续执行");
    }
}
```





###### CyclicBarrier

可循环使用的屏障，用于多个线程相互等待，直到所有线程都到达屏障后再同时执行

跟CountDownLatch类似，每个线程到达屏障后，调用`barrier.await()`等待其他线程，此时屏障值减一

直到屏障值为0，表示所有线程都到达屏障处，唤醒所有线程

```java
class CyclicBarrierExample {
    private static final int THREAD_COUNT = 3;
    private static final CyclicBarrier barrier = new CyclicBarrier(THREAD_COUNT);

    public static void main(String[] args) {
        for (int i = 0; i < THREAD_COUNT; i++) {
            new Thread(() -> {
                try {
                    System.out.println(Thread.currentThread().getName() + " 到达屏障");
                    barrier.await(); // 线程阻塞，直到所有线程都到达
                    System.out.println(Thread.currentThread().getName() + " 继续执行");
                } catch (InterruptedException | BrokenBarrierException e) {
                    e.printStackTrace();
                }
            }).start();
        }
    }
}
```

###### CyclicBarrier 和 CountDownLatch 有什么区别？

CyclicBarrier 让所有线程**相互等待**，全部到达后再继续；CountDownLatch 让**主线程等待**所有子线程执行完再继续。

| 对比项         | CyclicBarrier                                      | CountDownLatch                                     |
| -------------- | -------------------------------------------------- | -------------------------------------------------- |
| 主要用途       | 让所有线程相互等待，全部到达后再继续               | 让主线程等待所有子线程执行完                       |
| 可重用性       | ✅ 可重复使用，每次屏障打开后自动重置               | ❌ 不可重复使用，计数器归零后不能恢复               |
| 是否可执行回调 | ✅ 可以，所有线程到达屏障后可执行 barrierAction     | ❌ 不能                                             |
| 线程等待情况   | 所有线程互相等待，一个线程未到达，其他线程都会阻塞 | 主线程等待所有子线程完成，子线程执行完后可继续运行 |
| 适用场景       | 线程相互依赖，需要同步执行                         | 主线程等待子线程完成                               |
| 示例场景       | 计算任务拆分，所有线程都到达后才能继续             | 主线程等多个任务初始化完成                         |







------

## Mysql :white_check_mark:

重点：Mysql数据类型、索引、日志、锁、事务、MVCC、数据库优化 

#### Mysql基础

###### VARCHAR(100)和VARCHAR(10)的区别

> 1.VARCHAR(100)和 VARCHAR(10)都是变长类型，表示能存储最多 100 个字符和 10 个字符
>
> 2.存储相同的字符串，两者占用的**磁盘存储空间是一样的**
>
> 3.但是VARCHAR(100) 会消耗**更多的内存**。这是因为 VARCHAR 类型在内存中操作时，通常会**分配固定大小的内存块**来保存值，即使用字符类型中定义的长度



###### TEXT和BLOB类型，在使用临时表时无法使用内存临时表，只能在磁盘上创建临时表

> MySQL 默认使用 **MEMORY** **存储引擎**创建**内存临时表**，而 MEMORY 引擎：
>
> - 仅支持**定长数据类型**（如 `CHAR`、`INT`）。
> - **不支持** `TEXT` 和 `BLOB`，因为它们是**变长**数据类型，存储方式不同
>
> VARCHAR(n)虽然也是变长类型，但是它会按照最大的长度来分配建立内存临时表
>
> Mysql默认使用Memory存储引擎来构建内存临时表，但当无法构建内存临时表时，会转为使用InnoDB来构建磁盘临时表



###### 为什么不推荐使用BLOB和TEXT？

> - 不能有默认值
> - 在使用临时表时无法使用内存临时表，只能在磁盘上创建临时表
> - 检索效率较低
> - 不能直接创建索引，需要指定前缀长度
> - 可能会消耗大量的网络和 IO 带宽



###### DATETIME vs. Timestamp

存储空间、日期范围、默认值、时区

> Timestamp：**4**个字节，表示范围小，1970-01-01 00:00:01 ~ 2037-12-31 23:59:59，默认值为**当前时间戳**
>
> DATETIME：**8**个字节，表示范围大，1000-01-01 00:00:00 ~ 9999-12-31 23:59:59，默认值为**null**
>
> DATETIME 类型**没有时区**信息，TIMESTAMP 和**时区有关**



###### in 和 exists 的区别

> 使用 IN 时，MySQL 会首先执行子查询，然后将子查询的结果集用于外部查询的条件，即将子查询的结果集**全部加载到内存**中。<u>适合于子查询结果集较小的</u>
>
> EXISTS 会对外部查询的每一行，执行一次子查询。如果子查询返回任何行，则 `EXISTS` 条件为真。`EXISTS` 关注的是子查询**是否返回行**，而**不是返回的具体值**。<u>适合于子查询结果集较大</u>，如果子查询使用索引，那性能更好
>
> ```sql
> -- IN 的临时表可能成为性能瓶颈
> SELECT * FROM users 
> WHERE id IN (SELECT user_id FROM orders WHERE amount > 100);
> 
> -- EXISTS 可以利用关联索引
> SELECT * FROM users u
> WHERE EXISTS (SELECT 1 FROM orders o 
>          WHERE o.user_id = u.id AND o.amount > 100);
> ```
>
> ###### NULL值陷：
>
> `IN`: 如果子查询的结果集中包含 `NULL` 值，可能会导致意外的结果
>
> ​	例如，`WHERE column IN (subquery)`，如果 `subquery` 返回 `NULL`，则 `column IN (subquery)` 永远不会为真，除非 `column` 本身也为 `NULL`
>
> `EXISTS`: 对 `NULL` 值的处理更加直接。`EXISTS` 只是检查子查询是否返回行，不关心行的具体值，因此不受 `NULL` 值的影响



##### drop、delete 与 truncate 的区别

三者都表示删除，但是三者有一些差别：

| 区别     | **delete**                               | **truncate**                   | **drop**                                           |
| -------- | ---------------------------------------- | ------------------------------ | -------------------------------------------------- |
| 类型     | 属于 DML                                 | 属于 DDL                       | 属于 DDL                                           |
| 回滚     | 可回滚                                   | 不可回滚                       | 不可回滚                                           |
| 删除内容 | 表结构还在，删除表的全部或者一部分数据行 | 表结构还在，删除表中的所有数据 | 从数据库中删除表，所有数据行，索引和权限也会被删除 |
| 删除速度 | 删除速度慢，需要逐行删除                 | 删除速度快                     | 删除速度最快                                       |

不再需要一张表的时候，用 drop；想删除部分数据行，用 delete；在保留表而删除所有数据的时候用 truncate



##### UNION 与 UNION ALL 的区别

> 如果使用 UNION，会在表链接后**筛选掉重复的**记录行
>
> 如果使用 UNION ALL，不会合并重复的记录行
>
> 从效率上说，UNION ALL 要比 UNION 快很多，如果合并没有刻意要删除重复行，那么就使用 UNION All



##### count(1)、count(*) 与 count(列名) 的区别

> 是否忽略`null`值：
>
> * count(*)包括了**所有的列**，相当于行数，在统计结果的时候，**不会忽略**列值为 NULL
> * count(1)**忽略所有列**，用 1 代表代码行，在统计结果的时候，**不会忽略**列值为 NULL
> * count(列名)**只包括列名那一列**，在统计结果的时候，会**忽略列值为空**





#### Mysql存储引擎

创建表的时候可以指定存储引擎：Engine=InnoDB (默认)

修改表的存储引擎：ALTER TABLE your_table_name **ENGINE=InnoDB**;

##### MyISAM  vs. InnoDB

> MySQL 5.5.5 之前，MyISAM 是 MySQL 的默认存储引擎，5.5.5 版本之后，InnoDB 是 MySQL 的默认存储引擎
>
> 1.MyISAM只支持**表级锁**，不支持**行级锁** （每次上锁就会锁住整张表）
>
> 而InnoDB都支持，默认为行级锁
>
> （MyISAM不支持MVCC多版本并发控制，InnoDB支持）
>
> 2.MyISAM不支持事务，InnoDB支持事务（是**唯一一个**支持事务的存储引擎）
>
> 3.由第2点可知，MyISAM不支持数据库奔溃后的**安全恢复**，而InnoDB支持（靠的就是redo log）
>
> 4.MyISAM不支持外键，InnoDB支持外键（虽然不建议在数据库层面使用外键，而是建议在业务层解决）
>
> 5.MyISAM的表可以没有主键，InnoDB必须有主键（没有设置的话会有一个隐藏字段作为默认主键）
>
> 6.都使用B+树作为索引结构，但具体实现不同(MyISAM：非聚簇索引；InnoDB：**聚簇索引**)
>
> 7.InnoDB性能更强
>
> 8.数据缓存机制不同：InnoDB使用缓冲池（Buffer Pool）缓存**数据页和索引页**，MyISAM 使用键缓存（Key Cache）仅缓存索引页而不缓存数据页



##### InnoDB存储引擎中数据存储形式

<img src="https://cdn.tobebetterjavaer.com/stutymore/mysql-20240515110034.png" alt="不要迷恋发哥：段、区、页、行" style="zoom:67%;" />

> 表 => 叶子节点(数据段) => 段 => 区 => 页 => 记录
>
> 表 => 非叶子节点（索引段) => 段
>
> ①段：**表空间**分为3段：数据段、索引段、回滚段
>
> ②区：一个段分为多个区，每个区有一组连续的页（通常有64页，64*16KB = 1024KB = 1MB）
>
> ③页：InnoDB默认一个页为16KB，意味着数据库每次从磁盘读取数据都是以16KB为单位，一次至少读16KB到内存中，至少写16KB到磁盘中
>
> ​	一般一个**索引节点**就是一个页，成为索引页
>
> ④行：InnoDB采用按行存储的方式，每行有一些隐藏字段和表的字段



###### 数据页的具体结构

> 页的默认大小为16KB
>
> <img src="面经.assets/243b1466779a9e107ae3ef0155604a17.png" alt="图片" style="zoom:50%;" />
>
> <img src="面经.assets/fabd6dadd61a0aa342d7107213955a72.png" alt="图片" style="zoom: 50%;" />
>
> 页之间并不是物理连续的，而是逻辑连续的，通过File Header来连接
>
> <img src="https://cdn.xiaolincoding.com//mysql/other/557d17e05ce90f18591c2305871af665.png" alt="图片" style="zoom:67%;" />
>
> User Records：存储数据库中每一行的记录

###### 数据行(Row)的数据结构

> 每页中存放着数据库中的行/记录，存放在`User Records`中
>
> 在User Records中，**记录按照「主键」顺序组成单向链表**，单向链表的特点就是插入、删除非常方便，但是检索效率不高，最差的情况下需要遍历链表上的所有节点才能完成检索
>
> 所以InnoDB同时还维护了一个**页目录**，将**记录进行分组**，提高检索页中的某条记录的效率（可以理解为章节）
>
> ​	第一个分组中的记录只能有 1 条记录  ==> 槽0
>
> <img src="面经.assets/261011d237bec993821aa198b97ae8ce.png" alt="图片" style="zoom: 50%;" />
>
> 页目录创建的过程如下：
>
> 1. 将所有的记录划分成几个组，这些记录包括最小记录和最大记录，但不包括标记为“已删除”的记录
> 2. 每个记录组的最后一条记录就是组内最大的那条记录，并且最后一条记录的头信息中会存储该组一共有多少条记录，作为 n_owned 字段（上图中粉红色字段）
> 3. 页目录用来存储每组最后一条记录的地址偏移量，这些地址偏移量会按照先后顺序存储起来，每组的地址偏移量也被称之为槽（slot），**每个槽相当于指针指向了不同组的最后一个记录**
>
> 通过对槽进行**二分查找**，定位槽就知道某条记录属于哪个分组，再遍历这个组找到记录



###### InnoDB的存储文件格式

> 用两种格式的文件来存储，`.frm` 文件存储表的定义；`.ibd` 存储数据和索引



###### InnoDB的BufferPool

> InnoDB会将数据以**页**的形式从磁盘读取到内存中的BufferPool中
>
> 读取数据时如果BufferPool命中，可以直接返回，不需要到磁盘中读取。根据页替换算法替换到磁盘中
>
> 更新数据也是会先更新BufferPool，此时变为脏数据，再定期刷盘
>
> 而为了实现故障恢复，redo log就是记录了对BufferPool中哪些页中哪些字节修改成什么





#### Mysql索引

##### Mysql索引类型 :star:

###### 聚簇索引和非聚簇索引

> 聚簇索引（Clustered Index）：即**索引结构和数据一起存放**的索引，并不是一种单独的索引类型。InnoDB 中的**主键索引**就属于聚簇索引，**非主键索引**不是
>
> 非聚簇索引：索引结构和数据分开存放的索引，如二级索引就属于非聚簇索引，MyISam中所有索引都是非聚簇索引

###### 非主键索引和主键索引

InnoDB

> 主键索引：索引的叶子节点存储索引值和该记录完整的数据
>
> 非主键索引：索引的叶子节点存储索引值和该记录的主键，不包括记录的完整数据
>
> **回表**：当查询使用的是非主键索引时，例如select id, name, age from ... where `name = 'abc'`，那么先使用非主键索引`name`去查询到记录的主键`id`，再用过主键索引去找到该记录的完整数据

###### 覆盖索引

> **覆盖索引**：如果在上述过程中，要查询的值恰好是这个非主键索引的字段，如select id,  `name` from ... `where name = 'abc'`，那么就<u>不需要进行回表</u>。即如果<u>一个索引**覆盖**了要查询的值</u>，那么在查询时不需要进行回表，该索引称为覆盖索引
>
> 在使用`EXPLAIN`分析sql语句时，Extra=**Using index**表明查询使用了覆盖索引，不用回表，查询效率非常高

###### 联合索引 —— 最左前缀匹配

> 联合索引：使用表中的**多个字段**创建索引，就是 **联合索引**，也叫 **组合索引** 或 **复合索引**
>
> ALTER TABLE `cus_order` ADD INDEX id_score_name(**score, name**);
>

联合索引只有前一个列是等值的，后一个列在这个结果集里面才是有序的

> **最左前缀匹配原则**：在使用联合索引时，MySQL 会根据索引中的字段顺序，**从左到右**依次匹配where查询条件中的字段
>
> 假设有一个联合索引**(a, b, c)**，其从左到右的所有前缀为**(a）、(a, b)、(a, b, c)**
>
> 最左匹配原则会一直向右匹配，直到遇到**范围查询**（大于>、小于<）**为止**。即<u>之后的条件都不会走索引</u>，都是顺序扫描
>
> 下面几个select语句是否真的用到了联合索引？
>
> > 查询 `a=1 AND c=1`：根据最左前缀匹配原则，查询可以使用索引的前缀部分。因此，该查询仅在 `a=1` 上使用索引，然后对结果进行顺序扫描 `c=1` 的过滤。
> >
> > 查询 `c=1` ：由于查询中不包含最左列 `a`，根据最左前缀匹配原则，整个索引都无法被使用。
> >
> > 查询`b=1 AND c=1`：和第二种一样的情况，整个索引都不会使用
> >
> > 查询`a >= 1 AND b > 1 AND c < 1`：会用到 (a, b）索引，因为`b>1`是范围查询，所有只能先通过`a索引`定位到a>=1的结果，在这个结果中，b是有序的，可以继续使用b索引定位到b>1的结果。在这些结果中，c并不是有序的，所有c<1不能再用索引，只能顺序扫描
>
> 在创建联合索引时，应该根据查询条件将**最常用的放在前面**，遵守最左前缀原则

###### 全文索引

> **全文索引** 是专门用于 **全文搜索** 的索引类型，它可以用于 **快速查找文本数据**，尤其适用于 **大段文本的模糊查询**
>
> 一般用在`like`和文本的匹配上

###### 前缀索引

> 前缀索引(Prefix Index) 是针对 **字符串类型（`CHAR`、`VARCHAR`、`TEXT` 等）** 提供的一种索引优化方式。
>
> 它允许 **只索引字段的前几位字符**，而不是整个字段，以 **减少索引大小，提高查询效率**
>
> 在创建时，可以指定前缀的长度
>
> ```sql
> CREATE INDEX idx_prefix ON table_name(column_name(前缀长度));
> ```
>
> 优点：适合大文本类型，解决了大文本无法构建索引的问题
>
> ​		  减少索引的大小，提高查询效率
>
> 缺点：不能使用在group by 和 order by上，因为这两个操作都需要比较字符串完整的值

###### 唯一索引

> 用于保证某一列中的值是唯一的，允许空值
>
> ```sql
> CREATE UNIQUE INDEX idx_username ON users(username);
> ```
>
> 这个唯一索引就是为了保证每个用户名是唯一的

###### 索引下推

> Mysql架构中分为了**服务器层**和**存储引擎层**，一般where子句是由服务器层来执行，存储引擎将通过索引查询到的数据都返回给服务器层，由服务器层根据where条件来过滤出最终结果
>
> **索引下推**的目的就是让存储引擎层在遍历索引的同时，**处理部分where子句**的判断条件，直接过滤掉不满足条件的记录，返回给服务端。这样减少了存储引擎返回到服务器层的数据量，从而减少回表次数
>
> 只适合于**主键索引**或者**联合索引**
>

###### 索引失效的情况

> 创建了组合索引，但查询条件未遵守最左匹配原则
>
> 在索引列上进行计算、函数、类型转换等操作
>
> 以 % 开头的 LIKE 查询比如 `LIKE '%abc';` 导致全文索引/前缀索引失效
>
> 查询条件中使用 OR，且 OR 的前后条件中有一个列没有索引，涉及的索引都不会被使用到
>
> IN 的取值范围较大时会导致索引失效，走全表扫描（NOT IN 和 IN 的失效场景相同）
>
> 发生隐式转换
>
> 深度分页也会导致索引失效





##### 索引数据结构

###### B 树& B+树两者有何异同呢？ :star:

> - B 树的所有节点既存放键(key) 也存放数据(data)，而 B+树**只有叶子节点**存放 key 和 data，其他内节点只存放 key。
>
> - B 树的叶子节点都是独立的；B+树的叶子节点有一条**引用链**指向与它相邻的叶子节点，形成一条双向链表
>
> - B 树的检索的过程相当于对范围内的每个节点的关键字做二分查找，可能还没有到达叶子节点，检索就结束了
>
>   而 B+树的检索效率就很稳定了，任何查找都是从根节点到叶子节点的过程，叶子节点的顺序检索很明显。
>
> - 在 B 树中进行**范围查询**时，首先找到要查找的下限，然后对 B 树进行**中序遍历**，直到找到查找的上限；而 B+树的范围查询，只需要对**链表进行遍历**即可。
>
> B+树与 B 树相比，具备更少的 IO 次数、更稳定的查询效率和更适于范围查询这些优点

###### 红黑树

> 特性：
>
> * 每个节点非红即黑
> * **根节点总是黑色的**
> * 每个**叶子节点都是黑色**的空节点（NIL 节点）
> * 如果节点是红色的，则它的子节点必须是黑色的（反之不一定）
> * 从任意节点到它的叶子节点或空子节点的每条路径，必须包含相同数目的黑色节点（即相同的黑色高度）
>
> <img src="https://oss.javaguide.cn/github/javaguide/cs-basics/data-structure/red-black-tree.png" alt="红黑树" style="zoom:50%;" />



###### B+树的具体结构

> 每个节点为一个页，每个页都有一个页目录，方便对页中的记录进行定位
>
> 叶子节点还会存储数据，并且叶子节点之间通过双向链表连接起来
>
> ![图片](面经.assets/7c635d682bd3cdc421bb9eea33a5a413.png)







##### 索引失效的情况

>  ①索引列参与计算：WHERE **YEAR(create_time)** = 2024
>
>  ②联合索引不满足最左匹配原则
>
>  ③like查询以`%`开头：where  name **like '%abc'**
>
>  ④查询条件中使用 `OR`，且 OR 的前后条件中**有一个列没有索引**，涉及的索引都不会被使用到
>
>  ⑤深度分页：limit后跳过的页数太大，会变成全表查询：limt 100000
>
>  ⑥隐式类型转换
>
>  ⑦使用不等于（`<>`）或者 NOT 操作符：因为它们会扫描全表



##### 索引不适合的场景

> **数据表较小**：当表中的数据量很小，或者查询需要扫描表中大部分数据时，数据库优化器可能会选择全表扫描而不是使用索引。在这种情况下，维护索引的开销可能大于其带来的性能提升
>
> **频繁更新的列**:star:：对于经常进行更新、删除或插入操作的列，使用索引可能会导致性能下降。因为每次数据变更时，索引也需要更新，这会增加额外的写操作负担
>
> **区分度较低的列**：区分度 衡量一个字段在数据库表中唯一值的比例，**区分度 = 字段的唯一值数量 / 字段的总记录数**
>
> ​	例如性别字段是男或女，那么在1000条记录的表中，这个字段的区分度为 2 / 1000 = 0.002 非常小
>
> ​	所以区分度较低的字段不适合作为索引列
>
> ​	高区分度的字段更适合拿来作为索引，因为索引可以**更有效地缩小查询范围**
>
> **不经常出现在查询条件中的字段**：浪费空间





#### Mysql日志

##### 日志类型

> ①、**错误日志**（Error Log）：记录 MySQL 服务器启动、运行或停止时出现的问题。
>
> ②、**慢查询日志**（Slow Query Log）：记录执行时间超过 long_query_time 值的所有 SQL 语句。这个时间值是可配置的，默认情况下，慢查询日志功能是关闭的。可以用来识别和优化慢 SQL。
>
> ③、**一般查询日志**（General Query Log）：记录所有 MySQL 服务器的连接信息及所有的 SQL 语句，不论这些语句是否修改了数据。
>
> ④、**二进制日志**（Binary Log）：记录了所有修改数据库状态的 SQL 语句，以及每个语句的执行时间，如 INSERT、UPDATE、DELETE 等，但不包括 SELECT 和 SHOW 这类的操作。
>
> ⑤、**重做日志**（Redo Log）：记录了对于 InnoDB 表的每个写操作，不是 SQL 级别的，而是物理级别的，主要用于崩溃恢复。
>
> ⑥、**回滚日志**（Undo Log，或者叫事务日志）：记录数据被修改前的值，用于事务的回滚



##### 三大日志：bin log、redo log、undo log的区别 :star:

> `bin log` 
>
> ​	保证数据库的**一致性C**，数据**备份**、主备、主主、**主从复制**都离不开 binlog，也用于**数据库恢复**
>
> ​	**逻辑**日志（记录所有DML语句，insert、update、delete，不包含select）
>
> ​	有三种格式：
>
> ​		Statement（语句模式）：记录SQL语句（旧版本可能导致数据不一致，如`now()`函数）
>
> ​		Row（行模式）：记录每行变更（精确，但占用空间大）对于**大批量的更新**最好不要使用Row格式
>
> ​		Mixed（混合模式）：Statement和Row结合使用，自行判断哪个语句会产生数据丢失
>
> `redo log`
>
> ​	保证事务的**持久性D**   **物理日志**(记录**数据页**的物理变更：哪个页中哪几个字节做了什么修改)   
>
> ​	**InnoDB** 特有
>
> ​	利用redo log进行**崩溃恢复**
>
> ​	一个事务执行（**两阶段提交**）：开始事务-> 更新数据 -> 先写入redo log（prepare阶段）-> 提交事务 -> 写入binlog -> 提交redo log（commit阶段）
>
> `undo log`
>
> ​	保证事务的**原子性A **  **逻辑**日志（Insert操作记为delete操作...）
>
> ​	利用undo log进行**事务回滚**
>
> ​	MVCC中利用undo log来记录数据的旧版本
>
> ​	对于INSERT操作，每次事务提交都会被清除
>
> ​	对于UPDATE、DELETE操作，由**purge线程**来决定是否清除



##### 三种日志详细知识点

三种日志要吃透。。[JavaGuide指路](https://javaguide.cn/database/mysql/mysql-logs.html)

[面试指北](https://www.yuque.com/snailclimb/mf2z3k/zr4kfk)

[小林coding](https://xiaolincoding.com/mysql/log/how_update.html) 非常详细

**WAL （Write-Ahead Logging）**：MySQL 的写操作并不是立刻写到磁盘上，而是先写日志，然后在合适的时间再写到磁盘上。



##### redo log的两阶段提交

> 当客户端执行 commit 语句或者在自动提交的情况下，MySQL 内部开启一个 XA 事务，**分两阶段来完成 XA 事务的提交**，如下图：
>
> <img src="面经.assets/两阶段提交.drawio.png" alt="两阶段提交" style="zoom:67%;" />
>
> 从图中可看出，事务的提交过程有两个阶段，就是**将 redo log 的写入拆成了两个步骤：prepare 和 commit，中间再穿插写入binlog**，具体如下：
>
> - **prepare 阶段**：将 XID（内部 XA 事务的 ID） 写入到 redo log，同时将 redo log 对应的事务状态设置为 prepare，然后将 redo log 持久化到磁盘（innodb_flush_log_at_trx_commit = 1 的作用）；
> - **commit 阶段**：把 XID 写入到 binlog，然后将 binlog 持久化到磁盘（sync_binlog = 1 的作用），接着调用引擎的提交事务接口，将 redo log 状态设置为 commit，此时该状态并不需要持久化到磁盘，只需要 write 到文件系统的 page cache 中就够了，因为只要 binlog 写磁盘成功，就算 redo log 的状态还是 prepare 也没有关系，一样会被认为事务已经执行成功；
>
> ###### 异常现象分析
>
> <img src="面经.assets/两阶段提交崩溃点.drawio.png" alt="时刻 A 与时刻 B" style="zoom:67%;" />
>
> 时刻A 和 时刻B 发生故障，此时的redolog都是处于**prepare阶段**
>
> 在 MySQL 重启后会按顺序扫描 redo log 文件，碰到处于 prepare 状态的 redo log，就拿着 redo log 中的 **XID** 去 binlog 查看是否存在此 XID：
>
> - **如果 binlog 中没有当前内部 XA 事务的 XID，说明 redolog 完成刷盘，但是 binlog 还没有刷盘，则回滚事务**。对应<u>时刻 A</u> 崩溃恢复的情况
> - **如果 binlog 中有当前内部 XA 事务的 XID，说明 redolog 和 binlog 都已经完成了刷盘，则提交事务**。对应<u>时刻 B</u> 崩溃恢复的情况
>
> 所以prepare阶段的redo log既可以用来回滚事务，也可以用来提交事务
>
> ###### 为什么出现故障后不统一回滚，而是要根据binlog是否写入来决定回滚还是提交？
>
> 因为当 binlog 已经写入后，就有可能会被从库（或者用这个 binlog 恢复出来的库）使用
>
> 所以，在主库上也要提交这个事务。采用这个策略，主库和备库的数据就保证了一致性



##### undo log什么时候可以被purge线程清除？

> 在数据库系统中，判断undo log是否可以被purge，主要依据以下几点：
>
> 1. 事务提交和 undo log 的状态：当事务提交后，其对应的undo log并不会立即被清理。只有当系统确定没有其他事务再需要这些undo log来维护读一致性时，才会进行purge操作
> 2. 事务的提交顺序（trx_no）：每个写事务结束时都会获得一个递增的编号（trx_no）作为事务的提交序号。而每个读事务会在自己的ReadView中记录开始时看到的最大trx_no为m_low_limit_no。如果一个事务的trx_no小于当前所有活跃的读事务ReadView中的m_low_limit_no，说明该事务在所有读开始之前已经提交，其修改的新版本是可见的，因此不再需要通过undo构建之前的版本，这个事务的undo log就可以被清理了
> 3. undo log的生命周期管理：undo log的生命周期可能比事务的生命周期更长，因为它们可能被其他事务用于维护读一致性。只有当所有可能需要这些undo log的事务都完成之后，它们才可以被清理
> 4. **purge queue**的使用：系统会维护一个purge queue，其中按照事务提交顺序（trx_no）排序，确保先提交的事务对应的undo log先被处理。purge线程会按照这个顺序依次处理undo log，判断哪些记录已经不再需要



##### redo log 要写到磁盘，数据也要写磁盘，为什么要多此一举？

> 写入 redo log 的方式使用了追加操作， 所以磁盘操作是**顺序写**
>
> 写入数据需要先找到写入位置，然后才写到磁盘，所以磁盘操作是**随机写**
>
> WAL 技术的另外一个优点：**MySQL 的写操作从磁盘的「随机写」变成了「顺序写」**，提升语句的执行性能



##### Binlog 组提交

> **MySQL 引入了 binlog 组提交（group commit）机制，当有多个事务提交的时候，会将多个 binlog 刷盘操作合并成一个，从而减少磁盘 I/O 的次数**，如果说 10 个事务依次排队刷盘的时间成本是 10，那么将这 10 个事务一次性一起刷盘的时间成本则近似于 1。
>
> 引入了组提交机制后，prepare 阶段不变，只针对 commit 阶段，将 commit 阶段拆分为三个过程：
>
> - **flush 阶段**：多个事务按进入的顺序将 binlog 从 cache 写入文件（不刷盘）；
> - **sync 阶段**：对 binlog 文件做 fsync 操作（多个事务的 binlog 合并一次刷盘）；
> - **commit 阶段**：各个事务按顺序做 InnoDB commit 操作；
>
> 上面的**每个阶段都有一个队列**，每个阶段有锁进行保护，因此保证了事务写入的顺序，第一个进入队列的事务会成为 leader，leader领导所在队列的所有事务，全权负责整队的操作，完成后通知队内其他事务操作结束。
>
> <img src="面经.assets/commit_4.png" alt="每个阶段都有一个队列" style="zoom:67%;" />





#### Mysql Buffer Pool

##### Buffer Pool结构

> Buffer Pool默认大小128MB，可以通过参数`innodb_buffer_pool_size`设置
>
> Buffer Pool跟磁盘是以**页**的形式来交换数据的，一页默认16KB
>
> <img src="面经.assets/bufferpool内容.drawio.png" alt="img" style="zoom:67%;" />
>
> 为了更好的管理Buffer Pool，InnoDB给缓存池中每一页都添加了一个**控制块**（放在 Buffer Pool 的**最前面**）
>
> 控制块中记录了每一页的<u>表空间、页号、缓存页地址、链表节点</u>等等信息
>
> <img src="面经.assets/缓存页.drawio.png" alt="img" style="zoom:67%;" />

##### 管理Buffer Pool

###### 1.管理空闲页：Free链

> 为了快速的找到Buffer Pool中的空闲页，设置了一条**Free链**，它将**所有空闲页的控制块**连接了起来
>
> ​	Free链的头节点：记录了空闲块的个数
>
> ​	每当需要从磁盘中加载一个页到 Buffer Pool 中时，就从 Free链表中取一个空闲的缓存页，并且把该缓存页对应的控制块的信息填上，然后把该缓存页对应的控制块从 Free 链表中移除
>
> <img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/freelist.drawio.png" alt="img" style="zoom:50%;" />

###### 2.管理脏页：Flush链

> **Flush链表**的节点是每个脏页的控制块
>
> **后台线程**定期将这些脏页写回磁盘
>
> <img src="面经.assets/Flush.drawio.png" alt="img" style="zoom:50%;" />
>
> 下面几种情况会触发脏页的刷新：
>
> - 当 redo log 日志满了的情况下，会主动触发脏页刷新到磁盘；
> - Buffer Pool 空间不足时，需要将一部分数据页淘汰掉，如果淘汰的是脏页，需要先将脏页同步到磁盘；
> - MySQL 认为空闲时，后台线程会定期将适量的脏页刷入到磁盘；
> - MySQL 正常关闭之前，会把所有的脏页刷入到磁盘；

###### 3.页替换：LRU链

**传统的LRU链**存在问题

> <img src="面经.assets/bufferpoll_page.png" alt="img" style="zoom:50%;" />
>
> LRU链中，有干净页，也有脏页
>
> 但是存在两个问题：
>
> **①预读失效**
>
> ​	预读机制：基于程序局部性原理，MySQL 在加载数据页时，会提前把它<u>相邻的数据页</u>一并加载进来，目的是为了减少磁盘 IO
>
> ​	但是可能这些被提前加载进来的数据页，并没有被访问，相当于这个预读是白做了，这个就是**预读失效**
>
> ​	并且预读失效的页会导致一些<u>有用的页</u>反而被替换出去，从而降低缓存的命中率
>
> **②Buffer Pool 污染**
>
> ​	当某一个 SQL 语句扫描了<u>大量的数据</u>时，在 Buffer Pool 空间比较有限的情况下，可能会将 Buffer Pool 里的<u>所有页都替换出去</u>，导致大量热数据被淘汰了，等这些热数据又被再次访问的时候，由于缓存未命中，就会产生大量的磁盘 IO，MySQL 性能就会急剧下降，这个过程被称为 **Buffer Pool 污染**。

**解决**：

解决预读失效：

> Mysql将LRU链划分为**young 区**和**old 区**
>
> ​	young区存放热点页，old区存放临时页
>
> <img src="面经.assets/young+old.png" alt="img" style="zoom:67%;" />
>
> old 区域占整个 LRU 链表长度的比例可以通过 `innodb_old_blocks_pct` 参数来设置，默认是 37，代表整个 LRU 链表中 young 区域与 old 区域比例是 63:37。
>
> 划分这两个区域后，**预读的页就只需要加入到 old 区域的头部**，<u>当页被真正访问的时候，才将页插入 young 区域的头部</u>
>
> 如果预读的页一直没有被访问，就会从 old 区域移除，这样就<u>不会影响 young 区域中的热点数据</u>
>
> 例如，当读一个页时，页20也被预读进来，那么它会先加到old区
>
> ![img](面经.assets/lrutwo2.png)
>
> 如果`页20`一直没被访问，它会慢慢的移到old区的末尾，从而被淘汰
>
> 如果此时`页20`被访问，那么它会加到young区，并且将`页7`挤到old区
>
> ![img](面经.assets/lrutwo3.png)

解决Buffer Pool污染：

> 对于进入到 young 区域条件：增加了一个**停留在 old 区域的时间**判断，由 `innodb_old_blocks_time` 设置，默认1s
>
> ①如果**后续的访问时间**与**第一次访问的时间**在这个时间间隔内，那么该缓存页就**不会**被从 old 区域移动到 young 区域的头部；
>
> ②如果后续的访问时间与第一次访问的时间不在这个时间间隔内，那么该缓存页**移动到 young 区**的头部
>
> ==> 即需要在old区停留足够久，才可以进入young区，这样就解决了全表查询带来的影响，都进来的页一下子就会被覆盖

> MySQL 针对 young 区域其实做了一个优化，为了防止 young 区域节点频繁移动到头部
>
> young 区域**前面 1/4 被访问不会移动到链表头部**，只有后面的 3/4被访问了才会



##### 预读机制

> 
>



#### Mysql锁

<img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/mysql-a07e4525-ccc1-4287-aec5-ebf3f277857c.jpg" alt="三分恶面渣逆袭：MySQL 中的锁" style="zoom:67%;" />

###### 意向锁

> 意向锁的出现是为了支持 InnoDB 的**多粒度锁**，它解决的是**表锁和行锁**共存的问题
>
> 当我们需要**给一个表加表锁**的时候，我们需要根据去判断表中有没有数据行被锁定，以确定是否能加成功
>
> ​	假如没有意向锁，那么我们就得**遍历表**中所有数据行来判断有没有行锁
>
> ​	有了意向锁这个表级锁之后，则我们**直接判断一次**就知道表中是否有数据行被锁定了
>
> 例如：事务A就表中**某一行加锁**了，那么同时也**会对整个表加一个意向排他锁**
>
> ​	在事务A结束之前，事务B想要对整个表加表锁，发现表已经被加了意向排他锁，说明有的记录被加了行锁，需要等待
>
> ​	如果事务B想对某一行加锁，那就不会被影响到，直接看那一行是否被加锁就行

**意向共享锁（Intention Shared Lock，IS 锁）**：事务有意向对表中的某些记录加共享锁（S 锁），加共享锁前必须先取得该表的 IS 锁

**意向排他锁（Intention Exclusive Lock，IX 锁）**：事务有意向对表中的某些记录加排他锁（X 锁），加排他锁之前必须先取得该表的 IX 锁

**共享锁之间相互兼容不互斥**：即一个事务对表A加了IS/IX锁，事务B也可以对表A加IS/IX锁，不互斥

|       | IS 锁 | IX 锁 |
| ----- | ----- | ----- |
| IS 锁 | 兼容  | 兼容  |
| IX 锁 | 兼容  | 兼容  |

###### 意向锁和(表级)读写锁

|                 | IS 锁 | IX 锁 |
| --------------- | ----- | ----- |
| S锁（表级读锁） | 兼容  | 互斥  |
| X锁（表级写锁） | 互斥  | 互斥  |

注意：意向锁与行级读写锁不互斥



###### 行级锁会出现的问题？

InnoDB 行锁是通过**对索引数据页上的记录**加锁实现的（锁加在索引记录上，而不是表数据本身）

> 行级锁是基于**索引**的，才能快速**定位并锁定**特定的记录
>
> InnoDB 的**行锁**是针对**索引字段**加的锁，**表级锁**是针对**非索引字段**加的锁。当我们执行 `UPDATE`、`DELETE` 语句时，如果 `WHERE`条件中字段<u>没有命中唯一索引或者索引失效</u>的话，就会导致扫描全表对表中的<u>所有行记录进行加临建锁</u>，从而导致其他读写请求都被阻塞
>
> 另外，不是说 update 语句的 where 带上索引就能避免全表记录加锁，得看sql优化器最终选择的是索引扫描，还是全表扫描，如果走了全表扫描，就会对全表的记录加锁了



###### 行锁的类别

> **记录锁（Record Lock）**：属于单个行记录上的锁
>
> **间隙锁（Gap Lock）**：锁定一个范围，不包括记录本身
>
> **临键锁（Next-Key Lock）**：Record Lock+Gap Lock，锁定一个范围，包含记录本身，主要目的是为了解决幻读问题（MySQL 事务部分提到过）。记录锁只能锁住已经存在的记录，为了避免插入新记录，需要依赖间隙锁
>
> 在**可重复读**的事务隔离级别中，**行锁默认为临键锁**，已解决幻读的问题。不过有**锁降级机制**：
>
> ​	如果操作的索引是唯一索引或主键，InnoDB 会对 Next-Key Lock 进行优化，将其降级为 Record Lock，即仅锁住索引本身，而不是范围。
>
> ​	精确唯一等值查询 + 索引命中
>
> ​	范围查询 + 被锁的记录正好是唯一的

###### 行级锁降级机制

> 当我们用**唯一索引进行等值查询**的时候，查询的记录存不存在，加锁的规则也会不同：
>
> * 当查询的记录是「**存在**」的，在索引树上定位到这一条记录后，将该记录的索引中的 next-key lock 会退化成「**记录锁**」。
> * 当查询的记录是「**不存在**」的，在索引树找到<u>第一条大于该查询记录的记录</u>后，将该记录的索引中的next-key lock 会退化成「**间隙锁**」
>
> 当**唯一索引进行范围查询**时，对等值匹配到的记录加记录锁，其他记录加间隙锁





#### Mysql事务

##### 事务的ACID特性 :star:

> **原子性**（`Atomicity`）：事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用；
>
> **一致性**（`Consistency`）：执行事务前后，数据保持一致，例如转账业务中，无论事务是否成功，转账者和收款人的总额应该是不变的；
>
> **隔离性**（`Isolation`）：并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的；
>
> **持久性**（`Durability`）：一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响



##### 四个隔离级别 :star:

实现：主要依靠**读锁、写锁、MVCC**来实现

对于写操作，都需要加写锁，但是释放锁的时机不同，读未提交是在写完就释放，其他三个都是事务提交再释放

对于读操作，从：不加锁 => MVCC（读快照） => 读锁（当前读）

> **READ-UNCOMMITTED(读取未提交)** ：最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读
>
> * 查询直接读取当前最新的数据版本（即使未提交）
> * 读操作**不加锁**、也不做版本控制MVCC
> * 写操作会加**排他锁**（X锁/写锁），以保证数据一致性和防止写冲突
> * 该级别主要的特点是**释放锁的时机与众不同**：在**执行完写操作后立即释放**，而不像其他隔离级别在事务提交以后释放。所以才会脏读
>
> **READ-COMMITTED(读取已提交)** ：允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生
>
> * **每次查询**都获取新的快照，MVCC
> * 保证读取的是已提交数据，避免脏读
> * **读操作不会加锁**，而是每次查询都会获取**快照**（除非显式加锁 select ... for update）
> * 写操作也会加**排他锁**，锁的释放时机延迟到**事务提交之后**，解决了脏读
>
> **REPEATABLE-READ(可重复读)** ：对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。【**InnoDB默认隔离级别**】
>
> * **读操作不加锁**，而是在**事务开始时**创建一个**快照**，解决了可重复读（每次读取同一行记录的内容是一样的）
> * 写操作加**排他锁**，事务提交之后再释放
>
> **SERIALIZABLE(可串行化)** ：最高的隔离级别，完全服从 ACID 的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读
>
> 注意<u>不是严格顺序执行，但结果是等价的</u>
>
> * 对所有读取的数据都加 **共享锁（读锁/S锁）**，对写操作加 **排他锁（写锁/X锁）**
> * 所有资源（读写锁）都是在事务提交或回滚的时候释放
> * 不需要在事务一开始就对所需要的资源全部加锁，用到再加

每个级别是否**还存在**脏读、不可重复读、幻读的问题：

|     隔离级别     | 脏读（读未提交） | 不可重复读 | 幻读 |
| :--------------: | :--------------: | :--------: | :--: |
| READ-UNCOMMITTED |        √         |     √      |  √   |
|  READ-COMMITTED  |        ×         |     √      |  √   |
| REPEATABLE-READ  |        ×         |     ×      |  √   |
|   SERIALIZABLE   |        ×         |     ×      |  ×   |



##### 什么是脏写？

> 脏写是指一个事务A覆盖了**另一个未提交事务B的修改**，而此时**事务B想要回滚**，数据就会出现不一致
>
> 读未提交不可以解决脏写



##### 如何解决幻读？

> **幻读问题**：幻读与不可重复读类似。它发生在一个事务读取了几行数据，接着另一个并发事务插入(insert)了一些数据时。在随后的查询中，第一个事务就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读
>
> 解决：
>
> ①将事务隔离级别改为`可串行化`
>
> ②在 `REPEATABLE READ` 隔离级别下，使用**临键锁**解决幻读：（默认）
>
> * **记录锁（Record Lock）**：属于单个行记录上的锁
>* **间隙锁（Gap Lock）**：锁定一个范围，不包括记录本身
> * **临键锁（Next-Key Lock）**：记录锁+间隙锁，锁定一个范围，包含记录本身，主要目的是为了解决幻读问题。记录锁只能锁住已经存在的记录，为了避免插入新记录，需要依赖间隙锁。锁定它们的间隙，防止其它事务在查询范围内插入数据。只要我不让你插入，就不会发生幻读



##### 读快照 vs 当前读

> **读快照**：RC或者RR下，不管记录是否被加锁，都会生成一份读快照 【**一致性非锁定读**】
>
> ​	RC：每次select生成一份新的读快照
>
> ​	RR：每次事务开始生成一份快照
>
> **当前读**：可串行化，给行记录加 X 锁或 S 锁  【**一致性锁定读**】



##### MVCC（多版本并发控制）的实现 :star:

JavaGuide指路：https://javaguide.cn/database/mysql/innodb-implementation-of-mvcc.html

> MVCC 是一种并发控制机制，用于在多个并发事务同时读写数据库时保持数据的一致性和隔离性。它是通过在**每个数据行**上**维护多个版本**的数据来实现的。当一个事务要对数据库中的数据进行修改时，MVCC 会为该事务创建一个数据快照，而不是直接修改实际的数据行

###### InnoDB实现MVCC的三要素：隐藏字段、ReadView、UndoLog

> **隐藏字段**：为每行数据加**三个**隐藏字段来实现版本控制
>
> ​	`DB_TRX_ID`：6bytes，表示最后一次插入或更新改行的事务ID（删除除外）
>
> ​	`DB_ROLL_PTR`：7bytes，回滚指针，指向该数据行的undo log（用来找历史数据）
>
> ​	`DB_ROW_ID`：6bytes，如果该表**没有主键也没有唯一非空索引**，InnoDB会用该id生成聚簇索引来充当主键id
>
> 
>
> **ReadView**： 读快照，保存当前事务创建的这一份快照的信息
>
> ```c++
> class ReadView {
>     private:
>       /* 这份快照对于ID大于等于这个m_low_limit_id值的事务均不可见 */
>       /* m_low_limit_id = 当前最大的事务ID+1 即下一个事务将被分配的ID */
>       trx_id_t m_low_limit_id;     // 不可见的最小事务id 
> 	  
>       /* 这份快照对于ID小于这个m_up_limit_id值的事务均可见 */
>       /* 该值为活跃事务列表m_ids中最小的事务ID */
>       trx_id_t m_up_limit_id;       // 可见的最大事务id
> 	   /* 创建这份快照的事务ID */
>       trx_id_t m_creator_trx_id;   
> 	   /* 事务 Number, 小于该 Number 的 Undo Logs 均可以被 Purge */
>       trx_id_t m_low_limit_no;     
>       /* 创建这份快照时的活跃事务列表 */
>       /* 活跃的定义是修改改行数据的事务仍未提交 */
>       ids_t m_ids;                  
> 	  /* 标记这份快照是否 close */
>       m_closed;                     
> }
> ```
>
> ​	当前这份快照对谁可见？
>
> <img src="https://javaguide.cn/assets/trans_visible-ekj9bMvL.png" alt="trans_visible" style="zoom:67%;" />
>
> **undo log**：在MVCC中undo log的作用是获取记录的历史版本数据
>
> ​	当一条记录被多次修改时，记录中的`DB_ROLL_PTR`就会形成一条版本链
>
> ​	一个事务可以根据这条链，找到其**可见的**历史版本
>
> <img src="https://javaguide.cn/assets/6a276e7a-b0da-4c7b-bdf7-c0c7b7b3b31c-n52toho_.png" alt="img" style="zoom:80%;" />



###### 数据可见性算法 :star:

当前事务生成了一份快照，并想要读取某一行数据，该数据行有`DB_TRX_ID`隐藏字段，表示最新修改该行数据的事务

那么，这行数据的最新值对当前事务是否可见？如果不可见，那当前事务可见的历史版本又是什么？

==> 数据可见性算法：

我们应该如何根据这个`DB_TRX_ID`值，以及当前事务`trx_id`的快照中记录的信息，来找到对当前事务可见的数据？

> 1. 如果记录 `DB_TRX_ID < m_up_limit_id`，那么表明最新修改该行的事务（DB_TRX_ID）在**当前事务创建快照之前就提交了**，所以该记录行的值对当前事务是**可见**的
>
> 2. 如果 `DB_TRX_ID >= m_low_limit_id`，那么表明最新修改该行的事务（DB_TRX_ID）在**当前事务创建快照之后才修改该行**，所以该记录行的值对当前事务**不可见** ==> 跳到**步骤 5**
>
> 3.  如果`m_up_limit_id <= DB_TRX_ID < m_low_limit_id`，那么看`m_ids`活跃事务列表。如果`m_ids为空`，则表明**在当前事务创建快照之前，修改该行的事务就已经提交了**，所以该记录行的值对当前事务是**可见**的
>
> 4. 如果`m_ids不为空`，就要对活跃事务列表 m_ids 进行查找（源码中是用的二分查找，因为是有序的）
>
>    - 如果在 **m_ids 中能找到 DB_TRX_ID**，表明：
>
>      ① 在当前事务创建快照前，该记录行的值被事务 ID 为 DB_TRX_ID 的事务修改了，但没有提交；
>
>      或者 ② 在当前事务创建快照后，该记录行的值被事务 ID 为 DB_TRX_ID 的事务修改了。
>
>      这些情况下，这个记录行的值对当前事务都是**不可见**的 ==> 跳到步骤 5
>
>    - 在**活跃事务列表中找不到**，则表明该修改在当前事务创建快照前就已经提交了，所以记录行对当前事务**可见**
>
> 5. 在该记录行的 `DB_ROLL_PTR` 指针所指向的 `undo log` 取出快照记录，用快照记录的 `DB_TRX_ID` **跳到步骤 1** 重新开始判断，直到找到满足的快照版本或返回空



###### 一致性非锁定读 vs 锁定读

> **一致性非锁定读**：快照读，MVCC的实现就是依靠快照读、undo log和隐藏字段
>
> ​	对每行记录加一个**版本号**或者时间戳字段，在更新数据的同时版本号 + 1 或者更新时间戳。查询时，将当前可见的版本号与对应记录的版本号进行比对，如果记录的版本小于可见版本，则表示该记录可见
>
> **锁定读**：当前读，读某一条记录时会对该记录加上S锁，其他事务可以加S锁，但不可以加X锁
>
> ​	在锁定读下，读取到的数据永远是最新的
>
> ​	所以在两次查询中间如果插入了数据，就会导致幻读，所以需要加**临键锁**



###### 快照读在RC和RR下的区别

> 在 RC 隔离级别下的 **`每次select`** 查询前都生成一个`Read View` (m_ids 列表)
>
> 在 RR 隔离级别下只在事务开始后 **`第一次select`** 数据前生成一个`Read View`（m_ids 列表）
>
> 所以在RC下的快照读**不能保证可重复读**，RR下可以保证可重复读



##### 自增值不连续的 4 个场景

> 1. 自增初始值和自增步长设置不为 1
> 2. 唯一键冲突
> 3. 事务回滚
> 4. 批量插入（如 `insert...select` 语句），第一次申请1个id，第二次申请2个id，4个, ...



#### Mysql优化

##### 如何避免读写分离中的主从延迟？

主库和从库的数据同步存在延迟

master(binlog) => **I/O线程（接收）** => 写入relay Log => **SQL线程（执行）** => slave(binlog)

![MySQL 主从复制过程](面经.assets/主从复制过程.drawio.png)

> ①MySQL 主库在收到客户端提交事务的请求之后，会先写入 binlog，再提交事务，更新存储引擎中的数据，事务提交完成后，返回给客户端“操作成功”的响应。
>
> ②从库会创建一个专门的 I/O 线程，连接主库的 log dump 线程，来接收主库的 binlog 日志，再把 binlog 信息写入 relay log 的中继日志里，再返回给主库“复制成功”的响应。
>
> ③从库会创建一个用于回放 binlog 的线程，去读 relay log 中继日志，然后回放 binlog 更新存储引擎中的数据，最终实现主从的数据一致性。

> 原因：
>
> 1. 从库 **I/O 线程**接收 binlog 的速度跟不上主库写入 binlog 的速度，导致从库 relay log 的数据滞后于主库 binlog 的数据；
>
> 2. 从库 **SQL 线程**执行 relay log 的速度跟不上从库 I/O 线程接收 binlog 的速度，导致从库的数据滞后于从库 relay log 的数据。

> ①写操作后强制将**读请求**路由到**主库**处理
>
> ​	使用 Sharding-JDBC 的 `HintManager` 分片键值管理器，我们可以强制使用主库
>
> ②二次读取：从库读取失败后到主库中再次查询
>
> ③Canal监听binlog判断是否同步完成
>
> ④延迟读取
>
> ​	等主从同步后再处理读请求
>
> ​	你支付成功之后，跳转到一个支付成功的页面，当你点击返回之后才返回自己的账户

###### 主从复制模型

> - **同步复制**：MySQL 主库提交事务的线程要**等待所有从库的复制成功**响应，才返回客户端结果。这种方式在实际项目中，基本上没法用，原因有两个：一是性能很差，因为要复制到所有节点才返回响应；二是可用性也很差，主库和所有从库任何一个数据库出问题，都会影响业务。
> - **异步复制**（默认模型）：MySQL 主库提交事务的线程并不会等待 binlog 同步到各从库，就返回客户端结果。这种模式一旦主库宕机，数据就会发生丢失。
> - **半同步复制**：MySQL 5.7 版本之后增加的一种复制方式，介于两者之间，事务线程不用等待所有的从库复制成功响应，只要一部分复制成功响应回来就行，比如一主二从的集群，只要数据成功复制到任意一个从库上，主库的事务线程就可以返回给客户端。这种**半同步复制的方式，兼顾了异步复制和同步复制的优点，即使出现主库宕机，至少还有一个从库有最新的数据，不存在数据丢失的风险**。



##### pt-online-schema-change

> 对大表数据结构的修改一定要谨慎，会造成严重的锁表操作，尤其是生产环境，是不能容忍的。
>
> pt-online-schema-change 它会首先建立一个与原表结构相同的新表，并且在新表上进行表结构的修改，然后再把原表中的数据复制到新表中，并在原表中增加一些触发器。把原表中新增的数据也复制到新表中，在所有数据复制完成之后，把新表命名成原表，并把原来的表删除掉。把原来一个 DDL 操作，分解成多个小的批次进行



##### 分库和分表 :star:

> 分库：将一个数据库中的数据分到**不同的数据库**上
>
> * 垂直分库：将一个数据库中不同的表，按照业务进行分配，分散到不同服务器的数据库上
> * 水平分库：将一个数据库中所有的表按照一定的逻辑来分散到不同的数据库对应的表上
>
> 分表：对**单个表**的数据进行拆分
>
> * 垂直分表：对数据表**列的拆分**，把一张列比较多的表拆分为多张表
> * 水平分表：对数据表**行的拆分**，解决单一表数据量过大的问题
>
> 一般来说，水平分表和水平分库同时出现

###### 什么时候需要分表分库？

> 单表的数据达到**千万级别**以上，数据库读写速度比较缓慢
>
> 数据库中的数据占用的空间越来越大，**备份时间**越来越长

###### 分片算法：一条数据应该被分到哪个分表里

> * 哈希分片：求指定**分片键**的哈希，根据**哈希值**确定数据应被放置在哪个表中
> * 范围分片：比如 将 `id` 为 `1~299999` 的记录分到第一个表， `300000~599999` 的分到第二个表
> * 映射表分片：使用一个单独的表（称为映射表）来存储分片键和分片位置的对应关系
> * 地理位置分片
>
> 关键：分片键（sharding key），它影响着数组的分布和查询的效率

###### 分表分库带来的问题

> 分布式事务：单个操作涉及到多个数据库
>
> 无法进行join联表操作：
>
> * 大厂都建议不要使用join，建议在业务代码中进行join
> * 也可以适当的冗余一些字段，比如name，因为一般用户要看的都是name而不是id
>
> 分布式ID：如何保证分库分表中ID全局唯一呢？
>
> * 自增ID，并给每个表设置不同的步长
> * UUID
> * 分布式ID：雪花算法（64位ID）、美团Leaf（开源分布式ID）
>
> 跨库聚合查询问题：group by、order by

###### 分表分库推荐方案

> **ShardingSphere**（Sharding-JDBC）：Apache的一个项目，一款分布式的数据库生态系统， 可以将任意数据库转换为分布式数据库，并通过数据分片、弹性伸缩、加密等能力对原有数据库进行增强
>
>  **TiDB分布式关系型数据库**：不需要我们手动进行分库分表，一步到位

###### 分表分库后，数据如何从源数据库迁移到不同数据库中

> ①停机迁移：如在凌晨2点，系统使用的人数非常少的时候，挂一个公告说系统要维护升级预计1小时
>
> ②双写方案（不停机）：
>
> ​	数据迁移，将老库数据拷贝到新库
>
> ​	其间，查询走老库（查），更新操作走老库（增删改），同时也要写入新库（双写）。还需要冗余数据清理
>
> 推荐**Canal**做增量数据迁移中间件
>
> <img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/mysql-2d4d94c9-e816-47fc-93dd-a835b1318099.jpg" alt="img" style="zoom:67%;" />



##### 深度分页

> 深度分页：查询**偏移量（limit）过大**的场景我们称为深度分页，这会导致查询性能较低
>
> ```sql
> SELECT * FROM t_order ORDER BY id LIMIT 1000000, 10
> ```
>
> 当偏移量太大时，就**不会使用索引**而是**全表查询**
>
> 解决：
>
> ①使用范围查询：如果id是连续的，可以通过范围查询，这样就可以使用索引来查询
>
> ```sql
> SELECT * FROM t_order WHERE id > 100000 AND id <= 100010 ORDER BY id
> SELECT * FROM t_order WHERE id > 100000 LIMIT 10
> ```
>
> ​	需要记录上一次查询的最后一个id
>
> ②子查询：先用子查询查询出limit范围的id
>
> ```sql
> SELECT * FROM t_order
> WHERE id >= (
>  SELECT id FROM t_order where id > 1000000 
>  limit 1
> ) LIMIT 10;
> ```
>
> ​	这个子查询会使用索引快速定位到第10000001条记录
>
> ​	也可以通过子查询去获取的分页的目标id集合
>
> ​	问题：子查询会产生临时表
>
> ③:star:**​延迟关联**：通过 `INNER JOIN` 将子查询结果集成到主查询中，避免了子查询可能产生的临时表
>
> 先在主表中对主键索引进行limit查询，得到ID，再来和其他表进行关联，再进行查询
>
> 例如:
>
> ```sql
> -- 原sql：
> SELECT t1.*  FROM t_order t1
> limit 1000000, 10
> -- 优化后：
> SELECT t1.*  FROM t_order t1
> INNER JOIN (
>     SELECT id FROM t_order where id > 1000000 LIMIT 10  --- 先查出这个范围内的id
> ) t2 ON t1.id = t2.id;
> 
> -- 原sql
> SELECT e.id, e.name, d.details
> FROM employees e JOIN department d ON e.department_id = d.id
> ORDER BY e.id
> LIMIT 1000000, 20;
> -- 优化后：
> SELECT e.id, e.name, d.details
> FROM (
>     SELECT id
>     FROM employees
>     ORDER BY id
>     LIMIT 1000, 20
> ) AS sub
> JOIN employees e ON sub.id = e.id
> JOIN department d ON e.department_id = d.id;
> ```
>
> ④:star:**书签**：对于id连续的数据，可以记录上一次查询的last_max_id，下一次查询可以从这个id开始，而不是使用offset
>
> ```sql
> -- 原sql
> SELECT id, name FROM users
> LIMIT 1000000, 20;
> -- 优化后
> SELECT id, name FROM users
> WHERE id > last_max_id  -- 假设last_max_id是上一页最后一行的ID
> LIMIT 20;
> ```
>
> 



##### EXPLAIN分析sql的执行计划 :star: 

分析慢查询的关键

> ```sql
> explain <sql>
> ```
>
> 返回结果：执行计划结果中共有 12 列，各列代表的含义总结如下表：
>
> | **列名**      | **含义**                                     |
> | ------------- | -------------------------------------------- |
> | id            | SELECT 查询的序列标识符                      |
> | select_type   | SELECT 关键字对应的查询类型                  |
> | table         | 用到的表名                                   |
> | partitions    | 匹配的分区，对于未分区的表，值为 NULL        |
> | **type**      | 表的访问方法                                 |
> | possible_keys | 可能用到的索引                               |
> | **key**       | 实际用到的索引                               |
> | key_len       | 所选索引的长度                               |
> | ref           | 当使用索引等值查询时，与索引作比较的列或常量 |
> | **rows**      | 预计要读取的行数                             |
> | filtered      | 按表条件过滤后，留存的记录数的百分比         |
> | **Extra**     | 附加信息                                     |
>
> 几个重要的参数介绍：
>
> ①id：标记每个select语句的执行顺序（如果一个sql中由嵌套的select）
>
> ②select_type：查询类型
>
> - **SIMPLE**：简单查询，不包含 UNION 或者子查询
> - **PRIMARY**：主查询：查询中如果包含子查询或其他部分，**外层的 SELECT** 为主查询
> - **SUBQUERY**：子查询中的第一个 SELECT
> - **UNION**：在 UNION 语句中，**UNION 之后**出现的 SELECT
> - **DERIVED**：在 **FROM 中**出现的子查询将被标记为 DERIVED。
> - **UNION RESULT**：UNION 查询的结果
>
> ③table：这个select语句中用到的表，可能是某个表，也可能是某个子查询的结果
>
> ④type：查询执行的类型，描述了查询是如何执行的
>
> > system > const > eq_ref > ref > fulltext > ref_or_null > index_merge > unique_subquery > index_subquery > range > index > ALL
>
> - **range**：对索引列进行范围查询，执行计划中的 key 列表示哪个索引被使用了。
> - **index**：查询遍历了整棵索引树，与 ALL 类似，只不过扫描的是索引，而索引一般在内存中，速度更快。
> - **ALL**：全表扫描
>
> ⑤key：真正使用到的索引
>
> ⑥extra： 解析查询的额外信息
>
> - **Using filesort**：在排序时使用了外部的索引排序，没有用到表内索引进行排序
> - **Using temporary**：:star:MySQL 需要创建**临时表**来存储查询的结果，常见于 ORDER BY 和 GROUP BY
> - **Using index**：:star:表明查询使用了**覆盖索引**，不用回表，查询效率非常高
> - **Using index condition**：:star:表示查询优化器选择使用了**索引下推**这个特性
> - **Using where**：:star:表明查询使用了 **WHERE 子句**进行条件过滤。一般在没有使用到索引的时候会出现
> - **Using join buffer (Block Nested Loop)**：连表查询的方式，表示当被驱动表的没有使用索引的时候，MySQL 会先将驱动表读出来放到 join buffer 中，再遍历被驱动表与驱动表进行查询

一般索引失效了：key=null、type=ALL、Extra=using where

覆盖索引：Extra=using index



##### 一条sql的执行过程

> **Server层**：
>
> ​	**连接器：** 身份认证和权限相关(登录 MySQL 的时候)
>
> ​	**查询缓存：** 执行查询语句的时候，会先查询缓存（MySQL 8.0 版本后移除，因为这个功能不太实用）
>
> ​	**分析器：** 没有命中缓存的话，SQL 语句就会经过分析器，检查你的 SQL 语句语法是否正确，包括先词法分析再语法分析
>
> ​	**优化器：** 按照 MySQL 认为最优的方案去执行
>
> ​	**执行器：** 执行语句，调用存储引擎的接口获取数据，执行结束后返回结果
>
> **存储引擎层**：存储数据和索引，提供数据读写接口
>
> <img src="https://oss.javaguide.cn/javaguide/13526879-3037b144ed09eb88.png" alt="img" style="zoom: 80%;" />

> **查询语句**的执行流程如下：权限校验 --->查询缓存--->分析器--->优化器--->权限校验--->执行器--->引擎
>
> **更新语句**执行流程如下：分析器---->权限校验---->执行器--->引擎---><u>redo log(prepare 状态)--->binlog--->redo log(commit 状态)</u>
>
> <img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/mysql-812fb038-39de-4204-ac9f-93d8b7448a18.jpg" alt="update 执行" style="zoom: 67%;" />



##### sql优化的思路

<img src="https://cdn.tobebetterjavaer.com/stutymore/mysql-20240327104050.png" alt="沉默王二：SQL 优化" style="zoom: 50%;" />



如何删除百万数据？





------

## Redis :white_check_mark:

##### 延时任务：订单在 10 分钟后未支付就失效，如何用 Redis 实现？:star:

> **Redis 过期事件监听**
>
> 思路：
>
> ​	Redis内置了一个`__keyevent@0__:expired` channel，负责监听 **key** 的**过期事件**
>
> ​	当一个 key 过期之后，Redis 会发布一个 key 过期的事件到`__keyevent@<db>__:expired`这个 channel 中
>
> ​	只需要监听这个 channel，就可以拿到过期的 key 的消息，进而实现了延时任务功能
>
> 缺陷：
>
> ​	①**时效性差**：Redis采用**定期+惰性**删除结合的方式，过期事件消息是在 Redis 服务器**删除 key 时**发布的，而不是一个 key 过期之后就会就直接发布
>
> ​	②会丢消息：chanel中的消息并**不会持久化**
>
> ​	③重复消费：发布订阅是**广播**模式，多个服务实例会重复处理消息
>
> 
>
> **Redisson 内置的延时队列**（DelayedQueue） :star:推荐
>
> * Redisson的延时队列采用的是Redis中的SortedSet数据结构，SortedSet 是一个有序集合，其中的每个元素都可以**设置一个分数**，代表该元素的权重。Redisson 利用这一特性，将需要延迟执行的任务插入到 SortedSet 中，并给它们设置相应的**过期时间作为分数**
> * 使用 `zrangebyscore` 命令扫描 SortedSet 中过期的元素，如果value小于当前时间戳，表示已经过期
> * 将这些过期元素从 SortedSet 中移除，并将它们加入到**就绪消息列表**中
> * 就绪消息列表是一个**阻塞队列**，有消息进入就会被监听到，从而处理延时任务
>
> 优势：
>
> ​	①减少了丢消息的可能：DelayedQueue会被持久化
>
> ​	②不存在消息重复消费：每个客户端都是从**同一个目标队列**中获取任务的
>
> 
>
> **消息队列RabbitMQ实现延迟队列**
>
> 给消息设置TTL + 死信队列



##### Redis为什么快？

不要只答因为是内存

> ①**基于内存的数据存储**，Redis 将数据存储在内存当中，使得数据的读写操作避开了磁盘 I/O （**根本原因**）
>
> ②Redis 基于 Reactor 模式设计开发了一套高效的**事件处理模型**，主要是单线程事件循环和 IO 多路复用
>
> ③**IO 多路复⽤**，基于 Linux 的 `epoll` 机制。该机制允许内核中同时存在多个监听套接字和已连接套接字，内核会一直监听这些套接字上的连接请求或者数据请求，一旦有请求到达，就会交给 Redis 处理，就实现了所谓的 Redis 单个线程处理多个 IO 读写的请求
>
> ④**高效的数据结构**，Redis 提供了多种高效的数据结构，如字符串（String）、列表（List）、集合（Set）、有序集合（Sorted Set）等，这些数据结构经过了高度优化，能够支持快速的数据操作
>

###### 为什么选择使用Redis？

考察是否有进行**技术选型**的对比

其他分布式缓存技术有：Memcache、Tendis（腾讯开源项目，已经不再更新维护）

> **Redis vs. Memcache**
>
> **数据类型**：
>
> ​	Redis 支持更丰富的数据类型（支持更复杂的应用场景）。Redis 不仅仅支持简单的 k/v 类型的数据，同时还提供 list、set、zset、hash 等数据结构的存储；
>
> ​	而 Memcached 只支持最简单的 k/v 数据类型。
>
> **数据持久化**：
>
> ​	Redis 支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用；
>
> ​	而 Memcached 把数据全部存在内存之中。
>
> ​	也就是说，Redis 有灾难恢复机制，而 Memcached 没有。
>
> **集群模式支持**：
>
> ​	Memcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据；
>
> ​	而 Redis 自 3.0 版本起是原生支持集群模式的。
>
> **线程模型**：
>
> ​	Memcached 是多线程、非阻塞 IO 复用的网络模型；
>
> ​	而 Redis 使用单线程的多路 IO 复用模型（Redis 6.0 针对网络数据的读写引入了多线程）。
>
> **特性支持**：
>
> ​	Redis 支持发布订阅模型、Lua 脚本、事务等功能，并且，Redis 支持更多的编程语言。
>
> ​	而 Memcached 不支持。
>
> **过期数据删除**：
>
> ​	Memcached 过期数据的删除策略只用了惰性删除，
>
> ​	而 Redis 同时使用了惰性删除与定期删除。







##### Redis数据结构及其底层数据结构

> ![img](面经.assets/9fa26a74965efbf0f56b707a03bb9b7f.png)
>
> 压缩列表 被 listpack 取代
>
> 双向链表 被 quicklist 取代



###### Redis的有序集合SortedSet底层实现？ :star:

Redis 的 Sorted Set 在**元素较少时**使用压缩列表（ziplist）作为底层实现，是为了**节省内存、提高缓存效率**

当元素较多或成员或分数较大时，才会自动切换为跳表（skiplist）+哈希表结构

​	跳表只能根据score来查询

​	哈希表解决了根据key来查询分数

> **跳表**
>
> 跳表是对有序的单链表进行多级索引的建立，利用的是二分查找的思想。因为对即使是有序的单链表进行查找都需要从头节点开始遍历，所以出现了跳表这一高级数据结构。
>
> ![img](https://i-blog.csdnimg.cn/blog_migrate/25e6411ae7b3b2f0154da760cbc86343.png)
>
> [如图要找到6这个节点，可以先从三级索引出发，发现6大于1小于14，进入二级索引，发现6大于0小于7，进入一级索引发现6大于4小于7，最后进入单链表，从4出发可以找到6。]
>
> 查找的时间复杂度：O(logN)
>
> 插入和删除的时间复杂度为：O(logN)
>
> 空间复杂度：O(2N)
>
> **跳表是线程安全的**：
>
> Redis 本身是 **单线程** 处理命令的
>
> Redis 提供的 `ZADD`、`ZREM`、`ZINCRBY`、`ZRANGE` 等命令，**都是单独执行的原子操作**，不会被其他命令打断
>
> **原因：**
>
> `平衡树 vs 跳表`：平衡树的插入、删除和查询的时间复杂度和跳表一样都是 **O(log n)**。对于**范围查询**来说，平衡树也可以通过中序遍历的方式达到和跳表一样的效果。但是它的<u>每一次插入或者删除操作都需要保证整颗树左右节点的绝对平衡，只要不平衡就要通过旋转操作来保持平衡，这个过程是比较耗时的</u>。跳表诞生的初衷就是为了克服平衡树的一些缺点。跳表使用概率平衡而不是严格强制的平衡，因此，跳表中的插入和删除算法比平衡树的等效算法简单得多，速度也快得多。
>
> `红黑树 vs 跳表`：相比较于红黑树来说，跳表的实现也更简单一些，不需要通过<u>旋转和染色（红黑变换）</u>来保证黑平衡。并且，按照区间来查找数据这个操作，红黑树的效率没有跳表高。
>
> `B+树 vs 跳表`：B+树更适合作为数据库和文件系统中常用的索引结构之一，它的核心思想是<u>通过可能少的 IO 定位到尽可能多的索引来获得查询数据</u>。对于 Redis 这种内存数据库来说，它对这些并不感冒，因为 Redis 作为内存数据库它不可能存储大量的数据，所以对于索引不需要通过 B+树这种方式进行维护，只需按照概率进行随机维护即可，节约内存。而且使用跳表实现 zset 时相较前者来说更简单一些，在进行插入时只需通过索引将数据插入到链表中合适的位置再随机维护一定高度的索引即可，也不需要像 B+树那样插入时发现失衡时还需要对节点分裂与合并。

详情：https://javaguide.cn/database/redis/redis-skiplist.html



###### Redis压缩列表zipList

> ziplist 使用**顺序存储**（借助特定**比特位**表示**前后节点的长度**达到节省内存的效果）
>
> ![img](https://i-blog.csdnimg.cn/img_convert/9e4298e507c8d60d3c80a4db61692a84.png)
>
> * zlbytes：固定占用 4 字节，记录整个压缩列表占用的**字节总数**
> * zltail：固定占用 4 字节，记录**尾节点**（entryN）到压缩列表起始位置的**字节总数**
> * zllen：固定占用 2 字节，记录存储的**节点个数**
> * entryX：**内存大小不定**，由存储的内容决定
> * zlend：固定占用 1 字节，且为固定值 0xFF，用于标记**压缩列表的结束**
>
> 每一个 `entry` 的内部结构
>
> ![img](https://i-blog.csdnimg.cn/img_convert/b720a166d0d779d3a3509485fdfce249.png)
>
> 其中各个字段的含义如下：
>
> - previous_entry_length：存储大小为<u>1字节或5字节</u>，表示**前一个节点占用的字节数**
>
>   - 如果上一个节点长度<254 则使用 1 字节来存储，否则使用 5 字节存储
>   - 用来实现反向遍历
>
> - encoding：存储大小为1字节或2字节或5字节，表示**编码方式**以及**内容占用的长度**
>
> - content：实际存储的内容
>
>   case1：content 内容为字符串等字节数组
>
>   如果开头是 00 则表示 encoding 占用 1 字节，剩余 6 位为 content 的长度
>   如果开头是 01 则表示 encoding 占用 2 字节，剩余 14 位为 content 的长度
>   如果开头是 10 则表示 encoding 占用 5 字节，剩余 4 字节为 content 的长度
>
>   
>
>   case2：content 内容为整数（此时 encoding 占用 1 字节）
>
>   如果是 11000000，则表示 content 整数类型为 int16_t
>   如果是 11010000，则表示 content 整数类型为 int32_t
>   如果是 11100000，则表示 content 整数类型为 int64_t
>   如果是 11110000，则表示 content 整数类型为 24 位有符号整数
>   如果是 11111110，则表示 content 整数类型为 8 位有符号整数
>   如果是 1111xxxx，则表示 content 实际内容为 xxxx 部分（此时无需content额外存储）
>
> 存在**级联更新**的问题：
>
> 由于 entry1 没有前一个节点，因此 entry1 的 previous_entry_length 占用 1 字节，但是现在我们向 entry1 前插入一个新节点（长度>254）此时 entry1 需要更新 previous_entry_length 为 5 字节空间，所以entry1的长度就变了，此时 entry2 同样需要更新字段长度，... ，因此引发了后续级联的更新
> 
>



###### Redis的HyperLogLog

> 基数计数概率算法，它可以在固定大小（**12KB**） 的空间内高效地**估算去重后的元素个数**，并且误差控制在 **0.81%以内**
>
> **基数（Cardinality）**：一个集合中 **唯一元素** 的个数
>
> 用Set来去重的空间复杂度为O(n)
>
> 步骤：
>
> * 对元素进行哈希，把原始数据变成 **随机分布的二进制数**
> * 寻找二进制哈希值的前导零数量（连续 `0` 的个数）
> * 存储多个这样的统计值，取 平均 来估算基数



###### Redis的布隆过滤器底层实现

> 布隆过滤器由一个**长度为 m 的位数组** 和 **k 个哈希函数**组成
>
> 位数组中每一位为一个bit，0和1
>
> 添加一个key时，将这个key经过k个哈希函数得到k个下标，把这k个下标对应的位都置为1
>
> 查询一个key时，会先经过k个哈希函数得到k个不同的下标
>
> ​	如果这k个下标所对应的位都为1，那么认为这个key**可能存在**Redis中，继续访问
>
> ​	如果这k个下标有一个不为1，那么这个key**一定不存在**Redis中，直接返回null
>
> ![三分恶面渣逆袭：布隆过滤器](https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/redis-d0b8d85c-85dc-4843-b4be-d5d48338a44e.png)
>
> 缺点：存在误判和删除困难
>
> ​	有可能某一位为1是因为其他键设置的
>
> ​	如果删除一个键，不能单纯的就把其对应的k个下标的位都置为0，所以不支持删除操作
>
> 增大数组长度m，优化哈希函数来**减少误判率**

**应用**：快速的去判断url是否重复



**位数组长度（m）和哈希函数数量（k）的选择**：

> 位数组长度（m）的选择：
>
> 假设我们有 `n` 个元素要插入到布隆过滤器中，且希望过滤器的误判率为 `p`。那么，位数组的长度 `m` 可以通过以下公式估算：
> $$
> m= -\frac{n \cdot \ln(p)}{(\ln 2)^2}
> $$
> 其中：
>
> - `n` 是要插入布隆过滤器的元素数量
> - `p` 是允许的误判率
> - `ln` 是自然对数
>
> 哈希函数数量（k）的选择：
>
> 哈希函数的数量 `k` 会影响误判率，合理的选择能够优化误判率。`k` 可以通过以下公式估算：
> $$
> k = \frac{m}{n} \cdot \ln 2
> $$
>
> - `m` 是位数组的长度
> - `n` 是元素数量



###### Redis的String的底层实现：SDS

> 在 Redis 中，`string`（字符串）数据类型的底层是通过 **SDS（Simple Dynamic String，简单动态字符串）** 结构实现的，而不是 直接使用 C 语言的 `char*`
>
> SDS 解决了 C 语言字符串的一些缺陷，比如无法动态扩展、容易发生缓冲区溢出等问题
>
> ```c
> struct sdshdr {
>  int len; // buf 中已使用的长度
>  int free; // buf 中未使用的长度
>  char buf[]; // 数据空间
> };
> ```
>
> 优化：因为 C 语⾔的字符串不记录⾃身的⻓度信息，当需要获取字符串⻓度时，需要遍历整个字符串，时间复杂度为 O(N)
>
> ⽽ SDS 保存了⻓度信息，这样就将获取字符串⻓度的时间由 O(N) 降低到了 O(1)
>
> - **SDS 不仅可以保存文本数据，还可以保存二进制数据**。因为 SDS 使用 len 属性的值而不是空字符来判断字符串是否结束，并且 SDS 的所有 API 都会以处理二进制的方式来处理 SDS 存放在 buf[] 数组里的数据。所以 SDS 不光能存放文本数据，而且能保存图片、音频、视频、压缩文件这样的二进制数据。
> - **SDS 获取字符串长度的时间复杂度是 O(1)**。因为 C 语言的字符串并不记录自身长度，所以获取长度的复杂度为 O(n)；而 SDS 结构里用 len 属性记录了字符串长度，所以复杂度为 O(1)。
> - **Redis 的 SDS API 是安全的，拼接字符串不会造成缓冲区溢出**。因为 SDS 在拼接字符串之前会检查 SDS 空间是否满足要求，如果空间不够会自动扩容，所以不会导致缓冲区溢出的问题。

> ①如果字符串内容是一个**数值**，并且在long的范围内：
>
> 那么字符串对象会将整数值保存在字符串对象结构的`ptr`属性里面（将`void*`转换成 long），并将字符串对象的编码设置为`int`
>
> <img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/redis/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/int.png" alt="img" style="zoom:50%;" />
>
> ②如果字符串内容是一个**字符串**，并且长度小于**32字节**：
>
> 那么字符串对象将使用一个简单动态字符串（SDS）来保存这个字符串，并将对象的编码设置为`embstr`
>
> ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/redis/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/embstr.png)
>
> ③如果字符串内容长于32字节：
>
> 那么编码类型改为`raw`
>
> ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/redis/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/raw.png)
>
> 注意 `embstr` 和 `raw` 在内存上的区别：
>
> ​	`embstr`是**一次性分配一块**连续的内存，存储SDS以及字符串内容
>
> ​	`raw`是分配两块内存，一块存储SDS，另一块存储字符串内容
>
> 所以：`embstr`能提高CPU性能，每次读取一块连续的内存，并且释放时也只需要一次操作
>
> 但是：`embstr`的字符串内容是不可以修改的，如果要修改只能先转码为`raw`





###### Redis的hash数据结构的底层实现

> - 如果哈希类型**元素个数小于 512 个**（默认值，可由 hash-max-ziplist-entries 配置）
>
>   并且**所有值小于 64 字节**（默认值，可由 hash-max-ziplist-value 配置）
>
>   那么 Redis 会使用 **压缩列表** 作为 Hash 类型的底层数据结构 （Redis7使用 **listpack** 来替代 压缩列表）
>
> - 如果哈希类型元素不满足上面条件，Redis 会使用**哈希表**作为 Hash 类型的底层数据结构

Redis中字典dict的rehash方式

> dict的底层是由**数组+链表**实现的，链表解决哈希冲突
>
> 扩容：**渐进式rehash**
>
> 字典结构内部包含 **两个 hashtable**，通常情况下只有一个哈希表 `ht[0]` 有值，在扩容的时候，把 `ht[0]`里的值 rehash 到 `ht[1]`
>
> 然后进行 渐进式 rehash：rehash 的动作并不是一次性、集中式地完成的，而是分多次、渐进式地完成的
>
> 待搬迁结束后，`h[1]`就取代 `h[0]`存储字典的元素



###### Redis的List数据结构的底层实现

> - 如果列表的元素个数小于 `512` 个（默认值，可由 `list-max-ziplist-entries` 配置）
>
>   并且列表每个元素的值都小于 `64` 字节（默认值，可由 `list-max-ziplist-value` 配置）
>
>   那么Redis 会使用 **压缩列表** 作为 List 类型的底层数据结构
>
> - 如果列表的元素不满足上面的条件，Redis 会使用**双向链表**作为 List 类型的底层数据结构
>
> 后面被quicklist取代



###### Redis的Set数据结构的底层实现

> 如果集合中的元素都是整数且元素个数小于 512 （默认值，set-maxintset-entries配置）个，Redis 会使用**整数集合**作为 Set 类型的底层数据结构；
>
> 如果集合中的元素不满足上面条件，则 Redis 使用**哈希表**作为 Set 类型的底层数据结构



##### Redis单线程模型

###### Redis6.0之前为什么不使用多线程？

> 单线程编程容易并且更容易维护；
>
> Redis 的性能瓶颈不在 CPU ，主要在内存和**网络**； ==> 所以之后引入了网络的多线程
>
> 多线程就会存在死锁、线程上下文切换等问题，甚至会影响性能
>
> **Redis 在 2.6 版本**，会启动 2 个后台线程，分别处理**关闭文件**、**AOF 刷盘**这两个任务；
>
> **Redis 在 4.0 版本之后**，新增了一个新的后台线程，用来异步释放 Redis 内存，也就是 **lazyfree 线程**
>
> 例如执行 unlink key / flushdb async / flushall async 等命令，会把这些删除操作交给后台线程来执行，好处是不会导致 Redis 主线程卡顿。因此，当我们要删除一个大 key 的时候，不要使用 del 命令删除，因为 del 是在主线程处理的，这样会导致 Redis 主线程卡顿，因此我们应该使用 unlink 命令来异步删除大key

###### Redis6.0 之后为何引入了多线程

> **Redis6.0 引入多线程主要是为了提高网络 IO 读写性能**，因为这个算是 Redis 中的一个性能瓶颈（Redis 的瓶颈主要受限于内存和网络）
>
> Redis 6.0 对于网络 I/O 采用多线程来处理。**但是对于命令的执行，Redis 仍然使用单线程来处理**
>
> 配置 redis.conf
>
> ```conf
> io-threads-do-reads yes  # 默认不开启多线程
> io-threads: 4  # 4核CPU建议2~3  8核建议6  
> ```
>
> 

###### Redis的I/O多路复用模型

使用`epoll`

<img src="https://cdn.tobebetterjavaer.com/stutymore/redis-20240918114125.png" alt="有盐先生：IO 多路复用" style="zoom:67%;" />



###### Redis的单线程模型

<img src="面经.assets/redis单线程模型.drawio.png" alt="img" style="zoom: 50%;" />



##### Redis键过期删除策略 :star:

> Redis 采用的是 **定期删除+惰性删除** 结合的策略，这也是大部分缓存框架的选择。定期删除对内存更加友好，惰性删除对 CPU 更加友好
>
> 惰性删除时只会在取出/查询/修改 key 的时候才对数据进行过期检查，如果过期了就直接删除返回null
>
> Redis 的定期删除过程是随机的（周期性地随机从设置了过期时间的 key 中**抽查一批**），所以并不保证所有过期键都会被立即删除。这也就解释了为什么有的 key 过期了，并没有被删除。并且，Redis 底层会通过**限制删除操作执行的时长**和频率来减少删除操作对 CPU 时间的影响。
>
> 定期删除还会受到**执行时间**和**过期 key 的比例**的影响：
>
> - 执行时间已经超过了阈值，那么就**中断**这一次定期删除循环，以避免使用过多的 CPU 时间。
> - 如果这一批过期的 key 比例超过一个比例，就会重复执行此删除流程，以更积极地清理过期 key。相应地，如果过期的 key 比例低于这个比例，就会中断这一次定期删除循环，避免做过多的工作而获得很少的内存回收。
>
> Redis 7.2 版本的执行时间阈值是 **25ms**，过期 key 比例设定值是 **10%**，每次抽查**20**个过期键进行删除，定期删除的频率是由 **hz** 参数控制，默认为**10**（每秒10次）

###### Redis如何维护键的过期时间？/如何判断一个键是否过期？

> Redis 通过一个叫做**过期字典**来保存数据过期的时间，只有设置了过期时间的key才会存在过期字典中
>
> * 过期字典的**键**指向 Redis 数据库中的某个 key(键)
> * 过期字典的值是一个 long long 类型的整数，这个整数保存了 key 所指向的数据库键的过期时间（毫秒精度的 UNIX 时间戳）
>
> 在查询一个 key 的时候，Redis 首先检查该 key 是否存在于过期字典中（时间复杂度为 O(1)），如果不在就直接返回【即该key没有设置过期时间】，在的话需要判断一下这个 key 是否过期，过期直接删除 key 【惰性删除】然后返回 null

###### 如果有大量Key集中过期怎么办？==> 缓存雪崩

> **开启 lazy free 机制**：Redis 会在后台异步删除过期的 key，不会阻塞主线程的运行
>
> **尽量避免 key 集中过期**: 在设置键的过期时间时尽量随机一点



##### Redis内存淘汰策略

问题：MySQL 里有 2000w 数据，Redis 中只存 20w 的数据，如何保证 Redis 中的数据都是**热点**数据?

回答这个问题：使用Redis的LRU或者LFU内存淘汰策略

> 即Redis的**内存淘汰策略**
>
> 1. **volatile-lru（least recently used）**：从已设置过期时间的数据集（`server.db[i].expires`）中挑选最近最少使用的数据淘汰
> 2. **volatile-ttl**：从已设置过期时间的数据集（`server.db[i].expires`）中挑选将要过期的数据淘汰
> 3. **volatile-random**：从已设置过期时间的数据集（`server.db[i].expires`）中任意选择数据淘汰
> 4. **allkeys-lru（least recently used）**：从数据集（`server.db[i].dict`）中移除最近最少使用的数据淘汰
> 5. **allkeys-random**：从数据集（`server.db[i].dict`）中任意选择数据淘汰
> 6. **no-eviction**（默认内存淘汰策略）：禁止驱逐数据，当**内存不足**以容纳新写入数据时，新写入操作会**报错**
>
> 4.0 版本后增加以下两种：
>
> 1. **volatile-lfu（least frequently used）**：从已设置过期时间的数据集（`server.db[i].expires`）中挑选最不经常使用的数据淘汰。
> 2. **allkeys-lfu（least frequently used）**：从数据集（`server.db[i].dict`）中移除最不经常使用的数据淘汰。

==> 汇总

一类是volatile：lru、lfu、random、ttl （只针对设置了过期时间key）

一类是allkeys：lru、lfu、random

最后一种是no-eviction（默认），不清理过期键，内存满了报错



###### Redis 报内存不足怎么处理？

> ①**扩大内存**：修改配置文件 redis.conf 的 maxmemory 参数，增加 Redis 可用内存
>
> ​					 也可以通过命令 set maxmemory 动态设置内存上限
>
> ②**过期键清除**：及时释放内存空间
>
> ③使用 Redis **集群**模式，进行**横向扩容**



##### :star:Redis缓存穿透、缓存击穿、缓存雪崩

> ###### 缓存穿透
>
> **缓存穿透**：同一时间内有**大量不存在/不合理的key**的请求，这些key根本不存在Redis缓存中，也不存在数据库中，导致所有的请求都到达数据库，对数据库造成巨大的压力，从而因此宕机
>
> **解决方案**：	
>
> ①**缓存无效 key**：如果缓存和数据库都查不到某个 key 的数据就写一个到 Redis 中去并设置过期时间
>
> 可以解决请求的 key 变化不频繁的情况，如果黑客恶意攻击，每次构建不同的请求 key，会导致 Redis 中缓存大量无效的 key 
>
> ②**布隆过滤器** :star:：使用一个较大的 bit 数组来保存所有的数据，数组中的每个元素都只占用 1 bit ，并且每个元素只能是 0 或者 1
>
> 都存放在布隆过滤器中，当用户请求过来，先判断用户发来的请求的值是否存在于布隆过滤器中。不存在的话，直接返回请求参数错误信息给客户端，存在的话才会走下面的流程。
>
> ③**接口限流**：根据用户或ip进行接口限流，对于异常频繁的访问行为可以采用黑名单机制
>
> ###### 缓存击穿
>
> **缓存击穿**：请求的 key 对应的是 **热点数据** ，该数据 <u>存在于数据库中，但不存在于缓存中</u>（通常原因是缓存过期了），这就可能会导致瞬时大量的请求直接打到了数据库上。
>
> （例如：秒杀进行过程中，缓存中的某个秒杀商品的数据突然过期，这就导致瞬时大量对该商品的请求直接落到数据库上，对数据库造成了巨大的压力）
>
> **解决方案**：
>
> ①**永不过期**（不推荐）：设置热点数据永不过期或者过期时间比较长
>
> ②**提前预热**（推荐）：针对热点数据提前预热，将其存入缓存中并设置合理的过期时间比如秒杀场景下的数据在秒杀结束之前不过期
>
> ③**加锁**（看情况）：在缓存失效后，通过设置<u>互斥锁确保只有一个请求去查询数据库</u>并更新缓存
>
> ###### 缓存雪崩
>
> **缓存雪崩**：缓存在**同一时间大面积的失效**，导致大量的请求都直接落到了数据库上，对数据库造成了巨大的压力。
>
> 解决方案：
>
> ①**设置随机失效时间**
>
> ②**提前预热**
>
> ③**持久缓存策略**

缓存穿透 vs. 缓存击穿：

* 缓存穿透的key即不存在数据库中，也不存在缓存中
* 缓存击穿的key不存在缓存中，但存在数据库中（hotkey)

**提前预热**：

* 使用定时任务，比如 xxl-job
* 使用消息队列，比如 Kafka
* 每次启动将热点数据先加载到缓存中

###### 怎么查看热Key？

> ①`monitor`命令可以监控到 Redis 执行的所有命令
>
> ②`--bigkeys`来获取热键
>
> ③`redis-rdb-tools` 由python写好的分析RDB的工具

###### 如何处理热Key？

> 可删除key：
>
> ​	`UNLINK` 命令安全地**删除**大 Key，以**异步**的方式，逐步地清理传入的大 Key
>
> ​	`SCAN` 命令执行增量迭代**扫描** key，然后判断进行删除（**非阻塞**）
>
> ​		SCAN本身不删除key，只是扫描出key
>
> 不可删除key：**压缩和拆分 key**
>
> ​	当 vaule 是 string 时，比较难拆分，则使用序列化、压缩算法将 key 的大小控制在合理范围内
>
> ​	当 value 是 list/set 等集合类型时，根据预估的数据规模来进行分片，不同的元素计算后分到不同的片



##### 缓存和数据库如何保证一致性？:star:

> 1.更新数据库并更新缓存 | 先更新缓存再更新数据库 ：这两种都仍然存在读脏数据的问题
>
> 2.先删除缓存再更新数据库：读请求可能在写请求删除缓存之后、更新数据库之前到来，还是会读到**数据库的脏数据**
>
> 3.**更新数据库并删除缓存**：读请求在写请求更新数据库之后，删除缓存之前到，还是会读到**缓存中的脏数据**
>
> 4.**延迟双删**：方法2需配合「消息队列」，先删除缓存，再更新数据库，通过消息队列设置延迟任务，延迟一会再删除一次缓存
>
> ​	设置这一个延迟任务的可能是读请求可能在写请求修改数据库之前读到了脏数据，在写请求删除缓存之后，又把这个脏数据写到缓存中了，所以再延迟删一次缓存可以减少这种问题的出现
>
> **延迟时间**：根据**业务读取数据平均耗时**来确定
>
> ​		确保更新数据库后，大部分的请求的执行完毕，再把这个错误的缓存删掉
>
> ​		以更新完数据库前一刻开始算起，一个读请求恰好读到了数据库中的脏数据，它会把这个脏数据写入缓存
>
> ​		再它写入缓存后，把缓存删除就行
>
> ​		还要考虑数据库主从复制延迟



##### Redis三种缓存读写策略

> **①CAP(Cache Aside Pattern) 旁路缓存模式**
>
> ​	读：客户端从缓存中查询，缓存命中直接返回，不命中则客户端又去数据库中查询，客户端把数据写入缓存
>
> <img src="https://oss.javaguide.cn/github/javaguide/database/redis/cache-aside-read.png" alt="img" style="zoom: 80%;" />
>
> ​	写：直接修改db，再删除缓存
>
> <img src="https://oss.javaguide.cn/github/javaguide/database/redis/cache-aside-write.png" alt="img" style="zoom:80%;" />
>
> **②Read/Write Through Pattern 读写穿透**
>
> ​	把 cache 视为主要数据存储，客户端只跟cache交互，cache单独跟数据库查询
>
> ​	读：缓存命中直接返回，若不命中，先从 db 加载，写入到 cache， 后返回给客户端
>
> <img src="https://oss.javaguide.cn/github/javaguide/database/redis/read-through.png" alt="img" style="zoom:80%;" />
>
> ​	写：缓存存在，先修改缓存，再又缓存自己同步更新到db中（同步）
>
> ​			缓存不存在，直接更新db
>
> ![img](https://oss.javaguide.cn/github/javaguide/database/redis/write-through.png)
>
> **③Write Behind Pattern 异步缓存写入**
>
> ​	读：同上
>
> ​	写：异步更新到数据库（一致性差，性能高，适合浏览量、点赞量这些数据）



##### Redis三种持久化策略 :star:

Redis持久化的意义：重启机器、机器故障之后恢复数据、Redis 集群的主从节点通过 RDB 文件同步数据

> ①**RDB**：快照 【默认】
>
> - **每xx秒内**只要**有xx个key**发生变化，就调用一次bgsave进行快照
> - save：同步保存操作，会阻塞主线程，通过`save <seconds> <changes>`指令配置自动触发 RDB 持久化的条件
> - bgsave：fork出一个子进程进行保存【默认】
>
> 执行 bgsave 过程中，Redis 依然**可以继续处理操作命令**的，也就是数据是能被修改的
>
> 关键的技术就在于**写时复制技术**：
>
> ​	在bgsave时，会通过fork创建一个子进程，而子进程和父进程共享同一片内存数据
>
> ​	这样可以减少创建子进程的开销
>
> ​	<img src="面经.assets/c34a9d1f58d602ff1fe8601f7270baa7.png" alt="图片" style="zoom:50%;" />
>
> ​	如果此时父进程修改了内存数据，那么就会复制一份内存数据出来（写时复制）
>
> ​	所**以父线程修改的值**在本次RDB持久化中是**不能被保存**下来的
>
> ​	缺点就是有可能父进程修改了所有的内存页，那么每页都会复制两份
>
> ​	<img src="面经.assets/ebd620db8a1af66fbeb8f4d4ef6adc68.png" alt="图片" style="zoom:50%;" />



> ②**AOF**：只追加文件（append-only file） 【Redis6.0默认开启】
>
> - 过程：每执行一条会更改 Redis 中的数据的命令，Redis 就会将该命令写入到 **AOF 缓冲区** `server.aof_buf` 中，然后再写入到 **AOF 文件**中（此时还在系统内核缓存区未同步到磁盘），最后再根据持久化方式（`fsync`策略）的配置来决定何时将系统内核缓存区的数据**同步到硬盘**中的
>
> - AOF具体执行流程：
>
>   ①**命令追加（append）**：所有的写命令会追加到 **AOF 缓冲区**中
>
>   ②**文件写入（write）**：将 AOF 缓冲区的数据写入到 **AOF 文件**中
>
>   ​    这一步需要调用`write`函数（系统调用），将数据写入到了**系统内核缓冲区**之后直接返回
>
>   ③**文件同步（fsync）**：AOF文件根据对应的持久化方式（`fsync` 策略）向硬盘做同步操作
>
>   ​    这一步需要调用 `fsync` 函数（系统调用），`fsync` 针对单个文件操作，对其进行强制硬盘同步，`fsync` 将阻塞直到写入磁盘完成后返回，保证了数据持久化。
>
>   ④**文件重写（rewrite）**：随着 AOF 文件越来越大，需要定期对 AOF 文件进行重写，达到压缩的目的
>
>      这一步不会阻塞主线程，是fork出一个子线程来执行的
>
>   ⑤**重启加载（load）**：当 Redis 重启时，可以加载 AOF 文件进行数据恢复
>
>   <img src="https://cdn.xiaolincoding.com//mysql/other/4eeef4dd1bedd2ffe0b84d4eaa0dbdea.png" alt="img" style="zoom: 50%;" />
>
> - `fsync`策略：
>
>   - `appendfsync always`：每次wirte后就同步
>   - `appendfsync everysec`：每秒同步
>   - `appendfsync no`：由操作系统决定
>
> AOF文件记录的内容：
>
> <img src="面经.assets/337021a153944fd0f964ca834e34d0f2.png" alt="img" style="zoom:67%;" />
>
> `*3` ：表示有三个部分
>
> `$n`：表示字符串的长度



> ③**RDB 和 AOF 的混合持久化**（Redis 4.0 新增）
>
> * 先RDB一次得到快照，然后用AOF记录快照之后的写操作
>
> * AOF 重写的时候就直接**把 RDB 的内容**写到 AOF 文件开头
>
>   
>
> <img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/redis-19c531e5-da95-495a-a4c4-d63a0b8bba95.png" alt="三分恶面渣逆袭：混合持久化" style="zoom:67%;" />
>
> 需要恢复数据时，Redis 先加载 RDB 文件来**恢复到快照时刻**的状态，然后应用 RDB **之后记录的 AOF 命令**来恢复之后的数据更改，既快又可靠

###### AOF重写期间，Redis写命令会执行两次？

> 在AOF重写期间，写命令会同时写入到AOF缓冲区和旧的AOF文件中
>
> 防止在 AOF 重写尚未完成时，Redis 发生崩溃，导致数据丢失（也就是AOF缓冲区的数据没了）
>
> 当重写完成后，会通过原子操作将新的 AOF 文件替换旧的 AOF 文件

###### AOF文件重写具体操作

> 重写 AOF 过程是由后台子进程 `bgrewriteaof` 来完成的
>
> AOF 重写机制是在重写时，**读取当前数据库中的所有键值对**，然后将每一个键值对用**一条命令**记录到「新的 AOF 文件」，等到全部记录完后，就将新的 AOF 文件替换掉现有的 AOF 文件

###### RDB vs. AOF

> RDB性能好，恢复快，RDB生成快照时对系统影响小
>
> AOF适合频繁写的场景，数据完整性大，一般只会丢失1s内的数据。但恢复慢，需要执行所有写操作





##### Redis常见阻塞原因，如何排查

> 1.O(N)命令：如Keys *（返回所有符合规则的Key）、HGETALL（返回Hash中所有的键值对）
>
> 2.SAVE创建RDB快照：save会阻塞主线程，应该使用bgsave
>
> 3.AOF刷盘阻塞：三种策略always、everysec、no
>
> 4.AOF重写阻塞：虽然重写会fork 出一条子线程来将文件重写，但是当新AOF重写完毕后，服务器会将**重写缓冲区**中的所有内容追加到新 AOF 文件的末尾，使得新的 AOF 文件保存的数据库状态与现有的数据库状态一致。这一步将缓冲区中新数据写到新文件的过程中会产生**阻塞**
>
> 5.大Key、查找大Key、删除大Key：都会阻塞主线程
>
> 6.清空数据库：flushdb、flushall
>
> 7.集群扩容：在扩缩容的时候，需要进行数据迁移。而 Redis 为了保证迁移的一致性，迁移所有操作都是同步操作
>
> 8.网络问题
>
> ###### 排查：
>
> ![Redis阻塞排查](https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/redis-e6a35258-7a78-4489-90b7-e47a4190802b.png)



##### Redis高可用

###### 三种高可用模式

> ①单机模式
>
> ②主从复制：主节点将数据复制到一个或多个从节点，实现读写分离
>
> ③哨兵模式(sentinel)：用于**监控**主节点和从节点的状态，实现自动故障转移。如果主节点发生故障，哨兵可以自动将一个从节点升级为新的主节点
>
> ④集群模式：Redis 集群通过分片的方式存储数据，每个节点存储数据的一部分



###### 主从复制如何实现？

> **【第一步】构建主从关系**：命令`replicaof`
>
> 想要让B服务器作为A服务器的从节点，只需在B服务器上执行：
>
> ```
> replicaof <服务器 A 的 IP 地址> <服务器 A 的 Redis 端口号>
> ```
>
> **【第二步】数据同步**：将主节点中的所有数据同步到从节点
>
> ①Redis2.8之前：`sync` 命令，每次都进行主从的全量复制
>
> ​	slave发起 `sync` 请求后，master生成一个RDB并发送给slave，slave根据RDB进行复制，在复制的过程中，master会将期间的写请求写入缓冲区，等RDB复制完毕后，再发给slave进行同步
>
> ②Redis2.8：`psync` 命令，包括全量复制和增量复制
>
> ​	全量复制：一般用于初次复制场景，如果slave/master宕机，也得进行全量复制
>
> ​	增量复制：使用 `psync {runId} {offset}` 命令实现，`runid `为 master 的 runid，`offset` 为上一次同步到的偏移量
>
> ③Redis4.0：`psync2.0`，使用 `master_repl_offset` 和 `sencond_repl_offset` 来实现增量复制
>
> **【第三步】增量同步**：主节点每次都会将写命令转发给从节点
>
> <img src="面经.assets/ea4f7e86baf2435af3999e5cd38b6a26.png" alt="图片" style="zoom:67%;" />
>
> 可以设置充当**经理角色**的从节点，来分摊主节点的压力，其他从节点可以从经理进行数据备份

###### 主从节点之间的心跳机制

> ###### 怎么判断 Redis 某个节点是否正常工作？
>
> Redis 判断节点是否正常工作，基本都是通过互相的 ping-pong 心态检测机制，如果有一半以上的节点去 ping 一个节点的时候没有 pong 回应，集群就会认为这个节点挂掉了，会断开与这个节点的连接。
>
> Redis 主从节点发送的心态间隔是不一样的，而且作用也有一点区别：
>
> - **Redis 主节点默认每隔 10 秒对从节点发送 ping 命令**，判断从节点的存活性和连接状态，可通过参数repl-ping-slave-period控制发送频率
> - **Redis 从节点每隔 1 秒发送 replconf ack{offset} 命令**，给主节点上报自身当前的复制偏移量，目的是为了：
>   - 实时监测主从节点网络状态
>   - 上报自身复制偏移量， 检查复制数据是否丢失， 如果从节点数据丢失， 再从主节点的复制缓冲区中拉取丢失数据



###### 主从复制的问题——脑裂（数据不一致）

> Redis的脑裂问题是指：在主从模式下，由于网络分区或节点故障，可能导致**系统中出现多个主节点（大脑分裂）**，从而引发数据不一致、数据丢失等问题
>
> 使用哨兵模式、集群模式来解决

###### 如何解决主从数据不一致的问题

> ①监控主从复制进度：通过 Redis 的 `INFO replication` 命令监控主从节点的复制进度，及时发现和处理复制延迟
>
> ​	获取主节点的 `master_repl_offset` 和从节点的 `slave_repl_offset`
>
> ​	计算两者的差值，如果差值大于阈值，那么强制读到主Redis上，停止从Redis的读取服务
>
> <img src="https://cdn.tobebetterjavaer.com/stutymore/redis-20240709135618.png" alt="极客时间：Redis 核心技术与实战" style="zoom: 20%;" />



###### 哨兵模式 Sentinel

<img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/redis-8b1a055c-f077-49ff-9432-c194d4fc3639.png" alt="三分恶面渣逆袭：Redis Sentinel" style="zoom: 80%;" />

哨兵的工作流程包括定时监控、主观下线和客观下线、领导者 Sentinel 节点选举、故障转移等

> 定时监控：每个 Sentinel 实例会定期通过 PING 命令向主节点和从节点发送**心跳包**，检测是否正常
>
> 主观下线：如果一个节点长时间没有响应 PING 命令，Sentinel 会将该节点标记为主观下线
>
> 客观下线：当**一半以上的Sentinel** 同时认为一个节点不可用时，该节点被标记为客观下线
>
> 节点选举：当**主节点**被确认下线后，Sentinel 之间会通过**Raft选举算法**进行协商，选出一个 **Sentinel Leader 节点** 来负责执行故障转移
>
> 故障转移：将某个从节点提升为新的主节点，**通知**其他从节点重新复制新的主节点的数据
>
> ​	新的主节点：过滤掉**主观下线**的从节点和**优先级为0**的从节点、选出**优先级最高(值越小)**的从节点、**复制偏移量最大**的从节点、**runid最小**的节点

Sentinel节点的个数：建议为奇数并且大于等于3



###### 集群模式 —— 高可用的终极模式

集群之间的节点没有主从之分，方便进行**横向扩容**，只需要往集群中加入Redis节点就可以

![7fefb5f3-1978-432b-9a6b-3e1608d033df](面经.assets/7fefb5f3-1978-432b-9a6b-3e1608d033df.png)

> Redis集群是**去中心化**的：
>
> * 任何一个Master节点挂了，都不会影响到其他的Master节点
> * Master之间使用**Gossip协议**进行交换新的数据
> * 
> * 一个Master挂了，会从它的slave节点中选举一个新的Master（所以一个Master至少得保证有一个Slave）
>
> **数据分区(片)**：集群将数据分散到多个节点
>
> ​	每个集群有 `16384` 个**哈希槽**，每个key 根据 `CRC16(key) % Slot_Num` 映射到对应的哈希槽上
>
> ​	这些哈希槽会被**均匀地分配**到集群中所有的 Master 节点上，可以通过`CLUSTER SLOT`命令查看节点和槽的映射关系
>
> ​	客户端发送请求会自动根据哈希值去对应的哈希槽上拿数据
>
> ​	有时候存在集群中哈希槽分配信息未同步的情况(例如扩容后重新分配哈希槽)，会导致客户端的请求通过计算后，到指定的master节点上读取数据，发现这个哈希槽已经被移到其他master节点上了，此时该master节点会返回一个**重定向**，告诉它这个哈希槽现在在哪个节点上
>
> <img src="面经.assets/image-20250330155343094.png" alt="image-20250330155343094" style="zoom:67%;" />
>
> 集群**水平扩容**：往集群中加入新的Redis Master节点，可以进行水平扩容
>
> ​	此时，需要重新分配哈希槽，并进行哈希槽中数据的**动态迁移**
>
> ​	为了保证slot迁移过程中服务仍然可用，Redis集群提供了两种重定向：`ASK` 和 `MOVE`
>
> `ASK`重定向：临时重定向
>
> ​	先到根据key的哈希值计算得到的slot所在的节点去查询，如果该key还在该节点中（还没迁移），直接返回
>
> ​	如果该key已经被迁移到新的节点了，那么返回一个`ASK`重定向错误，告诉它应该去新的节点上获取
>
> ​	客户端收到`ASK`后，会向新的节点发送`ASKING`到新节点中获取数据
>
> ​	这个`ASK`重定向是一次性/临时的，即客户端下一次请求同一个key，还是会去旧节点获取数据
>
> `MOVE`重定向：永久重定向
>
> ​	当slot已经完全迁移完毕，旧节点会返回一个`MOVE`重定向，告诉客户端去新的节点获取
>
> ​	并且客户端会对应**修改slot分配表**，即下次就会直接去新的节点获取数据

Redis CLuster模式相当于集合了主从复制和Sentinel两种模式



##### 在Springboot中怎么使用Redis？

###### 1.Spring Cache

> `@Cacheable`：缓存**方法的返回值**
>
> `@CachePut`：用于**更新缓存**，每次调用方法都会将结果重新写入缓存
>
> `@CacheEvict`：用于删除缓存
>
> 一般Dao层，getXXX函数，可以使用@Cacaheable注解来缓存返回值
>
> updateXXX函数，使用@CacheEvict来删除缓存（使用先更新数据库，再删除缓存的方式）

优点：使用方便，通过统一的接口来支持多种缓存实现

缺点：数据库和缓存一致性没法得到保证

###### 2.RedisTemplate

> 使用Redis配置类配置一个RedisTemplate对象
>
> ```java
> @Configuration
> public class RedisConfig {
> 
>     @Bean
>     public RedisTemplate<String, Object> redisTemplate(RedisConnectionFactory redisConnectionFactory) {
>         RedisTemplate<String, Object> template = new RedisTemplate<>();
>         template.setConnectionFactory(redisConnectionFactory);
> 
>         // 设置 key 和 value 的序列化方式
>         template.setKeySerializer(new StringRedisSerializer());
>         template.setValueSerializer(new StringRedisSerializer());
> 
>         return template;
>     }
> }
> ```
>
> 再调用RedisTemplate的API
>
> redisTemplate.opsForValue().set(key, value, time, TimeUnit.SECONDS)
>
> redisTemplate.delete(key)
>
> redisTemplate.opsForValue().get(key)

优点：可以实现延迟双删

缺点：使用起来没注解方便



##### Redis事务

将命令打包，一起执行

> multi：标记一个事务块的开始
>
> exec：执行所有事务块内的命令
>
> discard：取消事务，放弃执行事务块内的所有命令
>
> watch：监视一个或多个 key，如果在事务执行之前这个 key 被其他命令所改动，那么事务将被打断
>
> ​	watch命令可以用于**实现乐观锁**，如果一个key被修改那么打断事务，否则可以执行事务

注意：**不支持回退**，一旦exec执行过的命令都不会回滚

可以通过 Lua 脚本来实现事务的原子性

###### Pipeline

> Pipeline 是 Redis 提供的一种优化手段，允许客户端一次性向服务器发送多个命令，而不必等待每个命令的响应，从而减少网络延迟



##### 多级缓存

使用本地缓存作为一级缓存，使用分布式Redis作为二级缓存

本地缓存：Caffeine

<img src="https://cdn.nlark.com/yuque/0/2022/png/738439/1666689226778-fb790f43-dcd3-485b-97fc-73edcc061895.png" alt="img" style="zoom:67%;" />

###### 多级缓存如何保证数据一致性？

> ①更新数据库中的数据
>
> ②使用Cannl监听binlog，一旦发现数据修改，就通知Redis更新缓存
>
> ③同步Redis缓存：Redis主节点更新完毕后，通知其他从节点更新缓存
>
> ④更新本地缓存：由于本地缓存存在不同服务器的JVM中，所以可以解决消息队列来广播通知



------

## MQ :white_check_mark:

重点：Kafka、RabbitMQ

##### JMS vs AMQP

> JMS（JAVA Message Service，Java 消息服务）是 Java 的消息服务，JMS 的客户端之间可以通过 JMS 服务进行异步的消息传输。**JMS API 是一个消息服务的标准或者说是规范**，允许应用程序组件基于 JavaEE 平台创建、发送、接收和读取消息。
>
> AMQP（即 Advanced Message Queuing Protocol），一个提供统一消息服务的应用层标准 **高级消息队列协议**（二进制应用层协议），是应用层协议的一个开放标准，为面向消息的中间件设计，兼容 JMS。基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件同产品，不同的开发语言等条件的限制。
>
> **RabbitMQ 就是基于 AMQP 协议实现的。**

|   对比方向   | JMS                                                          | AMQP                                                         |
| :----------: | :----------------------------------------------------------- | :----------------------------------------------------------- |
|     定义     | Java API                                                     | 协议                                                         |
|    跨语言    | 否                                                           | 是                                                           |
|    跨平台    | 否                                                           | 是                                                           |
| 支持消息模型 | 提供两种消息模型：<br />①Peer-2-Peer;<br />②Pub/sub          | 提供了五种消息模型：<br />①direct exchange；<br />②fanout exchange；<br />③topic change；<br />④headers exchange；<br />⑤system exchange。<br />本质来讲，后四种和 JMS 的 pub/sub 模型没有太大差别，仅是在路由机制上做了更详细的划分； |
| 支持消息类型 | 支持多种消息类型<br />StreamMessage<br />MapMessage<br />StringMessage<br />ObjectMessage<br />BytesMessage | byte[]（二进制）                                             |



##### RPC 和消息队列的区别

> RPC 和消息队列都是分布式微服务系统中重要的组件之一，下面我们来简单对比一下两者：
>
> - **从用途来看**：
>
>   RPC 主要用来<u>解决两个服务的远程通信</u>问题，不需要了解底层网络的通信机制。通过 RPC 可以帮助我们调用远程计算机上某个服务的方法，这个过程就像调用本地方法一样简单。
>
>   MQ主要用来降低系统耦合性、实现任务异步、有效地进行流量削峰。
>
> - **从通信方式来看**：RPC 是<u>双向直接</u>网络通讯，消息队列是<u>单向</u>引入中间载体的网络通讯。
>
> - **从架构上来看**：消息队列需要把消息存储起来，RPC 则没有这个要求，因为前面也说了 RPC 是双向直接网络通讯。
>
> - **从请求处理的时效性来看**：通过 RPC 发出的调用一般会<u>立即被处理</u>，存放在消息队列中的消息并不一定会立即被处理。
>
> RPC 和消息队列本质上是网络通讯的两种不同的实现机制，两者的用途不同，万不可将两者混为一谈。



##### Kafka架构以及常见问题

<img src="https://oss.javaguide.cn/github/javaguide/high-performance/message-queue20210507200944439.png" alt="img" style="zoom:80%;" />

Producer、Consumer（多个Consumer组成一个Consumer Group)

Broker(中间件/代理，多个Broker组成一个Kafka集群)

Topic主题，每个主题由多个Partition（同一个主题的Partition可以分布在不同的Broker上）

Replica副本，是Partition的副本，一个Partition有多个副本，其中一个为Leader（Producer和Consumer只跟Leader交互），其他的都为follower，主动从Leader拉取数据，当Leader宕机后重新选举



**问题1**：一个Topic有多个Partition，而一条消息会随机插入到这些Partition中的任意一个，也就是说，Kafka只能保证每个Partition中消息有序。那么，如何**保证消息顺序消费**？

> 方法一：1 个 Topic 只对应一个 Partition （BUT：违背了Kafka的设计初衷）
>
> 方法二（推荐）：发送消息的时候指定 key/Partition

**问题2**：怎么**保证消息不丢失**？

①生产者消息丢失：Producer调用send()发送消息后，有可能因为网络问题失败

> 方法一：异步回调
>
> 生产者(Producer) 使用 `send` 方法发送消息实际上是**异步**的操作，不过我们可以通过 `get()`方法获取调用结果，但是这样也让它变为了**同步**操作。因此可以通过回调函数来获取消息发送情况：
>
> ```java
> ListenableFuture<SendResult<String, Object>> future = kafkaTemplate.send(topic, o);
> future.addCallback(result -> logger.info("生产者成功发送消息到topic:{} partition:{}的消息", result.getRecordMetadata().topic(), result.getRecordMetadata().partition()),
>                 ex -> logger.error("生产者发送消失败，原因：{}", ex.getMessage()));
> ```
>
> 通过Future的callback来获取发送情况，失败了重新发送
>
> 方法二（推荐）：设置重试次数
>
> 为 Producer 的`retries`（重试次数）设置一个比较合理的值，当出现网络问题之后能够自动重试消息发送
>
> 消息不会因重试导致重复，幂等性

②消费者消息丢失

> 原因：消息在被追加到 Partition的时候都会分配一个特定的偏移量（offset），Kafka 通过offset可以保证消息在分区内的顺序性。当消费者拉取到了分区的某个消息之后，消费者会自动提交了 offset，表示这个offset对应的消息已经拉去。然后再去消费这个消息。如果消费者在提交offset后宕机了，导致消息不能真正被消费。所以消息就丢失了。
>
> 方法：**手动关闭自动提交 offset，每次在真正消费完消息之后再自己手动提交 offset**

③Kafka消息丢失

> 原因： leader 副本所在的 broker 突然挂掉，那么就要从 follower 副本重新选出一个 leader ，但是 leader 的数据还有一些没有被 follower 副本的同步的话，就会造成消息丢失
>
> 方法一：**设置 acks = all**（默认为1），表示只有所有follower全部收到消息时，生产者才会接收到来自服务器的响应
>
> 方法二：**replication.factor >= 3**，保证副本数大于等于3
>
> 方法三：**设置 min.insync.replicas > 1**，代表消息至少要被写入到 2 个副本才算是被成功发送

**问题3**：如何保证消息不**重复消费**？

> 重复消费的原因：
>
> - 服务端侧已经消费的数据没有成功提交 offset（根本原因）
> - Kafka 侧 由于服务端处理业务时间长或者网络链接等等原因让 Kafka 认为服务**假死**，触发了**分区 rebalance**
>
> 方法一：让消费逻辑**幂等**，即**即使同一条消息被消费多次，最终结果不变**。
>
> 例如Redis的set操作就是幂等的
>
> 方法二：自动提交 Offset

**问题4**：消费者消费消息失败怎么办？会不会影响后续的消息？

> Kafka 消费者在默认配置下会进行**最多 10 次 的重试**，每次重试的**时间间隔为 0**，即**立即进行重试**。如果在 10 次重试后仍然无法成功消费消息，则不再进行重试，消息将被视为消费失败
>
> 消费失败的消息会被放入**死信队列（Dead Letter Queue，简称 DLQ）** ，它是消息中间件中的一种特殊队列。它主要用于处理无法被消费者正确处理的消息。









------

## SpringBoot :white_check_mark:

##### Spring IOC（Inversion of Controll，控制反转）:star:

IOC是一种设计思想：将原本在程序中手动**创建对象**的控制权，交**由 Spring 框架**来管理

> 控制：指对象创建的权力
>
> 反转：指控制权交给了外部环境（Spring框架）

###### IOC容器

> 将对象之间的相互依赖关系交给 **IoC 容器**来管理，并由 IoC 容器完成对象的注入
>
>  IoC 容器实际上就是个 **Map**，Map 中存放的是各种对象
>
> ![IoC 图解](https://oss.javaguide.cn/java-guide-blog/frc-365faceb5697f04f31399937c059c162.png)
>
> Spring使用**XML文件**来配置Bean，Springboot使用**注解**
>
> ```xml
> <beans>
>     <bean id="transferService" class="com.acme.TransferServiceImpl"/>
> </beans>
> ```
>
> **Bean**就是IOC容器中管理的对象

###### Spring容器启动

> 分为两个阶段：**容器启动阶段** 和 **Bean实例化阶段**
>
> 容器启动阶段主要做的工作是加载和解析配置文件，将每个`<bean>`标签保存到对应的 `BeanDefination` 中，维护一个`BeanDefinationRegistry`，就是一个 `BeanDefination` 集合
>
> <img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/spring-dfb3d8c4-ba8d-4a2c-aef2-4ad425f7180c.png" alt="xml配置信息映射注册过程" style="zoom:67%;" />



##### Spring Bean

###### 创建Bean的注解：@Component vs. @Bean :star:

> 1.`@Component` 注解作用于**类**，而`@Bean`注解作用于**方法**
>
> 2.`@Component`通常是通过类路径扫描来自动侦测以及自动装配到 Spring 容器中  => ==自动==
>
> （可以使用 `@ComponentScan` 注解**定义要扫描的路径**，从中找出标识了需要装配的类自动装配到 Spring 的 bean 容器中）
>
> ```java
> @Component
> public class MyService {}
> ```
>
> `@Bean` 注解通常是我们在标有该注解的**方法中定义产生这个 bean**，它告诉了 Spring 这是某个类的实例，当我需要用它的时候还给我  => ==手动==
>
> ```java
> @Configuration
> public class AppConfig {
>     @Bean
>     public MyService myService() {
>         return new MyService();  // 手动实例化 Bean
>     }
> }
> ```
>
> 3.`@Bean` 注解比 `@Component` 注解的自定义性更强，而且很多地方我们只能通过 `@Bean` 注解来注册 bean
>
> ​	自己写的业务类（Service、Dao、Controller）等，可以使用`@Component`
>
> ​	但当我们引用**第三方库中的类**需要装配到 Spring容器时，则只能通过 `@Bean`来实现

@Component注解的子注解：

> `@Component`：通用的注解，可标注任意类为 `Spring` 组件。如果一个 Bean 不知道属于哪个层，可以使用`@Component` 注解标注
>
> `@Repository` : 对应持久层即 Dao 层，主要用于数据库相关操作
>
> `@Service` : 对应服务层，主要涉及一些复杂的逻辑，需要用到 Dao 层
>
> `@Controller` : 对应 Spring MVC 控制层，主要用于接受用户请求并调用 `Service` 层返回数据给前端页面

###### 注入Bean的注解：@Autowire 和 @Resource 和 @Inject

`@Autowire`

> `@Autowired` 属于 Spring 内置的注解，默认的注入方式为`by Type`（根据类型进行匹配），也就是说会优先根据接口类型去匹配并注入 Bean （接口的实现类）
>
> 如果一个接口有多个实现类，那么就会出问题
>
> 例如：SmsService接口有两个实现类：SmsServiceImpl1和SmsServiceImpl2
>
> ```java
> // 这种方式无法正确注入，by Name和by Type都无法匹配到bean
> @Autowire
> private SmsService smsService;
> ```
>
> 两种解决方法：
>
> ```java
> // 变量名应该和Bean对应 byName
> @Autowire
> private SmsService smsServiceImpl1;
> 
> // 如果变量名还是想叫做smsService，那么使用@Qualifier显示指定bean
> @Autowire
> @Qualifier(value = "smsServiceImpl1")
> private SmsService smsService;
> ```

`@Resource`

> `@Resource`属于 JDK 提供的注解，默认注入方式为 `by Name`
>
> 如果无法通过名称匹配到对应的 Bean 的话，注入方式会变为`byType`
>
> 如果仅指定name属性，那么注入方式为byName
>
> 如果仅指定type属性，那么注入方式为byType
>
> 不建议同时指定name和type
>
> ```java
> // 报错，byName 和 byType 都无法匹配到 bean
> @Resource
> private SmsService smsService;
> // 正确注入 SmsServiceImpl1 对象对应的 bean
> @Resource
> private SmsService smsServiceImpl1;
> // 正确注入 SmsServiceImpl1 对象对应的 bean（比较推荐这种方式）
> @Resource(name = "smsServiceImpl1")
> private SmsService smsService;
> ```

**总结**：

> 1.`@Autowired` 是 Spring 提供的注解，`@Resource` 是 JDK 提供的注解
>
> 2.`Autowired` 默认的注入方式为`byType`（根据类型进行匹配）
>
>   `@Resource`默认注入方式为 `byName`（根据名称进行匹配）
>
> 3.当**一个接口存在多个实现类**的情况下，`@Autowired` 和`@Resource`都需要**通过名称**才能正确匹配到对应的 Bean
>
> ​	`@Autowired` 可以通过 `@Qualifier` 注解来显式指定名称
>
> ​	`@Resource` 可以通过 `name` 属性来显式指定名称
>
> 4.`@Autowired` 支持在**构造函数**、方法（Setter）、字段和**参数**上使用
>
>   `@Resource` 主要用于字段和方法上的注入，不支持在构造函数或参数上使用

###### 为什么IDEA不推荐使用@Autowired

使用@Autowired注入成员变量时，提示“Field injection is not recommended” <字段注入不推荐>

> 字段注入的方式存在的问题：
>
> - 不能像构造方法那样**使用 final 注入不可变对象**（@Autowired不能加在final成员变量上）
>
> - 隐藏了依赖关系，调用者可以看到构造方法注入或者 setter 注入，但无法看到私有字段的注入
>
>   导致了调用者不清楚这个类的相关依赖
>   
> - 测试的时候无法通过构造函数或 setter 传入 mock 对象，只能使用反射去修改 private 字段
>
> - 字段注入在非 Spring 环境中完全失效
>
> 在 Spring 4.3 及以后，如果类**只有一个构造方法**，Spring **可以自动注入依赖**，因此**可以省略 `@Autowired`**

###### 推荐注入方式：构造函数注入

> **Spring 官方推荐构造函数注入**，这种注入方式的优势如下：
>
> 1. 依赖完整性：确保所有必需依赖在对象创建时就被注入，避免了空指针异常的风险。
> 2. 不可变性：有助于创建不可变对象，提高了线程安全性。
> 3. 初始化保证：组件在使用前已完全初始化，减少了潜在的错误。
> 4. **测试**便利性：在单元测试中，可以直接通过构造函数传入模拟的依赖项，而不必依赖 Spring 容器进行注入



###### Bean的作用域

> :star:**singleton** : IoC 容器中只有唯一的 bean 实例。Spring 中的 bean **默认**都是单例的，是对单例设计模式的应用 
>
> :star:**prototype** : 每次获取都会创建一个新的 bean 实例。也就是说，连续 `getBean()` 两次，得到的是不同的 Bean 实例
>
> **request** （仅 Web 应用可用）: 每一次 HTTP 请求都会产生一个新的 bean（请求 bean），该 bean 仅在当前 HTTP request 内有效
>
> **session** （仅 Web 应用可用） : 每一次来自新 session 的 HTTP 请求都会产生一个新的 bean（会话 bean），该 bean 仅在当前 HTTP session 内有效
>
> **application/global-session** （仅 Web 应用可用）：每个 Web 应用在**启动时创建一个 Bean**（应用 Bean），该 bean 仅在当前应用启动时间内有效
>
> **websocket** （仅 Web 应用可用）：每一次 WebSocket 会话产生一个新的 bean

两种方式来指定Bean的作用域：

> ①xml
>
> ```xml
> <bean id="..." class="..." scope="singleton"></bean>
> ```
>
> ②Scope注解
>
> ```java
> @Bean
> @Scope(value = ConfigurableBeanFactory.SCOPE_PROTOTYPE)
> public Person personPrototype() {
>  	return new Person();
> }
> 
> @Bean
> @Scope("singleton")
> public Person personSingleton() {
>     	return new Person();
> }
> ```



###### Bean是否线程安全？

> 重点关注`singleton`作用域下的Bean，因为IOC容器中只有唯一的bean实例
>
> 如果Bean**有状态**，即含有**可变的成员变量**如列表等，那么该Bean就存在安全问题
>
> 解决：
>
> ①避免可变的成员变量
>
> ②使用`ThreadLocal`：确保每个线程访问自己的可变成员变量
>
> ③使用`synchronized`或`ReentrantLook`同步控制
>



###### Bean的生命周期

**实例化** => 属性赋值 => 初始化 => 销毁

`AbstractAutowireCapableBeanFactory` 的 `doCreateBean()` 方法

```java
protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final @Nullable Object[] args)
    throws BeanCreationException {

    // 1. 创建 Bean 的实例
    BeanWrapper instanceWrapper = null;
    if (instanceWrapper == null) {
        instanceWrapper = createBeanInstance(beanName, mbd, args);
    }

    Object exposedObject = bean;
    try {
        // 2. Bean 属性赋值/填充
        populateBean(beanName, mbd, instanceWrapper);
        // 3. Bean 初始化
        exposedObject = initializeBean(beanName, exposedObject, mbd);
    }

    // 4. 销毁 Bean-注册回调接口
    try {
        registerDisposableBeanIfNecessary(beanName, bean, mbd);
    }

    return exposedObject;
}
```



![img](https://oss.javaguide.cn/github/javaguide/system-design/framework/spring/spring-bean-lifestyle.png)

**①实例化**：

> Spring 首先使用构造方法或者工厂方法创建一个 Bean 的实例。此时的 Bean 只是一个**空的 Java 对象**，还未设置任何属性
>
> 调用的是`doCreateBean()`方法其中的 `createBeanInstance()` 来创建空的Bean实例

**②属性赋值**：

> Spring 将配置文件中的属性值或依赖的 Bean 注入到该 Bean 中，例如@Autowired注入的对象、@Value注入的值、@Resource注入的对象等，这个过程称为**依赖注入**
>
> 调用的是`populateBean()`方法

**③初始化**：`initializeBean()`

> 3.1 检查`Aware`相关接口：
>
> * 如果 Bean 实现了 `BeanNameAware` 接口，调用 `setBeanName()`方法，传入 Bean 的名字
> * 如果 Bean 实现了 `BeanClassLoaderAware` 接口，调用 `setBeanClassLoader()`方法，传入 `ClassLoader`对象的实例
> * 如果 Bean 实现了 `BeanFactoryAware` 接口，调用 `setBeanFactory()`方法，传入 `BeanFactory`对象的实例
> * ...，如果实现了其他 `*.Aware`接口，就调用相应的方法
>
> 3.2 调用`BeanPostProcessor` 前置处理（**BeforeInitialization**）
>
> * 如果有和加载这个 Bean 的 Spring 容器相关的 `BeanPostProcessor` 对象，执行其前置处理方法`postProcessBeforeInitialization()` 
>
> 3.3 进行初始化 （**Initializing**）
>
> * 如果 Bean 实现了`InitializingBean`接口，执行`afterPropertiesSet()`方法进行初始化
> * 如果 Bean 在配置文件中的定义包含 `init-method` 属性，执行指定的方法。
>
> 3.4 调用`BeanPostProcessor` 后置处理（**AfterInitialization**）
>
> * 如果有和加载这个 Bean 的 Spring 容器相关的 `BeanPostProcessor` 对象，执行其后置处理方法`postProcessAfterInitialization()` 

**④销毁**：注册Bean销毁的方法，将来需要销毁时调用这个方法

> 可以通过实现`DisposableBean` 接口的 `destroy()` 方法
>
> 也可以在配置文件中的定义包含 `destroy-method` 属性，执行指定的 Bean 销毁方法
>
> 可以通过`@PreDestroy` 注解标记 Bean 销毁之前执行的方法

总结：

```
AbstractAutowireCapableBeanFactory::doCreateBean()
	step1: createBeanInstance() # 实例化
	step2: populateBean()		# 属性赋值
		AutowiredAnnotationBeanPostProcessor::postProcessProperties()  # 依赖注入
	step3: initializeBean()		# 初始化
		3.1 执行实现的Aware接口对应的方法
		3.2	BeanPostProcessor::postProcessBeforeInitialization()  # 初始化前
		3.3	InitializingBean::afterPropertiesSet() || init-method  # 初始化
		3.4 BeanPostProcessor::postProcessAfterInitialization()  # 初始化后
    step4: registerDisposableBeanIfNecessary() # 注册销毁回调方法
		DisposableBean::destory() || destory-method 
```



###### Aware接口

通过实现 Aware 接口，Bean 可以获取 Spring 容器的相关信息，如 BeanFactory、ApplicationContext 等。

常见 Aware 接口有：

| 接口                    | 作用                                                         |
| ----------------------- | ------------------------------------------------------------ |
| BeanNameAware           | 获取当前 Bean 的名称。                                       |
| BeanFactoryAware        | 获取当前 Bean 所在的 BeanFactory 实例，可以直接操作容器。    |
| ApplicationContextAware | 获取当前 Bean 所在的 ApplicationContext 实例。               |
| EnvironmentAware        | 获取 Environment 对象，用于获取配置文件中的属性或环境变量。  |
| ServletContextAware     | 在 Web 环境下获取 ServletContext 实例，访问 Web 应用上下文。 |
| ResourceLoaderAware     | 获取 ResourceLoader 对象，用于加载资源文件（如类路径文件或 URL）。 |



###### BeanPostProcessor

>  Spring 为**修改 Bean** 提供的强大扩展点
>
> ```java
> public interface BeanPostProcessor {
> 
>     // 初始化前置处理
>     default Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException {
>         return bean;
>     }
> 
>     // 初始化后置处理
>     default Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException {
>         return bean;
>     }
> 
> }
> ```
>
> ①postProcess<u>Before</u>Initialization：在**Bean实例化、属性注入**后，**InitializingBean.afterPropertiesSet()初始化**前执行
>
> ​	作用：设置/修改 Bean 的默认值、初始化一些外部资源或上下文信息
>
> ②postProcess<u>After</u>Initialization：**InitializingBean.afterPropertiesSet()初始化**、**init-method**之后执行
>
> ​	作用：返回 **代理对象（AOP）**、包装 Bean，增强功能、加日志、加权限控制、加缓存逻辑



###### InitializingBean::afterPropertiesSet() & init-method

>  Spring 为 **Bean 初始化**提供的扩展点
>
> ```java
> public interface InitializingBean {
>  	// 初始化逻辑
> 	void afterPropertiesSet() throws Exception;
> }
> ```
>
> or XML
>
> ```xml
> <beans>
> 	<bean id="demo" class="com.chaycao.Demo" init-method="init()"/>
> </beans>
> ```
>
> 



###### @Autowired注解的原理

> Bean的生命周期中，创建Bean时调用`doCreateBean()`方法实例化，再调用`populateBean()`进行属性赋值
>
> @Autowired的作用就是在**属性赋值**这一阶段体现的，依赖`AutowiredAnnotationBeanPostProcessor`来实现
>
> 在属性赋值中，一共调用了两次后置处理器`BeanPostProcessor`的方法：
>
> ​	第一次用来判断该Bean实例是否需要初始化，不需要直接return；需要的话继续向下执行初始化
>
> ​	第二次就会调用到 `AutowiredAnnotationBeanPostProcessor` 的 `postProcessProperties()`方法，在该方法中就会进行@Autowired 注解的解析，然后实现自动装配
>
> 在`postProcessProperties()`方法中：
>
> ​	①会先调用 `findAutowiringMetadata()`方法解析出 bean 中带有@Autowired 注解、@Inject 和@Value 注解的属性和方法
>
> ​	②再调用 `metadata.inject()` 方法，进行属性填充
>
> ```java
> public PropertyValues postProcessProperties(PropertyValues pvs, Object bean, String beanName) {
>        //查找@Autowired注解、@Inject和@Value注解的属性和方法
>        InjectionMetadata metadata = this.findAutowiringMetadata(beanName, bean.getClass(), pvs);
> 
>        try {
>            //属性填充
>            metadata.inject(bean, beanName, pvs);
>            return pvs;
>        } catch (BeanCreationException var6) {
>            throw var6;
>        } catch (Throwable var7) {
>            throw new BeanCreationException(beanName, "Injection of autowired dependencies failed", var7);
>        }
> }
> ```
>
> 



###### 一个项目有几千个Bean导致启动特别慢怎么办？

【淘天一面】

> | 优化策略                          | 适用场景                    | 预期效果                   |
> | --------------------------------- | --------------------------- | -------------------------- |
> | **懒加载 (`@Lazy`)**              | 非核心 Bean                 | 降低启动时创建 Bean 的数量 |
> | **`@ConditionalOnProperty`**      | 配置驱动的 Bean             | 只加载必要的 Bean          |
> | **精确 `@ComponentScan`**         | 避免扫描无关包              | 减少不必要的 Bean 扫描     |
> | **关闭不必要的自动配置**          | 依赖较少的项目              | 避免加载无用的 Spring 配置 |
> | **并行加载 Bean**                 | 依赖较多的 Bean             | 充分利用多核 CPU           |
> | **异步初始化 Bean**               | 需要数据库或网络连接的 Bean | 加快主线程启动速度         |
> | **使用 `spring-context-indexer`** | 组件扫描较慢                | 提高扫描速度               |
> | **Spring Boot Actuator 监控**     | 识别启动瓶颈                | 发现最耗时的 Bean          |
> | **GraalVM 原生编译**              | 需要极快启动的服务          | 启动时间缩短至毫秒级       |

\1. 使用懒加载减少启动时初始化的Bean数量。

\2. 优化组件扫描范围，减少不必要的类被扫描。

\3. 使用条件化配置减少Bean定义。

\4. 优化初始化方法，避免耗时操作。

\5. 解决循环依赖和复杂的依赖关系，加快依赖注入。

\6. 减少BeanPostProcessor的数量或优化其逻辑。

\7. 使用上下文索引加速组件扫描。





##### Spring AOP（面向切面编程）:star:

AOP能够将那些**与业务无关**，却为业务模块所**共同调用的逻辑或责任**封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性（例如事务处理、日志管理、权限控制）

###### Spring AOP 具体实现

> 在 Spring 容器初始化 Bean 的过程中，Spring AOP 会检查 Bean 是否需要应用切面。
>
> 如果需要，Spring 会为该 Bean 创建一个代理对象，并在代理对象中织入切面逻辑。
>
> 这一过程发生在 Spring 容器的**后处理器（BeanPostProcessor）阶段**

###### JDK代理 vs. CGLIB代理

> AOP是通过**动态代理**来实现的，两种类型：
>
> `JDK Proxy`：基于**接口**，目标类实现了某个接口
>
> ​	使用 JDK 动态代理时，Spring AOP 会创建一个代理对象，该**代理对象实现了目标对象所实现的接口**，并在方法调用前后插入横切逻辑
>
> ​	优点：只需依赖 JDK 自带的 `java.lang.reflect.Proxy` 类，不需要额外的库
>
> ​	缺点：只能代理接口，不能代理类本身
>
> `CGLIB Proxy`：基于**子类**，目标类继承了某个类
>
> ​	使用 CGLIB 动态代理时，Spring AOP 会**生成目标类的子类**，并在方法调用前后插入横切逻辑
>
> ​	优点：可以代理没有实现接口的类，灵活性更高
>
> ​	缺点：需要依赖 CGLIB 库，创建代理对象的开销相对较大
>
> ![SpringAOPProcess](https://oss.javaguide.cn/github/javaguide/system-design/framework/spring/230ae587a322d6e4d09510161987d346.jpeg)
>

###### Spring AOP vs. AspectJ

> Spring AOP继承了Java自带的`AspectJ`，两者的主要区别是：
>
> * Spring AOP是**运行时**增强，基于**代理**
> * AspectJ是**编译时**增强，基于**字节码**
>
> Spring AOP 属于`运行时增强`，主要具有如下特点：
>
> 1. 基于动态代理来实现，默认如果使用接口的，用 JDK 提供的动态代理实现，如果是方法则使用 CGLIB 实现
> 2. Spring AOP 需要依赖 IoC 容器来管理，并且只能作用于 Spring 容器，使用纯 Java 代码实现
> 3. 在性能上，由于 Spring AOP 是基于**动态代理**来实现的，在容器启动时需要生成代理实例，在方法调用上也会增加栈的深度，使得 Spring AOP 的性能不如 AspectJ 的那么好
> 4. Spring AOP 致力于解决企业级开发中最普遍的 AOP(方法织入)
> 5. 只能增强非private、非final方法
>
> 
>
> AspectJ 是一个易用的功能强大的 AOP 框架，属于 `编译时增强`， 可以单独使用，也可以整合到其它框架中，是 AOP 编程的完全解决方案。AspectJ 需要用到**单独的编译器 ajc**。
>
> AspectJ 属于**静态织入**，通过修改代码来实现，在实际运行之前就完成了织入，所以说它生成的类是没有额外运行时开销的，一般有如下几个织入的时机：
>
> 1. 编译期织入（Compile-time weaving）：如类 A 使用 AspectJ 添加了一个属性，类 B 引用了它，这个场景就需要编译期的时候就进行织入，否则没法编译类 B。
> 2. 编译后织入（Post-compile weaving）：也就是已经生成了 .class 文件，或已经打成 jar 包了，这种情况我们需要增强处理的话，就要用到编译后织入。
> 3. 类加载后织入（Load-time weaving）：指的是在加载类的时候进行织入，要实现这个时期的织入，有几种常见的方法
>
> 如果需要增强 `final` 方法、静态方法、字段访问、构造器调用等，或者需要在非 Spring 管理的对象上应用增强逻辑，AspectJ 是唯一的选择



###### AOP相关术语

| 术语                  |                             含义                             |                             解释                             |
| :-------------------- | :----------------------------------------------------------: | :----------------------------------------------------------: |
| **目标**(Target)      |                         被通知的对象                         |                     被AOP增强的原始对象                      |
| **代理**(Proxy)       |             向目标对象应用通知之后创建的代理对象             |                      AOP动态创建的代理                       |
| **连接点**(JoinPoint) |       目标对象的所属类中，定义的**所有方法**均为连接点       |                    要增强的方法就是连接点                    |
| **切入点**(Pointcut)  | 被切面拦截 / 增强的连接点<br />（切入点一定是连接点，连接点不一定是切入点） | 一般用execution(public/private 包路径.类名.方法名)指定切入点 |
| **通知**(Advice)      | 增强的逻辑 / 代码，也即拦截到目标对象的连接点之后要做的事情  | 在某个切入点要执行的具体的动作，一般使用@Before、@After等注解加在这个动作上 |
| **切面**(Aspect)      |                切入点(Pointcut)+通知(Advice)                 |                                                              |
| **织入**(Weaving)     |       将通知应用到目标对象，进而生成代理对象的过程动作       |                         发生在运行期                         |

通知类型：

<img src="https://oss.javaguide.cn/github/javaguide/system-design/framework/spring/aspectj-advice-types.jpg" alt="img" style="zoom: 33%;" />

多个@Aspect可以使用`@Order(n)`来指定切面的顺序

也可以让Aspect实现`Ordered`接口并重写`getOrder()`方法，返回顺序



###### 织入的时期

> ①编译期织入：切面在目标类编译时被织入  
>
> ②类加载期织入：切面在目标类加载到 JVM 时被织入。需要特殊的类加载器，它可以在目标类被引入应用之前增强该目标类的字节码
>
> ③运行期织入：切面在应用运行的某个时刻被织入。一般情况下，在织入切面时，AOP 容器会为目标对象动态地创建一个代理对象

其中：SpringAOP就是运行期织入、AspectJ就是编译期织入或者类加载器织入





###### 代码示例

**目标类**：该类下所有的方法都可以作为**连接点**

```java
@Service
public class UserService {

    public String getUserById(int id) {
        System.out.println("Executing getUserById() with id = " + id);
        return "User-" + id;
    }
}
```

**切面**：记录日志，在UserService的所有方法执行前后记录日志

多个切面的情况下，可以通过 `@Order` 指定先后顺序，数字越小，优先级越高

```java
package com.example.aspect;

import org.aspectj.lang.JoinPoint;
import org.aspectj.lang.annotation.*;
import org.springframework.stereotype.Component;

@Aspect  // 1. 声明这是一个切面
@Component  // 2. 让 Spring 识别为组件
public class LoggingAspect {

    // 3. 定义切点，匹配 UserService 的所有方法
    // 理解一下：切点一定是连接点，连接点不一定是切点
    // 不是所有的连接点我们都要增强，切点就是我们要增强的连接点
    // 比如这里要增强的USerServie下的所有连接点 当然我们也可以指定只增强某个方法
    @Pointcut("execution(* com.example.service.UserService.*(..))")
    public void userServiceMethods() {}

    // 4. 在方法执行前执行（前置通知）
    @Before("userServiceMethods()")
    public void logBeforeMethod(JoinPoint joinPoint) {
        System.out.println("[AOP] Before: " + joinPoint.getSignature());
    }

    // 5. 在方法执行后执行（后置通知）
    @After("userServiceMethods()")
    public void logAfterMethod(JoinPoint joinPoint) {
        System.out.println("[AOP] After: " + joinPoint.getSignature());
    }
}

```





##### Spring MVC

###### 工作流程

`@Controller`

> ![img](https://oss.javaguide.cn/github/javaguide/system-design/framework/spring/de6d2b213f112297298f3e223bf08f28.png)
>
> 1. 客户端（浏览器）发送请求， `DispatcherServlet`拦截请求
> 2. `DispatcherServlet` 根据请求信息调用 `HandlerMapping` `HandlerMapping` 根据 URL 去匹配查找能处理的 `Handler`（也就是我们平常说的 `Controller` 控制器） ，并会将请求涉及到的拦截器和 `Handler` 一起封装
> 3. `DispatcherServlet` 调用 `HandlerAdapter`适配器执行 `Handler` 
> 4. `Handler` 完成对用户请求的处理后，会返回一个 `ModelAndView` 对象给`DispatcherServlet`，`ModelAndView` 顾名思义，包含了数据模型以及相应的视图的信息。`Model` 是返回的数据对象，`View` 是个逻辑上的 `View`
> 5. `ViewResolver` 会根据逻辑 `View` 查找实际的 `View`
> 6. `DispaterServlet` 把返回的 `Model` 传给 `View`（视图渲染）
> 7. 把 `View` 返回给请求者（浏览器）
>

`@RestController`：`@Controller` + `@ResponseBody`

> 返回的结果不需要使用ViewResolver去渲染了，直接返回json格式的model



##### Spring循环依赖如何解决 :star:

```java
@Component
public class A {
    @Autowired
    private B b;
}
@Component
public class B {
    @Autowired
    private A a;
}
```

###### ① Spring框架使用**三级缓存**来解决，三级缓存就是**三个Map**

```java
// 一级缓存
/** Cache of singleton objects: bean name to bean instance. */
private final Map<String, Object> singletonObjects = new ConcurrentHashMap<>(256);

// 二级缓存
/** Cache of early singleton objects: bean name to bean instance. */
private final Map<String, Object> earlySingletonObjects = new HashMap<>(16);

// 三级缓存
/** Cache of singleton factories: bean name to ObjectFactory. */
private final Map<String, ObjectFactory<?>> singletonFactories = new HashMap<>(16);
```

> ①**一级缓存（singletonObjects）**：存放**最终形态**的单例 Bean（已经实例化、属性填充、初始化）
>
> :triangular_flag_on_post: ​一般情况我们获取 Bean 都是从这里获取的，但是并不是所有的 Bean 都在单例池里面，例如原型 Bean 就不在里面
>
> ②**二级缓存（earlySingletonObjects）**：存放过渡 Bean（**半成品**，尚未属性填充），也就是三级缓存中`ObjectFactory`产生的对象
>
> :triangular_flag_on_post: 二级缓存与三级缓存配合使用的，可以防止 AOP 的情况下，每次调用`ObjectFactory.getObject()`都是会产生新的代理对象的问题
>
> ③**三级缓存（singletonFactories）**：存放`ObjectFactory`，用于提前暴露 Bean
>
> :triangular_flag_on_post:`ObjectFactory`的`getObject()`方法（最终调用的是`getEarlyBeanReference()`方法）可以获取提前暴露的 **Bean 对象**或者**代理对象**（如果 Bean 被 AOP 切面代理）

创建Bean的过程：

> ① 先去 **一级缓存 ** 中获取，存在就返回（已经完全实例化的Bean）
>
> ② 如果不存在或者对象正在创建中，于是去 **二级缓存 **中获取 （半成品Bean)，并将其从二级缓存中移除，加入一级缓存
>
> ③ 如果还没有获取到，就去 **三级缓存** 中获取，通过调用 `ObjectFacotry` 的 `getObject()` 就可以获取该对象，获取成功之后，**从三级缓存移除**，并将该对象**加入到二级缓存**中
>
> ④ 创建后，Spring会在该Bean实例的属性**还没初始化完成前**，会提前暴露出去，即调用`addSingletonFactory()`方法向三级缓存中添加一个 `ObjectFactory` 对象
>
> ```java
> // AbstractAutowireCapableBeanFactory # doCreateBean #
> public abstract class AbstractAutowireCapableBeanFactory {
> 	protected Object doCreateBean(...) {
>         //...
> 
>         // 支撑循环依赖：将 ()->getEarlyBeanReference 作为一个 ObjectFactory 对象的 getObject() 方法加入到三级缓存中
> 		addSingletonFactory(beanName, () -> getEarlyBeanReference(beanName, mbd, bean));
>     }
> }
> ```
>
> 

解决循环依赖的具体流程：

> 当 Spring 创建 A **之后**，还没初始化A的属性之前，就将A实例对象**提前暴露出去**
>
> 接着在初始化A的属性时，发现 A 依赖了 B ，又去创建 B，B 依赖了 A ，又去创建 A
>
> 在 B 创建 A 的时候，那么此时 A 就发生了循环依赖，由于 A 此时还没有初始化完成，因此在 **一二级缓存** 中肯定没有 A
>
> 那么此时就去**三级缓存**中获取A对应的`ObjectFactory`，调用其 `getObject()` 方法获取 A 的 **提前暴露的对象** 
>
> 并将这个 `ObjectFactory` 从三级缓存中移除，将A的前期暴露对象放入到二级缓存中，那么 B 就将这个前期暴露对象注入到依赖，来支持循环依赖

不足之处：

> 增加内存开销
>
> 只能用来解决单例Bean
>
> 非单例的 Bean 和`@Async`注解的 Bean 无法支持循环依赖

为什么要三级缓存？⼆级不⾏吗？

> 不行，主要是为了 **⽣成代理对象**。如果是没有代理的情况下，使用二级缓存解决循环依赖也是 OK 的。但是如果存在代理，三级没有问题，二级就不行了
>
> 因为三级缓存中放的是⽣成具体对象的匿名内部类，获取 Object 的时候，它可以⽣成代理对象，也可以返回普通对象。使⽤三级缓存主要是为了保证不管什么时候使⽤的都是⼀个对象
>
> 假设只有⼆级缓存的情况，往⼆级缓存中放的显示⼀个普通的 Bean 对象，Bean 初始化过程中，通过 BeanPostProcessor 去⽣成代理对象之后，覆盖掉⼆级缓存中的普通 Bean 对象，那么可能就导致取到的 Bean 对象不一致了

###### ②**`@Lazy`**懒加载注解解决循环依赖

`@Lazy` 作用于类上、方法上、构造器上、方法参数上

也可以通过`spring.main.lazy-initialization=true`开启**全局懒加载**

`@Lazy`注解使用**代理对象**来解决循环依赖

> 如果在A的构造器上加上`@Lazy`注解，创建过程如下：
>
> 1.首先 Spring 会去创建 A 的 Bean，创建时需要注入 B 的属性
>
> 2.由于在 A 上标注了 `@Lazy` 注解，因此 Spring 会去**创建一个 B 的代理对象**，将这个代理对象注入到 A 中的 B 属性
>
> 3.之后开始执行 B 的实例化、初始化，在注入 B 中的 A 属性时，此时 A 已经创建完毕了，就可以将 A 给注入进去
>
> 4.等到 A 的实例去调用 B 成员变量的方法时，代理对象才会触发 B 的真正初始化



##### Spring 事务

###### 五种隔离级别

其中：DEFAULT是指使用后端数据库的默认隔离级别

```java
public enum Isolation {
    // 使用数据库默认隔离级别
	DEFAULT(TransactionDefinition.ISOLATION_DEFAULT),
    // 读未提交
    READ_UNCOMMITTED(TransactionDefinition.ISOLATION_READ_UNCOMMITTED),
    // 读已提交
    READ_COMMITTED(TransactionDefinition.ISOLATION_READ_COMMITTED),
    // 可重复读
    REPEATABLE_READ(TransactionDefinition.ISOLATION_REPEATABLE_READ),
    // 可串行化
    SERIALIZABLE(TransactionDefinition.ISOLATION_SERIALIZABLE);
}
```

###### 两种事务类型：声明式事务管理 和 编程式事务管理

> **编程式事务**可以使用 `TransactionTemplate` 和 `PlatformTransactionManager`来实现，需要显式执行事务
>
> ```java
> public class AccountService {
>     private TransactionTemplate transactionTemplate;
> 
>     public void setTransactionTemplate(TransactionTemplate transactionTemplate) {
>         this.transactionTemplate = transactionTemplate;
>     }
> 
>     public void transfer(final String out, final String in, final Double money) {
>         // 使用execute方法来执行事务
>         transactionTemplate.execute(new TransactionCallbackWithoutResult() {
>             @Override
>             protected void doInTransactionWithoutResult(TransactionStatus status) {
>                 // 转出
>                 accountDao.outMoney(out, money);
>                 // 转入
>                 accountDao.inMoney(in, money);
>             }
>         });
>     }
> }
> ```
>
> **声明式事务**是建立在 AOP 之上的。其本质是通过 AOP 功能，对方法前后进行拦截，将事务处理的功能编织到拦截的方法中，也就是在目标方法开始之前启动一个事务，在目标方法执行完之后根据执行情况提交或者回滚事务
>
> 注解：`@Transactional`
>
> 注意：只有publc方法才可以使用@Transactionial，因为是通过动态代理来实现的，代理类需要调用public方法

###### `@Transactional`失效的情况

> ①应用在非 public 修饰的方法上
>
> ②`propagation`属性设置错误：**事务的传播机制**如下：
>
> * require（默认）：如果当前没有事务，就创建，有就加入这个事务
> * require_new：始终新建事务，并把已有的事务挂起
> * support：如果当前有事务就加入，没有就以**非事务**的形式运行 【可能原因之一】
> * not_support：始终以非事务的形式运行，如果存在事务就抛出异常
> * mandatory：如果当前有事务就加入，没有就报错
>
> ③`rollbackFor`属性设置错误
>
> ​	`@Transactional`注解默认为rollbackFor=`RuntimeException`或者`Error`，不会回滚`Checked Exception`，需要手动指定，@Transactional(rollbackFor = "Execption.class")
>
> ④**同一个类中进行方法调用**，导致@Transactional 失效
>
> ​	在类内部调用了这个加了注解的方法，事务是不会生效的
>
> ​	只能通过类外部来调用这个方法才可以
>
> ​	因为是通过动态代理来实现的

###### 事务传播机制失效

> 事务传播机制是使用 ThreadLocal 实现的，所以，如果调用的方法是在新线程中，事务传播会失效。





##### 为什么使用SpringBoot？好处？

> Spring Boot 内嵌了 Tomcat、Jetty、Undertow 等容器，**直接运行 jar 包**就可以启动项目。
>
> Spring Boot 内置了 Starter 和**自动装配**，避免繁琐的手动配置。
>
> 例如，如果项目中添加了 spring-boot-starter-web，Spring Boot 会自动配置 Tomcat 和 Spring MVC。
>
> Spring Boot 内置了 Actuator 和 DevTools，便于调试和监控





##### SpringBoot自动装配 （Auto Configuration）:star:

> 自动配置引用的第三方包中的Bean，不需要开发人员再去写Bean相关的配置
>
> **步骤**：
>
> 1、引入Starter组件
>
> 2、SpringBoot基于约定去Starter组件的路径下（META-INF/spring.factories）去找配置类
>
> 3、SpringBoot使用ImportSelector去导入这些配置类，并根据@Conditional动态加载配置类里面的Bean到容器



###### 核心注解：@SpringbootApplication

> 可以把 `@SpringBootApplication`看作是 `@Configuration`、`@EnableAutoConfiguration`、`@ComponentScan` 注解的集合。这三个注解的作用分别是：
>
> - `@EnableAutoConfiguration`：启用 SpringBoot 的自动配置机制
> - `@Configuration`：允许在上下文中注册额外的 bean 或导入其他配置类
> - `@ComponentScan`：扫描被`@Component` (`@Service`,`@Controller`)注解的 bean，注解默认会扫描启动类所在的包下所有的类 ，可以自定义不扫描某些 bean



###### @EnableAutoConfiguration

自动装配核心功能的实现实际是通过 `AutoConfigurationImportSelector`类

```java
@Target({ElementType.TYPE})
@Retention(RetentionPolicy.RUNTIME)
@Documented
@Inherited
@AutoConfigurationPackage //作用：将main包下的所有组件注册到容器中
@Import({AutoConfigurationImportSelector.class}) //加载自动装配类 xxxAutoconfiguration
public @interface EnableAutoConfiguration {
    String ENABLED_OVERRIDE_PROPERTY = "spring.boot.enableautoconfiguration";

    Class<?>[] exclude() default {};

    String[] excludeName() default {};
}
```



> 该注解实现了 `ImportSelector`接口，并实现了这个接口中的 `selectImports`方法，该方法主要用于**获取所有符合条件的类的全限定类名，这些类需要被加载到 IoC 容器中**
>
> ```java
> private static final AutoConfigurationEntry EMPTY_ENTRY = new AutoConfigurationEntry();
> 
> AutoConfigurationEntry getAutoConfigurationEntry(AutoConfigurationMetadata autoConfigurationMetadata, AnnotationMetadata annotationMetadata) {
>     //<1>.
>     if (!this.isEnabled(annotationMetadata)) {
>         return EMPTY_ENTRY;
>     } else {
>         //<2>.
>         AnnotationAttributes attributes = this.getAttributes(annotationMetadata);
>         //<3>.
>         List<String> configurations = this.getCandidateConfigurations(annotationMetadata, attributes);
>         //<4>.
>         configurations = this.removeDuplicates(configurations);
>         Set<String> exclusions = this.getExclusions(annotationMetadata, attributes);
>         this.checkExcludedClasses(configurations, exclusions);
>         configurations.removeAll(exclusions);
>         configurations = this.filter(configurations, autoConfigurationMetadata);
>         this.fireAutoConfigurationImportEvents(configurations, exclusions);
>         return new AutoConfigurationImportSelector.AutoConfigurationEntry(configurations, exclusions);
>     }
> }
> ```
>
> 第一步：判断自动装配开关是否打开，默认`spring.boot.enableautoconfiguration=true`
>
> 第二步：用于获取`EnableAutoConfiguration`注解中的 `exclude` 和 `excludeName`
>
> 第三步：获取需要自动装配的所有配置类，读取`META-INF/spring.factories`
>
> 第四步：对所有配置类进行筛选，`@Conditional` 中的所有条件都满足，该类才会生效



##### SpringBoot常用注解

[javaGuide](https://javaguide.cn/system-design/framework/spring/spring-common-annotations.html)

Springboot面试题总结：https://www.yuque.com/snailclimb/mf2z3k/vqe4gz#kdk2h

<img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/spring-8d0a1518-a425-4887-9735-45321095d927.png" alt="三分恶面渣逆袭：Spring常用注解" style="zoom: 67%;" />





##### Spring Task

###### 定时任务@Schedule和@EnableSchedule

> ①`@Scheduled(cron = "0 15 5 * * ?")`：用于标记方法为计划任务的执行点
>
> ②`@EnableScheduling`：用于开启定时任务的支持



##### Spring Security

###### 有哪些控制请求访问权限的方法？

> - `permitAll()`：无条件允许任何形式访问，不管你登录还是没有登录。
> - `anonymous()`：允许匿名访问，也就是没有登录才可以访问。
> - `denyAll()`：无条件决绝任何形式的访问。
> - `authenticated()`：只允许已认证的用户访问。
> - `fullyAuthenticated()`：只允许已经登录或者通过 remember-me 登录的用户访问。
> - `hasRole(String)` : 只允许指定的角色访问。
> - `hasAnyRole(String)` : 指定一个或者多个角色，满足其一的用户即可访问。
> - `hasAuthority(String)`：只允许具有指定权限的用户访问
> - `hasAnyAuthority(String)`：指定一个或者多个权限，满足其一的用户即可访问。
> - `hasIpAddress(String)` : 只允许指定 ip 的用户访问
>
> 配置类：
>
> ```java
> @Configuration
> @EnableWebSecurity
> public class SecurityConfig {
> 
>     @Bean
>     public SecurityFilterChain filterChain(HttpSecurity http) throws Exception {
>         http
>             .authorizeHttpRequests((auth) -> auth
>                 .requestMatchers("/admin/**").hasRole("ADMIN")   // 只允许 ADMIN
>                 .requestMatchers("/user/**").hasAnyRole("USER", "ADMIN")  // USER 或 ADMIN
>                 .requestMatchers("/", "/login", "/register").permitAll() // 放行
>                 .anyRequest().authenticated()  // 其他请求需登录
>             )
>             .formLogin()  // 使用默认表单登录
>             .and()
>             .logout();    // 使用默认登出逻辑
> 
>         return http.build();
>     }
>     
> }
> ```
>
> 对Controller中某些请求进行拦截：`@PreAuthorize`等注解
>
> ```java
> @RestController
> @RequestMapping("/api")
> public class DemoController {
> 
>     // 只有 ADMIN 角色能访问
>     @PreAuthorize("hasRole('ADMIN')")
>     @GetMapping("/admin")
>     public String adminOnly() {
>         return "admin content";
>     }
> 
>     // 只要登录就能访问
>     @PreAuthorize("isAuthenticated()")
>     @GetMapping("/user")
>     public String userContent() {
>         return "user content";
>     }
> 
>     // 允许任意人访问
>     @PermitAll
>     @GetMapping("/public")
>     public String publicContent() {
>         return "public content";
>     }
> }
> ```
>
> 不推荐，不便于统一管理







------

## Mybatis

##### #{}和${}占位符的区别

> `#{}`是sql的参数占位符，MyBatis会先将sql进行预编译，将其中的#{}替换为?，在执行sql前使用参数按序填充，防止sql注入
>
> #{param} 对应 方法中的 param
>
> 并且MyBatis会处理参数，自动加引号
>
> 经过预编译的语句，一次解析，多次使用，效率高
>
> ```xml
> <select id="getUserById" resultType="User">
>  SELECT * FROM user WHERE id = #{id}
> </select>
> ```
>
> `${}`会直接将参数拼接到sql语句中，不会对SQL进行预编译，容易发生SQL注入
>
> ```xml
> <select id="getUserByName" resultType="User">
>  SELECT * FROM user WHERE name = '${name}'
> </select>
> ```
>
> 如果此时name参数的值为 "'admin' OR '1' = '1'"，那么最终拼接的sql为：
>
> ```sql
> SELECT * FROM user WHERE name = 'admin' OR '1'='1';
> ```
>
> 从而导致了sql注入问题
>
> 并且Mybatis不会自动加引号，需要自己手动加引号
>
> 当然也有使用场景：比如动态表名、列名就只能用${tablename}来实现



##### 常见标签

| 标签                  | 作用                            |
| --------------------- | ------------------------------- |
| `<select>`            | 查询数据                        |
| `<insert>`            | 插入数据                        |
| `<update>`            | 更新数据                        |
| `<delete>`            | 删除数据                        |
| `<if>`                | 条件判断                        |
| `<choose>`            | 类似 `switch-case`              |
| `<trim>`              | 处理 SQL 语句的多余 `AND`、`OR` |
| `<set>`               | 处理 `UPDATE` 语句的 `SET` 部分 |
| `<foreach>`           | 适用于 `IN` 语句                |
| `<sql>` + `<include>` | 复用 SQL 片段                   |
| `<resultMap>`         | 自定义字段映射                  |

###### trim标签

可以用于去掉WHERE子句中多余的AND/OR，非常方面这种if条件判断

```sql
<select id="getUserByCondition" resultType="User">
    SELECT * FROM user 
    <trim prefix="WHERE" prefixOverrides="AND |OR">
        <if test="name != null">AND name = #{name}</if>
        <if test="age != null">AND age = #{age}</if>
    </trim>
</select>
```

###### set标签

跟update和if结合使用，做到有就更新，没有就不更新

```sql
<update id="updateUser">
    UPDATE user
    <set>
        <if test="name != null">name = #{name},</if>
        <if test="age != null">age = #{age},</if>
    </set>
    WHERE id = #{id}
</update>
```



##### 怎么实现查询分页

项目中是使用Mybatis-plus

> ①在MyBatisPlusConfig配置类中配置分页拦截器
>
> ```java
> @Configuration
> public class MyBatisPlusConfig {
> 
>     @Bean
>     public PaginationInterceptor paginationInterceptor() {
>         PaginationInterceptor paginationInterceptor = new PaginationInterceptor();
>         paginationInterceptor.setDialectType("mysql"); 
>         return paginationInterceptor;
>     }
> }
> ```
>
> ②创建Page对象，定义分页的页号和页大小已经泛型
>
> ```java
> Page<User> page = new Page<>(pageNo, pageSize);
> ```
>
> ③使用selectPage接口，传入Page对象以及queryLambda
>
> ```java
> List<User> userList = userMapper.selectPage(page, UserMapper.lambdaQueryWrapper.eq(User::getAge, 18));
> ```
>
> 



##### CustomerMapper

一般使用MyBatis-plus自动生成的Mapper，都是继承BaseMapper的

这里可以自定义一个CustomerMapper来继承BaseMapper，并提供两个常用的wrapper

其他Mapper接口就都继承这个CustomerMapper即可

```java
class CustomerMapper<T> extends BaseMapper<T> {
    public static LambdaQueryWrapper<T> lambdaQueryWrapper = new lambdaQueryWrapper<>();
    public static LambdaUpdateWrapper<T> updateWrapper = new LambdaUpdateWrapper<>();
}
```



##### ORM

> ORM（Object Relational Mapping），对象关系映射，是一种为了解决关系型数据库数据与简单 Java 对象（POJO）的映射关系的技术
>
> 简单来说，ORM 是通过使用描述**对象**和**数据库**之间映射的元数据，将程序中的对象自动持久化到关系型数据库中
>
> <img src="面经.assets/mybatis-ea212850-56e0-4d12-98fb-03bb40007f44.png" alt="ORM简单示意图" style="zoom:67%;" />
>
> Mybatis是**半自动ORM映射工具**，需要手动编写 SQL 来完成
>
> Hibernate 属于全自动 ORM 映射工具



##### Mybatis二级缓存

> **一级缓存**: 基于 PerpetualCache 的 HashMap 本地缓存，其存储作用域为 SqlSession，**各个 SqlSession 之间的缓存相互隔离**，当 Session flush 或 close 之后，该 SqlSession 中的所有 Cache 就将清空，MyBatis **默认打开一级缓存**
>
> <img src="面经.assets/mybatis-54afb458-7dfc-4d48-9a90-4ad1a8739937.png" alt="Mybatis一级缓存" style="zoom: 33%;" />
>
> **二级缓存**：与一级缓存其机制相同，默认也是采用 PerpetualCache，HashMap 存储，不同之处在于其存储作用域为 Mapper，可以在多个 SqlSession 之间共享。**默认不打开二级缓存**，要开启二级缓存，使用二级缓存属性类需要实现 Serializable 序列化接口(可用来保存对象的状态)
>
> <img src="面经.assets/mybatis-8dae71da-ffd4-43f5-9ee9-258ea82d216b.png" alt="Mybatis二级缓存示意图" style="zoom:33%;" />



##### Mybatis工作原理

<img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/mybatis-61ac17ef-9eee-48c0-9a2d-545e1d554b13.png" alt="MyBatis的工作流程" style="zoom: 40%;" />

> ①创建SqlSessionFactory：
>
> ​	通过mybatis-config.xml来生成各种配置类对象（Configuration）
>
> ​	再根据Configuration来创建一个SqlSession工厂
>
> <img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/mybatis-234a4d1b-2d44-4576-9954-26f56162750e.png" alt="构建会话工厂" style="zoom: 50%;" />

> ②创建SqlSeesion：
>
> ​	通过SqlSession工厂来创建
>
> ​	主要包括四大构建：
>
> <img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/mybatis-da477d50-209e-45b3-a003-6d63e674bd99.png" alt="MyBatis会话运行四大关键组件" style="zoom: 67%;" />
>
> * Executor：它提供了相应的查询和更新方法，以及事务方法
> * StatementHandler：处理数据库会话
> * ParameterHandler：处理参数
> * ResultSetHandler：结果处理器，将查询结果包装成java对象
>
> <img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/mybatis-ebd0712a-1f62-4154-b391-2cb596634710.png" alt="会话运行的简单示意图" style="zoom: 67%;" />

> 总结整个流程如图：
>
> <img src="https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/mybatis-dc142e94-8e7f-4ec6-a1f6-1d20669292ad.png" alt="MyBatis整体工作原理图" style="zoom: 67%;" />









------

## Netty :white_check_mark:

##### Netty是什么？

> Netty 是一个 **基于 Java 的高性能、异步事件驱动的网络通信框架**，用于简化开发 **高并发、高吞吐量的网络应用**。它主要用于 **TCP、UDP、WebSocket 等协议的网络编程**，并且基于 **NIO（New I/O）**，可以轻松处理**海量连接**

###### BIO、NIO、AIO

> BIO：同步阻塞IO，每个连接都需要一个独立的线程进行处理，线程会阻塞直到IO操作完成
>
> NIO：同步非阻塞IO，基于**IO多路复用+非阻塞**，一个线程可以管理**多个连接**，数据读写**不会阻塞**，但依然是**同步**的（主动轮询数据是否准备完毕）
>
> AIO：异步IO，**基于事件回调**，不需要主动轮询是否有数据可读，准备好了会发送通知

###### 为什么不直接使用Java NIO，而是使用Netty？

> Netty它封装了 Java NIO 复杂的底层操作，并提供了强大的 API 来处理高并发网络应用，开发难度降低
>
> 其次Netty中进行了线程池优化，详见Netty线程模型（基于Reactor)



##### Netty核心组件

###### 1.ByteBuf 字节容器

> ByteBuf是Netty提供的一个字节缓冲器，就是一个字节数组，Netty传输数据时使用
>
> 为什么不直接使用Java的`ByteBuffer`?
>
> * **自动扩容**（不像 `ByteBuffer` 需要手动分配）
> * **支持读写分离**，提高性能
> * **支持零拷贝**（减少数据复制）

###### 2.Channel 网络操作抽象类

> 一个 Channel 表示一个**客户端-服务端的网络连接**，它封装了底层的 **Socket或文件 IO**
>
> 客户端：`NioSocketChannel`类
>
> 服务端：`NioServerSocketChannel`类

###### 3.EventLoop 事件循环 :star:

> 核心组件
>
> EventLoop 管理 **I/O 事件、任务执行、定时任务** 等
>
> 一个EventLoop和多个Channel连接，来处理这些Channel中的I/O事件，通常一个EventLoop为一个线程
>
> Channel注册到EventLoop上，EventLoop负责处理注册到其上的所有Channel中的IO事件（读写操作）

###### 4.EventLoopGroup 

> 一个EventLoopGroup包含了多个EventLoop（通常一个EventLoop为一个线程，保证线程安全）
>
> ![image-20250329213216476](面经.assets/image-20250329213216476.png)

> EventLoopGroup 的构造函数默认创建的线程数为` 2 * CPU核数`
>
> 也有 EventLoopGroup(int nThread) {} 构造函数来指定线程数

###### 5.ChannelHandler和ChannelPipeline

> 一个Channel中有一个ChannelPipeline，而ChannelPipeline中是一条ChannelHandler链
>
> ![image-20250329213406896](面经.assets/image-20250329213406896.png)
>
> ChannelHandler是消息的具体处理器，主要负责客户端/服务端之间的数据发送和接收
>
> 不同 ChannelHandler 可以处理不同任务（如编码、解码、业务逻辑）
>
> 分为 入站ChannelHandler 和 出站ChannelHandler 
>
> 一个ChannelHandler和其所在的ChannelPipeline的绑定关系为：ChannelHandlerContext

###### 6.Future/Promise 异步结果

> Netty 所有 I/O 操作都是 **异步的**，返回 **Future** 对象



##### Reactor线程模型

###### 1.单线程模型

> 所有的IO操作都由一个线程来完成，包括连接、读写、业务处理
>
> <img src="面经.assets/image-20250329214131790.png" alt="image-20250329214131790" style="zoom:67%;" />

###### 2.多线程模型

> 一个线程负责接受请求，多个NIO线程负责处理IO事件
>
> <img src="面经.assets/image-20250329214255431.png" alt="image-20250329214255431" style="zoom:67%;" />

###### 3.主从多线程模型

> 一组NIO线程负责接受请求，一组NIO线程负责处理IO事件
>
> <img src="面经.assets/image-20250329214407968.png" alt="image-20250329214407968" style="zoom:67%;" />



##### Netty线程模型

基于`EventLoopGroup`中的线程池



###### 1.单线程模型

> 一个线程处理客户端的连接和IO读写（accept、read、decode、process、encode、send）

###### 2.多线程模型

> 一个`Acceptor`线程负责监听客户端的连接，一组NIO线程负责IO读写
>
> <img src="面经.assets/image-20250329214914162.png" alt="image-20250329214914162" style="zoom:67%;" />

###### 3.主从线程池模型

> 创建两个线程组：`BossGroup`：负责接受请求，`WorkerGroup`：负责处理IO事件
>
> 从BossGroup线程组中选取一个NIO线程来监听客户端连接
>
> 从WorkerGroup线程组中选取一个NIO线程来处理IO
>
> <img src="面经.assets/image-20250329215120520.png" alt="image-20250329215120520" style="zoom:67%;" />





##### Netty长连接和心跳机制

> 在TCP三次握手建立连接后，如果客户端和服务器之间长时间（超过某个阈值）没有进行数据接发，就会处于idle状态
>
> 此时他们会发送一个心跳包给对方，接受后进行回应，从而保证连接不会被自动中断
>
> TCP自身就带有长连接的选线，`keep-alive`，但不是很灵活，一般会在应用层自己实现长连接
>
> Netty实现了长连接，关键类为`IdleStateHandler`





##### Netty的零拷贝

> 操作系统层面的零拷贝就是为了避免用户态和内核态之间的数据拷贝
>
> Netty层面的零拷贝是对数据操作进行优化：
>
> * 使用了**直接内存**，而不是JVM内存，避免了从JVM堆拷贝到内核
> * `CompositeByteBuf`类，可以将多个`ByteBuf`合并，较少了`Bytebuf`之间的拷贝
> * `ByteBuf`提供了`slice`操作来分割成多个共享同一内存区域的`ByteBuf`对象，避免拷贝
> * `FileRegion`类采用 `sendfile()` 零拷贝，直接将文件从**内核态**传输到目标Channel，避免用户态数据拷贝



##### Netty内存池

https://blog.csdn.net/crazymakercircle/article/details/124364027

借鉴jemalloc算法



------

## 分布式 :white_check_mark:

##### 分布式ID

> 在分库之后， 数据遍布在不同服务器上的数据库，数据库的自增主键已经没办法满足生成的主键唯一了。**我们如何为不同的数据节点生成全局唯一主键呢**？
>
> 分布式ID需要保证**全局唯一**

###### 生成分布式ID的方法

①**数据库自增ID** 或 使用`sequence_id`生成唯一ID

​	不同的数据库设置一个不同的起始id，步长设置为数据库的数量

​	缺点：每次获取ID都要访问一次数据库（性能差），支持的并发量不大、存在数据库单点问题



②**数据库==号段==模式**[主流]

​	一次性获取一个范围内的id到内存中，需要时直接从内存中获取id

​	滴滴开源的[ Tinyid ](https://github.com/didi/tinyid/wiki/tinyid原理介绍)使用了双号段缓存、增加多 db 支持

​	优点：减少了数据库的访问，减小数据库压力

首先创建一个表来存放id

```sql
CREATE TABLE `sequence_id_generator` (
  `id` int(10) NOT NULL,
  `current_max_id` bigint(20) NOT NULL COMMENT '当前最大id',
  `step` int(10) NOT NULL COMMENT '号段的长度',
  `version` int(20) NOT NULL COMMENT '版本号',
  `biz_type`    int(20) NOT NULL COMMENT '业务类型',
   PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
```

`current_max_id` 字段和`step`字段主要用于获取批量 ID，`biz_type`为业务类型，`version`主要用于乐观锁

其次，往表里插入一条数据，表示**业务类型101**由从**0到100的id**可用

```sql
INSERT INTO `sequence_id_generator` (`id`, `current_max_id`, `step`, `version`, `biz_type`)VALUES (1, 0, 100, 0, 101);
```

获取该业务的id范围

```sql
SELECT `current_max_id`, `step`,`version` FROM `sequence_id_generator` where `biz_type` = 101
```

不够用的话，更新之后重新 SELECT 即可



③通过 **Redis** 的 `incr` 命令即可实现对 id 原子顺序递增

④UUID：32个16进制的数组成

⑤Twitter的**雪花算法**：64bit的二进制数组成

| **位数**       | **字段名称** | **作用**                                         |
| -------------- | ------------ | ------------------------------------------------ |
| 1 bit          | 符号位       | 恒为 0（正数）                                   |
| 41 bit         | 时间戳       | 记录毫秒级时间，表示 ID 生成时间                 |
| 10 bit         | 机器 ID      | 记录当前生成 ID 的机器编号                       |
| 12 bit         | 序列号       | 记录**同一毫秒内**生成的 ID **序号**，支持高并发 |
| **合计 64 位** | **唯一 ID**  |                                                  |

雪花算法不是连续的



⑥雪花算法优化版：美团 的 Leaf、百度的 UidGenerator

Leaf-Segment：基于数据库号段模式来生成ID

Leaf-Snowflake：基于雪花算法来生成ID



##### 分布式锁

###### Redis分布式锁

**获取锁**：

在 Redis 中， `SETNX` 命令是可以帮助我们实现互斥。`SETNX` 即 **SET** if **N**ot e**X**ists 

只有key不存在，才可以设置，如果存在（可以理解为锁被占用），那么什么也不做

SETNX已经被弃用，取而代之的是SET中的NX参数

```
SET key value NX PX 30000
```

- `key` 是锁名
- `value` 是**锁的持有者标识**，可以使用 UUID 作为 value：用于防止被其他线程释放了锁
- `NX` 只在 key 不存在时才创建（避免覆盖锁）
- `PX 30000`：设置锁的过期时间为 30 秒（防止死锁）

**释放锁**：

使用`DEL`命令。为了防止其他进程误删了当前进程的锁，可以先将key和该线程对应的value进行比较，如果相等才能删除，不相等则不能删除，这样可以防止误删。这一过程使用`Lua`脚本来执行，保证原子性

<img src="https://oss.javaguide.cn/github/javaguide/distributed-system/distributed-lock/distributed-lock-setnx.png" alt="Redis 实现简易分布式锁" style="zoom:67%;" />

给锁设置一个过期时间，防止锁不能正常释放而导致其他线程饿死

如果锁过期时间到了，但是当前线程的任务还没执行完，应该进行**续锁**

续锁可以使用`Redisson`的`Watch Dog`<img src="https://oss.javaguide.cn/github/javaguide/distributed-system/distributed-lock/distributed-lock-redisson-renew-expiration.png" alt="Redisson 看门狗自动续期" style="zoom:67%;" />



如何做到**锁可重入**？

> 可重入分布式锁的实现核心思路是线程**在获取锁的时候判断是否为自己的锁**，如果是的话，就不用再重新获取了。
>
> 为此，我们可以**为每个锁关联一个可重入计数器和一个占有它的线程**。
>
> 当可重入计数器大于 0 时，则锁被占有，需要判断占有该锁的线程和请求获取锁的线程是否为同一个

如何保证Redis集群中分布式锁的可靠性？

> 使用 `Redlock` 算法 (红锁)
>
> **Redlock 是 Redis 官方推荐的分布式锁算法**，它使用 **多个独立 Redis 节点** 进行投票，确保锁的可靠性：
>
> 假设 Redis 集群有 5 个实例：
> 1. 在至少一半以上 **N/2 + 1 = 3 个 Redis 节点** 上尝试加锁（SET NX PX）超时时间PX相同
> 2. 计算获取锁的总时间（必须小于超时时间 `TTL`） 也就是在`TTL`时间内有3个节点都申请到了锁
> 3. 如果超过半数节点成功加锁，认为加锁成功
> 4. 释放锁时，依次删除各个节点的锁



###### ZooKeeper分布式锁

**获取锁**：

> ①首先我们要有一个持久节点`/locks`，客户端获取锁就是在`locks`下创建临时顺序节点。
>
> ②假设客户端 1 创建了`/locks/lock1`节点，创建成功之后，会判断 `lock1`是否是 `/locks` 下最小的子节点。
>
> ③如果 `lock1`是最小的子节点，则获取锁成功。否则，获取锁失败。
>
> ④如果获取锁失败，则说明有其他的客户端已经成功获取锁。
>
> ⑤客户端 1 并**不会不停地循环**去尝试加锁，而是在**前一个节点**比如`/locks/lock0`上注册一个**事件监听器（Watcher）**
>
> 这个监听器的作用是当前一个节点释放锁之后通知客户端 1（避免无效自旋），这样客户端 1 就加锁成功了

**释放锁**：

> ①成功获取锁的客户端在执行完业务流程之后，会将对应的子节点删除。
>
> ②成功获取锁的客户端在出现故障之后，对应的子节点由于是**临时顺序节点**，也**会被自动删除**，避免了锁无法被释放。
>
> ③我们前面说的事件监听器其实监听的就是这个子节点删除事件，子节点删除就意味着锁被释放

**工具**：

推荐使用 `Curator` 



###### Redis分布式锁和Zookeeper分布式锁的区别

ZooKeeper 相比于 Redis 实现分布式锁，除了提供相对**更高的可靠性**之外，在功能层面还有一个非常有用的特性：**Watch 机制**，但是ZooKeeper分布式锁**性能低**

如果对**性能要求比较高**，建议使用 Redis 实现分布式锁，推荐优先选择 **Redisson** 提供的现成分布式锁

如果对**可靠性要求比较高**，建议使用 ZooKeeper 实现分布式锁，推荐基于 **Curator** 框架来实现





##### 分布式事务

###### CAP和BASE

**ACID 是数据库事务完整性的理论，CAP 是分布式系统设计理论，BASE 是 CAP 理论中 AP 方案的延伸。**

> CAP：对于一个分布式系统来说，只能能同时满足以下三点中的两个：
>
> - **一致性（Consistence）** : 所有节点访问同一份最新的数据副本
> - **可用性（Availability）**: 非故障的节点在合理的时间内返回合理的响应（不是错误或者超时的响应）
> - **分区容错性（Partition tolerance）** : 分布式系统出现网络分区的时候，仍然能够对外提供服务
>
> CAP 理论中**分区容错性 P 是一定要满足的**，在此基础上，只能满足可用性 A 或者一致性 C，无法同时满足CA
>
> > **ZooKeeper 保证的是 CP**： 任何时刻对 ZooKeeper 的读请求都能得到一致性的结果，但是， ZooKeeper 不保证每次请求的可用性比如在 Leader 选举过程中或者半数以上的机器不可用的时候服务就是不可用的

> **BASE** 是 **Basically Available（基本可用）** 、**Soft-state（软状态）** 和 **Eventually Consistent（最终一致性）** 三个短语的缩写。BASE 理论是对 CAP 中一致性 C 和可用性 A 权衡的结果
>
> * 基本可用：基本可用是指分布式系统在出现不可预知故障的时候，**允许损失部分可用性**
> * 软状态：允许系统中的数据存在中间状态（**CAP 理论中的数据不一致**），并不影响系统可用性
> * 最终一致性：强调系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态

###### 柔性事务和刚性事务

分布式事务中**数据一致性的三种级别**：强一致性、弱一致性、最终一致性

> 柔性事务追求的是最终一致性，如TCC、MQ事务
>
> 刚性事务追求的是强一致性，如2PC、3PC就是刚性事务
>

###### 分布式事务的解决方案：2PC、3PC、TCC、MQ事务

**2PC**：2 Phrase Commit 两阶段提交

> 分为两阶段提交：
>
> * 准备阶段：TM(事务管理者)去询问所有事务相关的RM(资源管理者，数据库)是否执行成功，RM们写入redolog/undolog，但不会提交事务，向TM回复yes
> * 提交阶段：TM收到所有的回复后，向所有RM发送可以提交事务，RM提交事务后回复ACK
>
> 如果其中有一个RM回复失败，那么TM就通知所有RM执行回滚

**3PC**：三阶段提交

> 比2PC多了一个预提交阶段
>
> * 准备阶段：同样询问事务是否执行成功
> * 预提交阶段：全部返回执行成功，则通知写入redo log；否则通知写入undo log
> * 提交阶段：RM提交事务
>
> 3PC还引入了超时中断机制，一定时间内没有收到事务参与者的消息，默认失败，避免参与者一直阻塞

**TCC**：补偿事务

> Try尝试阶段：尝试执行，检查资源是否满足，并**预留资源** （例如检查余额）
>
> Commit阶段：如果尝试阶段没问题，那么真正的执行事务，处理Try阶段预留的资源
>
> Cancel：前两个阶段有一个失败了， 就取消执行，释放Try阶段预留的资源

**MQ事务**：

> 1.MQ发送方给MQ服务器发送一个**“半”消息**
>
> 2.MQ服务器收到半消息后，回复MQ发送方
>
> 3.发送方收到半消息后，**开始执行本地事务**
>
> 4.MQ发送方根据事务执行的结果（Comit/rollback）告诉给MQ服务器，服务器通知MQ订阅方
>
> 5.如果是Commit，那么将消息发送给消费者进行消费
>
> 6.如果是Rollback，那么将消息丢掉不转发

![image-20250311144232156](面经.assets/image-20250311144232156.png)



##### 分布式限流算法

###### ①计数器

> 固定窗口计数器：
>
> ​	计数器比较简单粗暴，比如我们要**限制 1s 能够通过的请求数**，实现的思路就是从第一个请求进来开始计时，在接下来的 1s 内，每个请求进来请求数就+1，超过最大请求数的请求会被拒绝，等到 1s 结束后计数清零，重新开始计数
>
> 这种方式有个很大的弊端：比如前 10ms 已经通过了最大的请求数，那么后面的 990ms 的请求只能拒绝，这种现象叫做“**突刺现象**”
>
> 解决：`平滑窗口计数器`：**把时间以一定比例分片**，粒度更小
>
> 例如将1s内的100次限制均等分在每个10ms窗口中

###### ②漏桶算法

> 假设有一个桶，桶底**出水的速度**恒定，**进水的速度**可能快慢不一
>
> 当进水量大于出水量的时候，水会被装在桶里，不会直接被丢弃，溢出的部分会被丢弃

###### ③令牌桶算法

> 令牌桶就是生产访问令牌的一个地方，生产的速度恒定，用户访问的时候当桶中有令牌时就可以访问，否则将触发限流
>
> <img src="http://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene//fenbushi-b80e74ed-9b0a-4327-9ea0-3bef4da76634.jpg" alt="img" style="zoom:50%;" />
>
> ①先使用Redis初始化一个令牌桶，大小为100
>
> ```
> redis-cli SET "token_bucket" "100"
> ```
>
> ②使用 **Lua 脚本**实现令牌桶算法：假设每秒向桶中添加 10 个令牌，但不超过桶的最大容量
>
> ```lua
> -- Lua 脚本来添加令牌，并确保不超过最大容量
> local bucket = KEYS[1]
> local add_count = tonumber(ARGV[1])
> local max_tokens = tonumber(ARGV[2])
> local current = tonumber(redis.call('GET', bucket) or 0)
> local new_count = math.min(current + add_count, max_tokens)
> redis.call('SET', bucket, tostring(new_count))
> return new_count
> ```
>
> 写一个死循环调用这个脚本，以恒定的速率创建令牌
>
> ```shell
> #!/bin/bash
> while true; do
>     redis-cli EVAL "$(cat add_tokens.lua)" 1 token_bucket 10 100
>     sleep 1
> done
> ```
>
> ③当请求到达时，需要检查并消耗一个令牌
>
> ```lua
> -- Lua 脚本来消耗一个令牌
> local bucket = KEYS[1]
> local tokens = tonumber(redis.call('GET', bucket) or 0)
> if tokens > 0 then
>     redis.call('DECR', bucket)
>     return 1  -- 成功消耗令牌
> else
>     return 0  -- 令牌不足
> end
> ```
>
> 

###### 其他限流方法

> ①针对IP进行限流
>
> ②针对业务ID进行限流
>
> ③个性化限流：例如VIP不限流、普通用户限流

###### 网盘这种下载限流如何实现?

> ①令牌桶：每个用户有一个令牌桶，**令牌按照固定速率生成**来处理下载请求，**下载文件时消耗令牌**，没有令牌则限速
>
> ②直接控制 TCP 连接的带宽：控制发送窗口的大小
>
> ③使用 HTTP `Range` 头部 来控制分段下载：在range中限制分段的范围



###### 项目中可以使用网关进行限流

> Spring Cloud Gateway 可以结合 `Sentinel` 实现更强大的网关流量控制



##### 分布式一致性算法

分布式系统**共识**算法

###### Paxos算法

Basic Paxos

> Basic Paxos 中存在 3 个重要的角色：
>
> 1. **提议者（Proposer）**：也可以叫做协调者（coordinator），提议者负责接受客户端的请求并发起提案。提案信息通常包括提案编号 (Proposal ID) 和提议的值 (Value)
> 2. **接受者（Acceptor）**：也可以叫做投票员（voter），负责对提议者的提案进行投票，同时需要记住自己的投票历史
> 3. **学习者（Learner）**：如果**有超过半数接受者就某个提议达成了共识**，那么学习者就需要接受这个提议，并就该提议作出运算，然后将运算结果返回给客户端
>
> <img src="https://oss.javaguide.cn/github/javaguide/distributed-system/protocol/up-890fa3212e8bf72886a595a34654918486c.png" alt="img" style="zoom:67%;" />
>
> **一句话**：<u>提议者发起提案，接收者进行投票，超过半数投票者投票，那么将其转发给学习者学习</u>

Multi Paxos

> 因为Basic Paxos 算法的**仅能就==单个值==达成共识**，为了能够对一系列的值达成共识，我们需要用到 Multi Paxos 思想
>
> 通过多个 Basic Paxos 实例就一系列值达成共识
>
> 变体 => Raft



###### Raft算法

三种角色/节点：

> `Leader`：负责发起**心跳**，响应客户端请求，创建日志，同步日志。【一个集群中只有一个Leader】
>
> `Candidate`：Leader 选举过程中的**临时角色**，由 Follower 转化而来，发起投票参与竞选
>
> `Follower`：接受 Leader 的心跳和日志同步数据，投票给 Candidate
>
> ​					Follower是被动的，它们不会发送任何请求，只是响应来自 Leader 和 Candidate 的请求
>
> 一个集群中，只有一个Leader，剩下的全是Follower

任期：

> Raft将时间划分为多个**任意长度**的Term(任期)，每个任期有一个连续的序号（Term1、Term2、... ）
>
> ![img](https://oss.javaguide.cn/github/javaguide/paxos-term.png)
>
> **每一个任期的开始**都是一次**选举**（蓝色部分），在选举开始时， Candidate 们会尝试成为 Leader
>
> 如果某个Candidate选举成功，那么它将在该任期中充当Leader
>
> 如果没有Candidate选举成功，那么**直接进入下一个任期**（如图中Term3没有选举出Leader，直接进入Term4）
>
> 
>
> 关于Term号的更新和同步：
>
> ​	每个节点都存有一个当前的Term号，当服务器之间进行**通信时**会交换当前的 term 号
>
> ​	如果一个节点收到的请求的Term号比自己小，说明这个Term号已经过期，忽略此请求
>
> ​	如果一个节点收到的请求的Term号比自己大，说明它的Term号已经过期，它会更新自己的Term号
>
> ​	如果一个Candidate或者Leader发现自己的Term号过期，那么降级为Follower（此时如果Leader变为Follower，那么进入下一个Term）

日志：

> **只有Leader可以创建和修改日志**，Follower从Leader拉取日志
>
> 日志log是一个entry数组，每个entry为一个(termId, index, cmd)元组，cmd为可以应用到状态机的操作
>
> Leader添加日志后，发起共识请求，将entry应用到状态机，然后Follower从Leader获取新日志，并应用到状态机中

:star:Leader的选举：Raft 使用心跳机制来触发 Leader 的选举

> 在一个任期内，Leader会向Follower们周期性地发送心跳包来保持自己的Leader地位
>
> 如果有Follower在某个周期内没收到Leader的心跳包（选举超时），就认为此时集群中已经没有Leader
>
> 因此它自己就升级为Candidate，并将Term加一，表示进入下一个任期。并向所有节点发送**RequestVoteRPC请求**
>
> 该Candidate赢得选举的条件是：在该任期内收到了集群的多数投票，即`N/2 + 1`
>
> **特殊情况1**：如果在选举过程中，该Candidate收到了Leader的心跳
>
> * 如果该心跳的Term大于等于自己的Term，那么它又恢复为Follower(表示已经进入新的Term并且已经有Leader)
> * 如果该心跳的Term小于自己的Term，那么拒绝，继续选举
>
> **特殊情况2**：在Leader发送心跳周期中，可能有多个Follower都没有收到心跳，它们都变成Candidate
>
> ​	这会导致可能没有某个Candidate能收到大于半数的投票，从而选举不出Leader
>
> ​	Raft使用**随机选举超时时间**，每一个 Candidate 在发起选举后，都会随机化一个新的选举超时时间

日志复制：Leader 和 Follower 的日志保持一致

> Leader 收到客户端请求后，会生成一个 entry，包含`<index,term,cmd>`，再将这个 entry 添加到自己的日志末尾后，向所有的节点广播该 entry，要求其他服务器复制这条 entry。
>
> 如果 Follower 接受该 entry，则会将 entry 添加到自己的日志后面，同时返回给 Leader 同意。
>
> 如果 Leader 收到了**多数的成功响应**，Leader 会将这个 entry 应用到自己的状态机中，之后可以称这个 entry 是 committed 的，并且向客户端返回执行结果。
>
> 
>
> `Leader` 给每一个`Follower` 维护了一个 `nextIndex`，它表示 `Leader` 将要发送给该追随者的下一条日志条目的索引。
>
> 当一个 `Leader` 开始掌权时，它会将 `nextIndex` 初始化为它的最新的日志条目索引数+1。
>
> 如果一个 `Follower` 的日志和 `Leader` 的不一致，`AppendEntries` 一致性检查会在下一次 `AppendEntries RPC` 时返回失败。在失败之后，`Leader` 会将 `nextIndex` 递减然后重试 `AppendEntries RPC`。最终 `nextIndex` 会达到一个 `Leader` 和 `Follower` 日志一致的地方。
>
> 这时，`AppendEntries` 会返回成功，`Follower` 中冲突的日志条目都被移除了，并且添加所缺少的上了 `Leader` 的日志条目。一旦 `AppendEntries` 返回成功，`Follower` 和 `Leader` 的日志就一致了，这样的状态会保持到该任期结束

Candidate如何得到其他节点的投票？

> 每个 Candidate 发送 RequestVoteRPC 时，都会带上日志中**最后一个 entry** 的信息
>
> 所有节点收到投票信息时，会对该 entry 进行比较，如果发现自己的新，则拒绝投票给该 Candidate
>
> 判断一条日志是否新：如果两个日志的 term 不同，term 大的更新；如果 term 相同，更长的 index 更新（一个任期内日志的序号）



###### Gossip协议

分布式系统中用于同步数据，解决**主节点为中心**向所有其他节点共享最新数据存在的问题：单点故障

在 Gossip 协议下，没有所谓的中心节点，每个节点周期性地随机找一个节点互相同步彼此的信息，理论上来说，各个节点的状态最终会保持一致。是分布式系统中一种**去中心化的通信协议**

**Redis集群**就是使用了Gossip协议

Gossip中的两种消息传播模式：**反熵** & **传谣**

> **反熵**就是指<u>消除不同节点中数据的差异</u>，提升节点间数据的相似度，从而降低熵值
>
> - **推**方式，就是将自己的**所有**副本数据，推给对方，修复对方副本中的熵
> - **拉**方式，就是拉取对方的所有副本数据，修复自己副本中的熵
> - **推拉**就是同时修复自己副本和对方副本中的熵
>
> 反熵一般会设计成一个**闭环**将进行
>
> ![img](https://javaguide.cn/assets/%E5%8F%8D%E7%86%B5-%E9%97%AD%E7%8E%AF-BPAGw_p4.png)
>
> * 节点 A 推送数据给节点 B，节点 B 获取到节点 A 中的最新数据
> * 节点 B 推送数据给 C，节点 C 获取到节点 A，B 中的最新数据
> * 节点 C 推送数据给 A，节点 A 获取到节点 B，C 中的最新数据
> * 节点 A 再推送数据给 B 形成闭环，这样节点 B 就获取到节点 C 中的最新数据

> **传谣**指的是分布式系统中的一个节点一旦有了**新数据**之后，就会变为活跃节点，活跃节点会**周期性地**联系其他节点向其发送**新数据**，直到所有的节点都存储了该新数据
>
> 传谣一般只传播该节点新增的数据而不是全部数据，以减少网络开销。传谣适合节点多的情况，速度快

Gossip强调的是**最终一致性**，数据在传播的过程中，节点之间的数据肯定是不一致的

优点：简单、去中心化、允许节点的随意加入和退出、速度快

缺点：最终一致、不允许存在恶意节点、存在消息冗余现象（一个节点收到多次重复数据）



##### 分布式调用RPC

RPC，Remote Procedure Call，远程过程调用，用于一个微服务调用另一个微服务

RPC 的出现就是为了可以像**调用本地方法一样**来调用远程方法

常见的微服务框架：Apache Dubbo、Apache Thrift

###### 原理

![RPC原理图](https://oss.javaguide.cn/github/javaguide/distributed-system/rpc/37345851.jpg)

> 1.服务消费端（client）以**本地调用的方式**调用远程服务
>
> 2.客户端 Stub（client stub） 接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体（序列化）：`RpcRequest`
>
> 3.客户端 Stub（client stub） 找到远程服务的地址，并将消息发送到服务提供端
>
> 4.服务端 Stub（桩）收到消息将消息反序列化为 Java 对象: `RpcRequest`
>
> 5.服务端 Stub（桩）根据`RpcRequest`中的类、方法、方法参数等信息调用本地的方法
>
> 6.服务端 Stub（桩）得到方法执行结果并将组装成能够进行网络传输的消息体`RpcResponse`（序列化）发送至消费方
>
> 7.客户端 Stub（client stub）接收到消息并将消息反序列化为 Java 对象:`RpcResponse` ，这样也就得到了最终结果

RPC框架提供了 IDL（接口定义语言）来定义服务接口，以及序列化协议来进行数据传输

###### RPC vs. HTTP

> ①服务注册与发现：注册中心如Nacos可以去管理服务提供方的服务接口和地址，服务调用方可以去订阅
>
> ​	一旦服务提供方换了地址，服务调用方也可以即使收到通知
>
> ②**强类型接口定义**：通过IDL（接口描述语言）明确定义服务契约，自动生成客户端桩/服务端桩代码，减少手写代码错误
>
> ​	开发效率也更高
>
> ③RPC性能更高
>
> ④跨语言

###### RPC常用框架Dubbo

六大核心功能：

> 1. 面向接口代理的高性能 RPC 调用
> 2. 智能容错和负载均衡
> 3. 服务自动注册和发现
> 4. 高度可扩展能力
> 5. 运行期流量调度
> 6. 可视化的服务治理与运维

框架：

> ![dubbo-relation](https://oss.javaguide.cn/%E6%BA%90%E7%A0%81/dubbo/dubbo-relation.jpg)
>
> **Registry**：服务注册与发现的注册中心
>
> **Consumer：** 调用远程服务的服务消费方，会向注册中心订阅自己所需的服务
>
> **Provider：** 暴露服务的服务提供方，会向注册中心注册自己提供的服务
>
> ​	**Container：** 服务运行容器，负责加载、运行服务提供者
>
> **Monitor：** 统计服务的调用次数和调用时间的监控中心

Dubbo中的`Invoker`

> `Invoker` 就是 Dubbo 对**远程调用的抽象**
>
> ​	Consumer方的Invoker
>
> ​	Prodiver方的Invoker
>
> ![dubbo_rpc_invoke.jpg](https://oss.javaguide.cn/java-guide-blog/dubbo_rpc_invoke.jpg)



##### 分布式微服务

微服务框架的组件：

<img src="https://cdn.tobebetterjavaer.com/paicoding/99350e76ce2f763ed5ce5ba6594941f8.png" alt="微服务组件示意图" style="zoom:67%;" />

###### 注册中心

> 背景：原始项目中，每个微服务的地址都必须写死在配置文件中，不便于对服务进行扩容或者故障转移
>
> ​		将这些东西交给注册中心来维护，注册中心维护了一份可用的服务列表，动态的获取可用服务的地址
>
> ​		当服务地址发生改变，可以自动地通知相关服务，从而更新服务地址，无需手动更新、重启服务
>
> **注册中心**是用来管理和维护分布式系统中各个服务的地址和元数据的组件

服务注册中心的三大基本功能：

> ①服务注册以及服务发现
>
> ②服务状态变更通知、服务健康检查、不可用服务剔除
>
> ③服务权重配置（权重越高访问越高） => 从而实现负载均衡

> **服务注册**：服务提供方向Nacos注册自己的服务并提供地址
>
> **服务发现**：服务使用方拿着服务信息去Nacos中查找服务地址，并在自己本地缓存一份服务地址列表（防止Nacos宕机）
>
> ![service-registration-and-discovery](面经.assets/service-registration-and-discovery.png)

**Eureka、ZooKeeper、Nacos 的区别？**

| 特性     | **Eureka**                       | **ZooKeeper**                      | **Nacos**                          |
| -------- | -------------------------------- | ---------------------------------- | ---------------------------------- |
| 开发公司 | Netflix                          | Apache                             | 阿里巴巴                           |
| **CAP**  | AP（可用性和分区容忍性）         | CP（一致性和分区容忍性）           | 既支持 AP，也支持 CP               |
| 功能     | 服务注册与发现                   | 分布式协调、配置管理、分布式锁     | 服务注册与发现、配置管理、服务管理 |
| 定位     | 适用于构建基于 HTTP 的微服务架构 | 通用的分布式协调服务框架           | 适用于微服务和云原生应用           |
| 访问协议 | HTTP                             | TCP                                | HTTP/DNS                           |
| 自我保护 | 支持                             | -                                  | 支持                               |
| 数据存储 | 内嵌数据库、多个实例形成集群     | ACID 特性的分布式文件系统 ZAB 协议 | 内嵌数据库、MySQL 等               |
| 健康检查 | Client Beat                      | Keep Alive                         | TCP/HTTP/MYSQL/Client Beat         |
| 特点     | 简单易用、自我保护机制           | 高性能、强一致性                   | 动态配置管理、流量管理、灰度发布等 |

可以看到 Eureka 和 ZooKeeper 的最大区别是一个支持`AP`，一个支持`CP`，Nacos 既支持既支持`AP`，也支持`CP`

**基本原理**：以Eureka为例

> ①**服务注册与发现**: 当一个服务实例启动时，它会向 Eureka Server 发送注册请求，将自己的信息注册到注册中心。Eureka Server 会将这些信息保存在内存中，并提供 REST 接口供其他服务查询。服务消费者可以通过查询服务实例列表来获取可用的服务提供者实例，从而实现服务的发现
>
> ②**服务健康检查**: Eureka 通过心跳机制来检测服务实例的健康状态。服务实例会定期向 Eureka Server 发送心跳，也就是续约，以表明自己的存活状态。如果 Eureka Server 在一定时间内没有收到某个服务实例的心跳，则会将其标记为不可用，并从服务列表中移除，下线实例
>
> ③**服务负载均衡**: Eureka 客户端在调用其他服务时，会从本地缓存中获取服务的注册信息。如果缓存中没有对应的信息，则会向 Eureka Server 发送查询请求。Eureka Server 会返回一个可用的服务实例列表给客户端，客户端可以使用负载均衡算法选择其中一个进行调用



###### 配置中心

> 用于集中管理微服务的配置信息，可以动态修改配置而不需要重启服务。如数据库连接地址、服务端口、日志级别等

**Nacos的原理**：

> **配置信息存储**：Nacos 默认使用内嵌数据库 Derby 来存储配置信息，还可以采用 MySQL 等关系型数据库
>
> **注册配置信息**：服务启动时，Nacos Client 会向 Nacos Server 注册自己的配置信息，这个注册过程就是<u>把配置信息写入存储，并生成版本号</u>
>
> **获取配置信息**：服务运行期间，Nacos Client 通过 API 从 Nacos Server 获取配置信息。Server 根据键查找对应的配置信息，并返回给 Client
>
> **监听配置变化**：Nacos Client 可以通过注册监听器的方式，实现对配置信息的监听。<u>当配置信息发生变化时，Nacos Server 会通知已注册的监听器，并触发相应的回调方法</u>
>
> <img src="https://cdn.tobebetterjavaer.com/paicoding/1fe746722ca8f0d4ed608c1402fbd692.png" alt="配置中心" style="zoom:67%;" />

**Nacos的长轮询机制**：

> 客户端和服务端两种推送机制：Pull 和 Push
>
> Nacos客户端使用的是**Pull + 长轮询**，即：
>
> ​	Nacos客户端**定时**向服务端发起请求，检查配置信息是否发生变更。如果没有变更，服务端会**"hold"住**这个请求，即暂时不返回结果，直到配置发生变化或达到一定的超时时间
>
> ![Nacos长轮询](https://cdn.tobebetterjavaer.com/paicoding/16de67d4619e2844fb0812cf994cd7e7.png)



###### 熔断降级

> **服务雪崩**：在微服务中，假如一个或者多个服务出现故障，如果这时候，依赖的服务还在不断发起请求，或者重试，那么这些请求的压力会不断在下游堆积，导致下游服务的负载急剧增加。不断累计之下，可能会导致故障的进一步加剧，可能会导致级联式的失败，甚至导致整个系统崩溃，这就叫服务雪崩
>
> 解决措施：熔断 or 降级  or 限流

> **熔断**：当某个服务出现故障或异常时，服务熔断可以**快速隔离该服务**，确保系统稳定可用。
>
> ​	它通过监控服务的调用情况，当<u>错误率或响应时间</u>超过阈值时，触发熔断机制，后续请求将<u>返回默认值或错误信息</u>，避免资源浪费和系统崩溃
>
> ​	服务熔断还支持**自动恢复**，重新尝试对故障服务的请求，确保服务恢复正常后继续使用

> **(服务)降级**：当系统出现异常情况时，服务降级会主动<u>屏蔽一些非核心或可选的功能</u>，而只提供<u>最基本的功能</u>，以确保系统的稳定运行



------

## 高性能 :white_check_mark:

掌握CDN、负载均衡及算法、数据库优化（见Mysql）、消息队列（见MQ）

##### CDN （内容分发网络） 

Content Delivery Network/Content Distribution Network

> 内容：**静态资源**（图片、视频、HTML、CSS、JS...）
>
> 分发网络：将这些静态资源<u>分发到位于多个不同的地理位置机房中的服务器</u>上，从而实现静态资源的**就近访问**
>
> **CDN 就是将静态资源分发到多个不同的地方以实现就近访问，进而加快静态资源的访问速度，减轻服务器以及带宽的负担**

###### 静态资源如何被缓存到CDN节点中？

> **预热**：指提前将内容缓存到 CDN 节点上。这样当用户在请求这些资源时，能够快速地从最近的 CDN 节点获取到而不需要回源，进而减少了对源站的访问压力，提高了访问速度
>
> **回源**：当 CDN 节点上没有用户请求的资源或该资源的缓存已经过期时，CDN 节点需要**从原始服务器获取最新的资源**内容，这个过程就是回源。当用户请求发生回源的话，会导致该请求的响应速度比未使用 CDN 还慢，因为相比于未使用 CDN 还多了一层 CDN 的调用流程
>
> <img src="https://oss.javaguide.cn/github/javaguide/high-performance/cdn/cdn-back-to-source.png" alt="CDN 回源" style="zoom:67%;" />
>
> 资源更新时，可以删除CDN节点缓存，强制从源站获取最新资源
>
> **命中率**和**回源率**是用来衡量CDN服务指令的两个重要指标
>
> 命中率越高越好，回源率越低越好

###### 如何找到最合适的CDN节点？

> 通过**全局负载均衡(GSLB)**来实现
>
> <img src="https://oss.javaguide.cn/github/javaguide/high-performance/cdn/cdn-overview.png" alt="CDN 原理示意图" style="zoom:67%;" />
>
> GSLB可以看成CDN专用的DNS服务器，并结合了负载均衡
>
> CDN 会通过 GSLB 找到最合适的 CDN 节点，更具体点来说是下面这样的：
>
> 1. 浏览器向 DNS 服务器发送域名请求
> 2. DNS 服务器向根据 **CNAME**( Canonical Name ) 别名记录向 GSLB 发送请求
> 3. GSLB 返回性能最好（通常距离请求地址最近）的 CDN 节点（边缘服务器，真正缓存内容的地方）的地址给浏览器
> 4. 浏览器直接访问指定的 CDN 节点

**CDN防盗刷**：

> ①设置 **Referer 防盗链**： HTTP请求头中 `referer` 获取到当前请求页面的来源页面的网站地址
>
> ②时间戳防盗链：间戳防盗链加密的 URL 具有时效性，过期之后就无法再被允许访问。一个是签名字符串，一个是过期时间。签名字符串一般是通过对用户设定的加密字符串、请求路径、过期时间通过 MD5 哈希算法取哈希的方式获得





##### 负载均衡

> **负载均衡** 指的是将用户**请求分摊**到不同的服务器上处理，以提高系统整体的并发处理能力以及可靠性
>
> 分类：
>
> ①服务端负载均衡：请求与网关层之间，软硬件均可实现
>
> ​	还可以分为：二层、三层、**四层**、**七层**负载均衡（层表示工作在网络分层中第几层）
>
> ​	四层：工作在传输层，**IP+端口层面**的负载均衡，不涉及报文内容
>
> ​	七层：工作在应用层，读取**报文内容**，根据数据内容进行负载均衡（反向代理服务器）
>
> ​	四层负载均衡性能强，七层负载均衡功能强
>
> ​	工作中，**LVS**(Linux Virtual Server虚拟服务器)来做四层负载均衡，**Ngnix**做七层负载均衡
>
> ②客户端负载均衡：客户端维护一份**服务器地址列表**，发请求前根据负载均衡算法去发
>
> ​	Spring Cloud Load Balancer

###### 负载均衡算法

> ①随机法：随机分散，也可以给服务器配权重，比如性能高的服务器权重大一点
>
> ②轮询法：挨个轮询服务器处理  ==> 改进：平滑的加权轮询法
>
> ​	S1（权重5）S2（权重3）S3（权重1）
>
> ​	轮询法：S1、S1、S1、S1、S1、S2、S2、S2、S3
>
> ​	平滑的加权轮询：S1, S2, S3, S1, S2, S1, S3, S1, S2, S1 
>
> ③两次随机法：随机两台，根据这两台服务器的负载去选择
>
> ④哈希法：将请求中的参数信息通过hash，再来选择服务器
>
> 问题：`serviceIndex = hash(key) % N`
>
> ​	当服务器数量 N 发生变化时(例如横向扩展)，几乎所有的映射关系都会发生变化，导致大量 **缓存失效、数据迁移**，带来 **缓存雪崩** 和 **大量数据库请求**
>
> ⑤**一致性哈希法**：使用 **哈希环** 来避免取模哈希带来的大规模数据迁移问题
>
> 将**数据和节点**都映射到一个**哈希环**上，然后根据哈希值的顺序来确定数据属于哪个节点。当服务器增加或删除时，只影响该服务器的哈希，而不会导致整个服务集群的哈希键值重新分布
>
> ⑥最小连接法：遍历服务器节点列表选择其中**连接数**最小的服务器
>
> ⑦最小活跃法：选择**正在处理的请求数**最少的
>
> ⑧最快响应时间法：客户端维护每台服务器的响应时间，每次挑响应时间最短的

###### 七层负载均衡常用实现方法 —— DNS解析 & 反向代理

DNS解析

> 在 DNS 服务器中为同一个域名记录配置多个 IP 地址，这些 IP 地址对应不同的服务器。当用户请求域名的时候，DNS 服务器采用轮询算法返回 IP 地址，这样就实现了轮询版负载均衡

反向代理

> 客户端将请求发送到反向代理服务器，由反向代理服务器去选择目标服务器，**获取数据后**再返回给客户端。即对外暴露反向代理服务器的地址，隐藏了真实服务器IP地址
>
> Nginx
>
> 单点故障： **Keepalived + 两台 Nginx 做主备**，构建一个高可用的负载均衡层





##### API网关（微服务）

> 微服务背景下，一个系统被拆分为多个服务，但是像安全认证，流量控制，日志，监控等功能是每个服务都需要的，没有网关的话，我们就需要在每个服务中单独实现，这使得我们做了很多重复的事情并且没有一个全局的视图来统一管理这些功能
>
> **网关**可以为我们提供请求转发、安全认证（身份/权限认证）、流量控制、负载均衡、降级熔断、日志、监控、参数校验、协议转换等功能
>
> 为了避免网关的单点故障，可以使用**负载均衡**，将请求分散到**不同的网关**上
>
> <img src="https://oss.javaguide.cn/github/javaguide/high-performance/load-balancing/server-load-balancing.png" alt="基于 Nginx 的服务端负载均衡" style="zoom:67%;" />

###### 网关的功能

最主要的两个功能：请求转发 + 请求过滤

> **请求转发**：将请求转发到目标微服务
>
> **负载均衡**：根据各个微服务实例的负载情况或者具体的负载均衡策略配置对请求实现动态的负载均衡
>
> **安全认证**：对用户请求进行身份验证并仅允许可信客户端访问 API，并且还能够使用类似 RBAC 等方式来授权
>
> **参数校验**：支持参数映射与校验逻辑
>
> **日志记录**：记录所有请求的行为日志供后续使用
>
> **监控告警**：从业务指标、机器指标、JVM 指标等方面进行监控并提供配套的告警机制
>
> **流量控制**：对请求的流量进行控制，也就是限制某一时刻内的请求数
>
> **熔断降级**：实时监控请求的统计信息，达到配置的失败阈值后，自动熔断，返回默认值
>
> **响应缓存**：当用户请求获取的是一些静态的或更新不频繁的数据时，一段时间内多次请求获取到的数据很可能是一样的。对于这种情况可以将响应缓存起来。这样用户请求可以直接在网关层得到响应数据，无需再去访问业务服务，减轻业务服务的负担
>
> **响应聚合**：某些情况下用户请求要获取的响应内容可能会来自于多个业务服务。网关作为业务服务的调用方，可以把多个服务的响应整合起来，再一并返回给用户。
>
> **灰度发布**：将请求动态分流到不同的服务版本（最基本的一种灰度发布）。
>
> **异常处理**：对于业务服务返回的异常响应，可以在网关层在返回给用户之前做转换处理。这样可以把一些业务侧返回的异常细节隐藏，转换成用户友好的错误提示返回。
>
> **API 文档：** 如果计划将 API 暴露给组织以外的开发人员，那么必须考虑使用 API 文档，例如 Swagger 或 OpenAPI。
>
> **协议转换**：通过协议转换整合后台基于 REST、AMQP、Dubbo 等不同风格和实现技术的微服务，面向 Web Mobile、开放平台等特定客户端提供统一服务。
>
> **证书管理**：将 SSL 证书部署到 API 网关，由一个统一的入口管理接口，降低了证书更换时的复杂度。



###### OpenResty

> 基于 Ngnix + Lua
>
> 在 Nginx 内部里嵌入 Lua 脚本，使得可以通过简单的 Lua 语言来扩展网关的功能，比如实现自定义的路由规则、过滤器、缓存策略等
>



###### Spring Cloud Gateway

Spring Cloud Gateway 基于 Spring WebFlux 。Spring WebFlux 使用 Reactor 库来实现响应式编程模型，底层基于 Netty 实现同步非阻塞的 I/O



**框架**：

<img src="https://oss.javaguide.cn/github/javaguide/system-design/distributed-system/api-gateway/springcloud-gateway-%20demo.png" alt="img" style="zoom: 50%;" />

**基本流程**：

![Spring Cloud Gateway 的工作流程](https://oss.javaguide.cn/github/javaguide/system-design/distributed-system/api-gateway/spring-cloud-gateway-workflow.png)

> 工作流程：
>
> ①**路由判断**：客户端的请求到达网关后，先经过 Gateway Handler Mapping 处理，这里面会做**断言**（Predicate）判断，看下**符合哪个路由规则**，这个路由映射后端的某个服务
>
> ②**请求过滤**：然后请求到达 Gateway Web Handler，这里面有很多过滤器，组成**过滤器链**（Filter Chain），这些过滤器可以对请求进行拦截和修改，比如添加请求头、参数校验等等，然后将请求转发到实际的后端服务。这些过滤器逻辑上可以称作 `Pre-Filters`
>
> ③**服务处理**：后端服务会对请求进行处理
>
> ④**响应过滤**：后端处理完结果后，返回给 Gateway 的过滤器再次做处理，逻辑上可以称作 `Post-Filters`
>
> ⑤**响应返回**：响应经过过滤处理后，返回给客户端

**什么是断言`predicates`？**

> **断言**：如果客户端发送的请求满足了断言的条件，则映射到指定的路由器，从而转发到指定服务器
>
> 一个路由规则可以配置一组断言，只有这组断言中**所有断言都满足**，才可以通过
>
> 如果一个请求可以通过多个路由中的断言，那么会映射到**第一个**路由规则
>
> 总结为：
>
> * **一对多**：路由和断言是一对多的关系，一个路由中可以有多个断言
> * **同时满足**：只有路由中的所有断言都满足，才可以匹配到该路由
> * **第一个匹配成功**：如果一个请求匹配了多个路由，则映射为配置文件中第一个路由规则
>
> <img src="https://oss.javaguide.cn/github/javaguide/system-design/distributed-system/api-gateway/spring-cloud-gateway-predicate-route.png" alt="路由和断言的对应关系" style="zoom: 50%;" />
>
> 例如：这个路由(id为1)配置了一个断言，根据请求路径来匹配
>
> ```xml
> routers:
> 	id: 1
> 	predicates:
> 		- Path=/api/thirdparty/**
> ```
>
> ​	如果请求路径中含有api/thirdparty，那么会匹配这个路由
>
> **断言的规则**：
>
> <img src="https://oss.javaguide.cn/github/javaguide/system-design/distributed-system/api-gateway/spring-cloud-gateway-predicate-rules.png" alt="Spring Cloud GateWay 路由断言规则" style="zoom:67%;" />

**Spring Cloud Gateway动态路由配置**：使用`Nacos`注册中心

> 应该避免网关的重启
>
> Spring Cloud Gateway 可以从注册中心获取服务的元数据（例如服务名称、路径等），然后根据这些信息自动生成路由规则。这样，当添加、移除或更新服务实例时，网关会自动感知并相应地调整路由规则，无需手动维护路由配置。



**Spring Cloud Gateway中的过滤器**：

前置 / 后置

> **Pre 类型**：在请求被转发到微服务之前，对请求进行拦截和修改，例如参数校验、权限校验、流量监控、日志输出以及协议转换等操作
>
> **Post 类型**：微服务处理完请求后，返回响应给网关，网关可以再次进行处理，例如修改响应内容或响应头、日志输出、流量监控等

全局/局部

> **GatewayFilter**：局部过滤器，应用在**单个路由或一组路由**上的过滤器。标红色表示比较常用的过滤器
>
> **GlobalFilter**：全局过滤器，应用在**所有路由**上的过滤器
>
> 例如：在router中设置的局部过滤器：
>
> ```xml
> routers:
> 	id: 1
> 	filters: #过滤器
>   		- RewritePath=/api/(?<segment>.*),/$\{segment} # 将跳转路径中包含的 “api” 替换成空
> ```
>
> 



**限流功能**：

> 1.Spring Cloud Gateway 自带了限流过滤器，对应的接口是 `RateLimiter`（基于 Redis + Lua 实现的限流） 不推荐
>
> 2.Spring Cloud Gateway 可以结合 `Sentinel` 实现更强大的网关流量控制 （推荐）



**全局异常处理**：

Springboot项目可以使用@RestControllerAdvice注解实现

Spring Cloud Gateway 可以实现`ErrorWebExceptionHandler`并重写其中的`handle`方法

```java
public class GlobalErrorWebExceptionHandler implements ErrorWebExceptionHandler {
    private final ObjectMapper objectMapper;

    @Override
    public Mono<Void> handle(ServerWebExchange exchange, Throwable ex) {
        // ...
    }
}
```





------

## 系统安全性 :white_check_mark:

##### 认证授权

###### 认证 vs. 授权

> 认证（Authentication）：验证用户的身份，系统是否存在这个用户
>
> ​	例如：账号密码，登录
>
> 授权（Authorization）：发生在认证之后，用来管理用户访问系统各部分的权限
>
> ​	例如：admin是一种管理员权限，普通用户权限，访问

###### RBAC模型（Role-Based Access Control，基于角色的访问控制）

是一种基于角色的权限访问控制模型

> 用户、角色、权限的关系：多对多
>
> ![RBAC 权限模型示意图](https://oss.javaguide.cn/github/javaguide/system-design/security/design-of-authority-system/rbac.png)



###### ABAC模型（Attribute-Based Access Control，基于属性的访问控制）

> 在 `ABAC模型` 中，一个操作是否被允许是基于对象、资源、操作和环境信息共同动态计算决定的。
>
> - **对象**：对象是当前请求访问资源的用户。用户的属性包括 ID，个人资源，角色，部门和组织成员身份等
> - **资源**：资源是当前用户要访问的资产或对象，例如文件，数据，服务器，甚至 API
> - **操作**：操作是用户试图对资源进行的操作。常见的操作包括“读取”，“写入”，“编辑”，“复制”和“删除”
> - **环境**：环境是每个访问请求的上下文。环境属性包含访问的时间和位置，对象的设备，通信协议和加密强度等



##### Cookie vs. Session

`Cookie` 和 `Session` 都是用来**跟踪浏览器用户身份**的会话方式

###### 实践

> 在Springboot项目中，用户登录后，创建一个Cookie，并设置在响应头中返回给客户端
>
> ```java
> @GetMapping("/login")
> public String setCookie(HttpServletResponse response) {
>     // 创建一个 cookie
>     Cookie cookie = new Cookie("username", "Jovan");
>     //设置 cookie过期时间
>     cookie.setMaxAge(7 * 24 * 60 * 60); // expires in 7 days
>     //添加到 response 中
>     response.addCookie(cookie);
> 
>     return "login success!";
> }
> ```
>
> 也可以直接使用提供的@CookieValue注解获取特定的cookie值
>
> ```java
> @GetMapping("/")
> public String readCookie(@CookieValue(value = "username", defaultValue = "Atta") String username) {
>     return "Hey! My username is " + username;
> }
> ```
>
> 

一般使用Session-Cookie进行身份验证

> 用户成功登陆系统，服务端生成一个 `Session` 和对应 `SessionID`
>
> 然后返回给客户端具有 `SessionID` 的 `Cookie` 
>
> 通过 `SessionID` 来实现特定的用户，`SessionID` 一般会选择存放在 Redis 中，并设置过期时间
>
> 当用户下次访问时向后端发起请求会把 `SessionID` 带上，这样后端就知道用户的身份状态了

Seesion依赖于Cookie，需要确保客户端开启了Cookie

但是没有Cookie并不代表无法实现Session，可以将sessionID放在url上



###### 分布式Session-Cookie

> 如果服务部署了多个节点，那么一个用户第一次请求通过负载均衡转发到了A节点，并在A节点上生成了Session，但该用户下一次请求通过负载均衡转发给了B节点，B节点并没有该用户相关的Session信息

解决：

> ①哈希：同一个用户的所有请求都哈希到同一个服务器
>
> ②使用Redis来存储Session，所有服务器都到该Redis中获取Session信息，为了避免单点故障，可以使用Redis集群
>
> ③Spring Session可以实现分布式会话管理





##### JWT（Json Web Token）

目前最流行的跨域认证解决方案，基于Token的认证授权机制

JWT本身带有了身份验证所需要的所有信息，**不需要服务器存储**

###### 组成

> 形式：xxxx.yyyy.zzzz
>
> 使用`.`切分成三个Base64编码：
>
> * Header：描述JWT的元数据，指定生成签名的算法、Token类型
> * Payload：存放实际要传送的数据，包含声明（Claims）、主题（sub）、JWT ID（jti）等
> * Signature（签名）：服务器通过Header、Payload和**密钥**，使用Header中指定的签名算法生成
>
> 例如：
>
> ```
> eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c
> ```
>
> 编译后：
>
> ```json
> {
>     "Header": {    
>       "alg": "HS256",   // 摘要算法
>       "typ": "JWT"		// 令牌类型
>     },
>     "Payload": {
>         "uid": "ff1212f5-d8d1-4496-bf41-d2dda73de19a",
>         "sub": "1234567890",
>         "name": "John Doe",
>         "exp": 15323232,
>         "iat": 1516239022,
>         "scope": ["admin", "user"]
>     }
> }
> ```
>
> 

###### Payload中的声明Claims

> **Registered Claims（注册声明）**：预定义的一些声明，建议使用，但不是强制性的
>
> **Public Claims（公有声明）**：JWT 签发方可以自定义的声明，但是为了避免冲突，应该在 [IANA JSON Web Token Registry](https://www.iana.org/assignments/jwt/jwt.xhtml) 中定义它们
>
> **Private Claims（私有声明）**：JWT 签发方因为项目需要而自定义的声明，更符合实际项目场景使用
>
> 常见声明：
>
> - `iss`（issuer）：JWT 签发方。
> - `iat`（issued at time）：JWT 签发时间。
> - `sub`（subject）：JWT 主题。
> - `aud`（audience）：JWT 接收方。
> - `exp`（expiration time）：JWT 的过期时间。
> - `nbf`（not before time）：JWT 生效时间，早于该定义的时间的 JWT 不能被接受处理。
> - `jti`（JWT ID）：JWT 唯一标识。

###### JWT验证过程

> ![ JWT 身份验证示意图](https://oss.javaguide.cn/github/javaguide/system-design/jwt/jwt-authentication%20process.png)
>
> 1.用户向服务器发送用户名、密码以及验证码等
>
> 2.服务端生成已经签名好的Token，即JWT
>
> 3.用户每次向服务端发送请求时都会带上JWT
>
> 4.服务端从这个JWT中解析获取用户信息
>
> 客户端一般把JWT 存放在 `localStorage` 中而不是`Cookie`中，避免CSRF
>
> 一般JWT都是携带在Header的`Authorization`中

JWT解决了移动端没有Cookie机制的问题

###### 优缺点

> 优点：
>
> ①无状态：因为JWT自身包含了身份验证所需要的所有信息，服务端不需要存储，也减轻了服务端的压力
>
> ②避免了CSRF（跨站请求伪造）：依赖于Cookie，Cookie被窃取后可以伪造你本人发送一些请求。而JWT存储在localStorage中，每次请求客户端都会自动将其携带上，整个过程不涉及Cookie
>
> ③适合移动端：移动端没有Cookie
>
> ④单点登录SSO友好：因为JWT存储在localStorage中
>
> 缺点：
>
> 退出登录、修改密码等JWT应该失效的场景下，JWT还是有效的（只能等到JWT过期了才会失效）
>
>  => 解决：需要服务端进行额外的处理，例如：
>
> * 将JWT存入Redis中，失效的JWT直接将其删除
> * 修改密钥：每个用户都有一个专属密钥，要让JWT失效，只需要秀嘎用户对应的密钥即可，例如用户修改了密码，对应的密钥也会修改。但是，在分布式服务中，如果修改密钥，还需要同步到每个服务端
> * 让JWT有效期尽可能短，但是用户就得经常重新登录

###### JWT续期问题

JWT过期后，如何实现动态刷新JWT？

> **用户登录返回两个 JWT（推荐）**
>
> 第一个是 `accessJWT` ，它的过期时间为 JWT 本身的过期时间，比如半个小时
>
> 另外一个是 `refreshJWT`， 它的过期时间更长一点，比如为 1 天。`refreshJWT` 只用来获取 `accessJWT`，不容易被泄露
>
> 客户端登录后，将 accessJWT 和 refreshJWT 保存在本地，每次访问将 accessJWT 传给服务端。服务端校验 accessJWT 的有效性，如果过期的话，就将 refreshJWT 传给服务端。如果有效，服务端就生成新的 accessJWT 给客户端。否则，客户端就重新登录即可

##### SSO（Single Sign On，单点登录）

用户只需要登录一次就可以访问所有相互信任的应用系统

<img src="https://oss.javaguide.cn/github/javaguide/system-design/security/sso/sso-system.png-kblb.png" alt="单点登录（SSO）设计" style="zoom:67%;" />

###### 模块及职责

| 应用/模块/对象    | 说明                                |
| ----------------- | ----------------------------------- |
| 前台站点          | 需要登录的站点                      |
| SSO 站点-登录     | 提供登录的页面                      |
| SSO 站点-登出     | 提供注销登录的入口                  |
| SSO 服务-登录     | 提供登录服务                        |
| SSO 服务-登录状态 | 提供登录状态校验/登录信息查询的服务 |
| SSO 服务-登出     | 提供用户注销登录的服务              |
| 数据库            | 存储用户账户信息                    |
| 缓存              | 存储用户的登录信息，通常使用 Redis  |

用户首次登录成功后，生成一个`AuthToken`给客户端存储起来，需要同时记录这个`AuthToken`所属的`domain`(一般一个公司的网站都属于一个根域名下，例如domain=<u>163.com</u>，那么访问所有的<u>*.163.com</u>都可以实现SSO)

用户在浏览其他需要登录的页面时，客户端将该`AuthToken`提交给SSO服务器及逆行校验登录状态

`AuthToken`：UUID即可

###### 时序图

**首次登录：**

![SSO系统设计-登录时序图](https://oss.javaguide.cn/github/javaguide/system-design/security/sso/sso-login-sequence.png-kbrb.png)

**访问其他网站**：查询登录状态

​	查询成功，直接返回要访问的页面

​	查询失败，跳转到登录页面

![SSO系统设计-登录信息获取/登录状态校验](https://oss.javaguide.cn/github/javaguide/system-design/security/sso/sso-logincheck-sequence.png-kbrb.png)

**用户登出：**服务端清空Redis中的登录状态，客户端清除`AuthToken`

![SSO系统设计-用户登出](面经.assets/sso-logout-sequence.png-kbrb.png)



###### 跨域登录

`AuthToken`会绑定domain，那如果是跨域的SSO要怎么解决？

1. 登录完成之后通过回调的方式，将 AuthToken 传递给主域名之外的站点，该站点自行将 AuthToken 保存在当前域下的 Cookie 中
2. 登出完成之后通过回调的方式，调用非主域名站点的登出页面，完成设置 Cookie 中的 AuthToken 过期的操作



> 在主域名登录完成后，访问跨域网站：
>
> ![SSO系统设计-跨域登录（主域名已登录）](https://oss.javaguide.cn/github/javaguide/system-design/security/sso/sso-crossdomain-login-loggedin-sequence.png-kbrb.png)
>
> 主域名未登录时，访问跨域网站：
>
> ![SSO系统设计-跨域登录（主域名未登录）](https://oss.javaguide.cn/github/javaguide/system-design/security/sso/sso-crossdomain-login-unlogin-sequence.png-kbrb.png)
>
> 跨域登出：清除两个域名下的`AuthToken`



##### 加密算法

###### 哈希算法

对任意长度的数据生成一个固定长度的唯一标识，成为哈希值、摘要

①哈希算法的是**不可逆的**，你无法通过哈希之后的值再得到原值

②作用是可以用来验证数据的**完整性和一致性**

**MD5**：Message Diges，消息摘要

> 它可以生成一个 **128** 位（16 字节）的哈希值
>
> 存在弱碰撞（Collision）问题，即多个不同的输入产生相同的 MD5 值
>
> 不推荐使用

**SHA**：Secure Hash Algorithm，安全哈希算法

> SHA-1 生成一个 **160** 位的哈希值，存在安全漏洞，弱碰撞问题，不推荐使用
>
> SHA-256 算法的哈希值长度为 **256** 位，目前还没有找到任何两个不同的数据，它们的 SHA-256 哈希值相同

**Bcrypt**：基于 Blowfish 加密算法的密码哈希算法，专门为密码加密而设计，安全性高

> 由于 Bcrypt 采用了 **salt（盐）** 和 **cost（成本）** 两种机制，它可以有效地防止彩虹表攻击和暴力破解攻击，从而保证密码的安全性。
>
> salt 是一个随机生成的字符串，用于和密码混合，增加密码的复杂度和唯一性
>
> cost 是一个数值参数，用于控制 Bcrypt 算法的迭代次数，增加密码哈希的计算时间和资源消耗
>
> Sprintboot 自带 `BCryptPasswordEncoder`，官方推荐使用

###### 对称加密算法

DES：使用 **64** 位的密钥(有效秘钥长度为 56 位,8 位奇偶校验位) 和 64 位的明文进行加密

> DES **一次只能加密 64 位**，需要把明文划分成 64 位一组的块，就可以实现任意长度明文的加密。
>
> 如果明文长度不是 64 位的倍数，必须进行**填充**

AES：Advanced Encryption Standard

使用 **128 位、192 位或 256 位**的密钥对数据进行加密或解密，密钥越长，安全性越高

> AES 也是一种分组密码，分组长度只能是 **128 位**



###### 非对称加密

非对称加密算法是指加密和解密使用不同的密钥的算法，也叫公开密钥加密算法

这两个密钥互不相同，一个称为**公钥**，另一个称为**私钥**。公钥可以公开给任何人使用，私钥则要保密。

**RSA**：SSL/TLS、SSH 等协议中就用到了 RSA 算法

**DSA**：数字签名速度快，适合生成数字证书；缺点是不能用于数据加密，且签名过程需要随机数





##### 敏感词过滤

###### Trie字典树



###### KMP算法：字符串匹配算法

> 单模式匹配：匹配一个字符串中是否包含某个pattern子串
>
> 关键：构造Next数组，当失配时，跳转到当前位置所指的Next指针处继续匹配



###### ==AC自动机==  [腾讯一面问到了]

> Aho-Corasick（AC）自动机是一种建立在 Trie 树上的一种改进算法，是一种**多模式匹配**算法
>
> AC 自动机算法使用 Trie 树来存放模式串的前缀，通过**失败匹配指针**（失配指针）来处理**匹配失败的跳转**
>
> ​	AC = Trie + Fail指针（KMP）

==场景题==：给一段文本，和一个敏感词列表，怎么快速找出文本中是否含有禁词

1.使用敏感词构造一个字典树Trie

把Trie 的每个节点看作一个**状态**，**每个状态事实上都对应了某个模式串的前缀**

<img src="面经.assets/e3c01ed30231fa4946a1490e2f3093fa.png" alt="img" style="zoom: 33%;" />

2.为Trie树上的所有节点构建**失配指针Fail**

​	节点u的Fail指针指向节点v，其中关系：v是u的最长后缀（即**v所代表的前缀**就是**u所代表的前缀**的**最长后缀**）

​	即，Fail指针就是当前状态(u)的最长后缀状态

> Fail指针和KMP中的next指针：都是表示失配后跳转的位置
>
> Fail指针的推导：BFS递推
>
> - 根节点的Fail指针指向自己，Fail[root] = root
> - 对于节点Node，它的父节点Parent
>   - 如果Fail[Parent]的子节点与节点Node相等，那么Fail[Node]指向这个子节点
>   - 否则，Fail[Node] = root
> - 例子：看She的e节点，它的父节点h的Fail指针指向了her的h节点，并且该节点的子节点为e，和当前节点e相同
>   - 所以She的e节点的Fail指针指向Her的e节点
>   - 即She的最长后缀的he

<img src="面经.assets/d65cb82c89cd76d776e33726a57559ee.png" alt="img" style="zoom: 50%;" />

3.匹配过程：

现在有一个文本ushers ==> she、her

此时指针位于root上，从第一个字符u开始

* u失配，回退到Fail[root] = root的位置
* s：匹配，指针移到s
* h：匹配，指针移到h
* e：匹配，指针移到e（发现e是结束符，所以匹配到了she这个敏感词）
* 指针回退到Fail[e]的位置
* r：匹配，指针移到r（发现r是结束符，所以匹配到了her这个敏感词，这里就体现了Fail指针的用处，不需要重新匹配前缀）
* 指针回退到Fail[r] = root的位置
* s：匹配，指针移到s处
* 到达结尾，但是当前指针所处位置不是结束符

时间复杂度：**O(字符串长度)**

==**工业级优化**==

1.双数组Trie树

2.失败路径压缩

3.内存爆炸：根据业务来划分不同的Trie树，采用LRU缓存淘汰策略

4.分布式AC自动机集群



##### 数据脱敏

###### Hotool

**Hutool**自带的`DesensitizedUtil`，提供各种类型的脱敏接口

![img](https://oss.javaguide.cn/github/javaguide/system-design/security/2023-08-01-10-2119fnVCIDozqHgRGx.png)

```java
DesensitizedUtil.mobilePhone(phone);
DesensitizedUtil.bankCard(bankCard);
```

但是，再调用这些api时，不够统一，需要根据不同的类型来调用不同的api

因此，可以自己定义一个`@Desensitization`注解来优雅的解决各种数据字段的脱敏



第一步：自定义脱敏类型

包括id、身份证、...等等数据类型

```java
/**
	脱敏策略枚举
 */
public enum DesensitizationTypeEnum {
    //自定义
    MY_RULE,
    //用户id
    USER_ID,
    //中文名
    CHINESE_NAME,
    //身份证号
    ID_CARD,
    //座机号
    FIXED_PHONE,
    //手机号
    MOBILE_PHONE,
    //地址
    ADDRESS,
    //电子邮件
    EMAIL,
    //密码
    PASSWORD,
    //银行卡
    BANK_CARD
}
```

第二步：自定义一个@Desensitization注解

```java
@Target(ElementType.FIELD)    // 指定注解的作用域在字段上
@Retention(RetentionPolicy.RUNTIME) // 指定运行时生效
@JacksonAnnotationsInside	// 序列化相关
@JsonSerialize(using = DesensitizationSerialize.class)  // 指定自定义的序列化类
public @interface Desensitization {
    /**
     * 脱敏数据类型，在MY_RULE的时候，startInclude和endExclude生效
     */
    DesensitizationTypeEnum type() default DesensitizationTypeEnum.MY_RULE;

    /**
     * 脱敏开始位置（包含）
     */
    int startInclude() default 0;

    /**
     * 脱敏结束位置（不包含）
     */
    int endExclude() default 0;
}
```

第三步：自定义一个Json序列化类，用来序列化对象的时候将对应加了注解的字段进行脱敏

```java
/**
 * @description: 自定义序列化类
 */
@AllArgsConstructor
@NoArgsConstructor
public class DesensitizationSerialize extends JsonSerializer<String> implements ContextualSerializer {
    private DesensitizationTypeEnum type;

    private Integer startInclude;

    private Integer endExclude;

    @Override
    public void serialize(String str, JsonGenerator jsonGenerator, SerializerProvider serializerProvider) throws IOException {
        switch (type) {
            // 自定义类型脱敏
            case MY_RULE:
                jsonGenerator.writeString(CharSequenceUtil.hide(str, startInclude, endExclude));
                break;
            // userId脱敏
            case USER_ID:
                jsonGenerator.writeString(String.valueOf(DesensitizedUtil.userId()));
                break;
            // 中文姓名脱敏
            case CHINESE_NAME:
                jsonGenerator.writeString(DesensitizedUtil.chineseName(String.valueOf(str)));
                break;
            // 身份证脱敏
            case ID_CARD:
                jsonGenerator.writeString(DesensitizedUtil.idCardNum(String.valueOf(str), 1, 2));
                break;
            // 固定电话脱敏
            case FIXED_PHONE:
                jsonGenerator.writeString(DesensitizedUtil.fixedPhone(String.valueOf(str)));
                break;
            // 手机号脱敏
            case MOBILE_PHONE:
                jsonGenerator.writeString(DesensitizedUtil.mobilePhone(String.valueOf(str)));
                break;
            // 地址脱敏
            case ADDRESS:
                jsonGenerator.writeString(DesensitizedUtil.address(String.valueOf(str), 8));
                break;
            // 邮箱脱敏
            case EMAIL:
                jsonGenerator.writeString(DesensitizedUtil.email(String.valueOf(str)));
                break;
            // 密码脱敏
            case PASSWORD:
                jsonGenerator.writeString(DesensitizedUtil.password(String.valueOf(str)));
                break;
            // 中国车牌脱敏
            case CAR_LICENSE:
                jsonGenerator.writeString(DesensitizedUtil.carLicense(String.valueOf(str)));
                break;
            // 银行卡脱敏
            case BANK_CARD:
                jsonGenerator.writeString(DesensitizedUtil.bankCard(String.valueOf(str)));
                break;
            default:
        }
    }
}
```

第四步：使用注解，在pojo类上需要脱敏的字段使用`@Desensitization`注解

```java
@Data
@NoArgsConstructor
@AllArgsConstructor
public class TestPojo {
	// 用户名不需要脱敏
    private String userName;
	// 手机号需要脱敏：指定脱敏类型位MOBILE_PHONE
    @Desensitization(type = DesensitizationTypeEnum.MOBILE_PHONE)
    private String phone;
	// 密码需要脱敏
    @Desensitization(type = DesensitizationTypeEnum.PASSWORD)
    private String password;
	// 地址需要脱敏
    @Desensitization(type = DesensitizationTypeEnum.MY_RULE, startInclude = 0, endExclude = 2)
    private String address;
}
```



###### ShardingJDBC

> ShardingSphere 下面存在一个数据脱敏模块，此模块集成的常用的数据脱敏的功能
>
> 其基本原理**是对用户输入的 SQL 进行解析拦截**，并依靠用户的**脱敏配置**进行 SQL 的改写，从而实现对原文字段的加密及加密字段的解密。
>
> 最终实现对用户无感的加解密存储、查询。



###### Mabatis-Flex

MyBatis-Flex 提供了 `@ColumnMask()` 注解，以及内置的 9 种脱敏规则 `Masks`，开箱即用：

```java
@Table("tb_account")
public class Account {

    @Id(keyType = KeyType.Auto)
    private Long id;

    @ColumnMask(Masks.CHINESE_NAME)
    private String userName;

    @ColumnMask(Masks.EMAIL)
    private String email;

}
```





------

## 项目拷打

基于简历中写道的技术，要熟练掌握，面试官会根据项目来深挖知识点



### 关于数据库

##### 优化慢查询的步骤和例子

> ①开启sql慢查询日志（默认关闭），找到执行时间长于`long_query_time`的sql语句，发现慢查询语句是优化sql的第一步
>
> 使用`mysqldumpslow` 可以查看到慢查询sql以及它们具体的执行时间、扫描过的行数等等信息
>
> ②使用`EXPLAIN`命令，获取慢查询的执行计划
>
> ​	发现该sql的key=null，type='ALL'，即全表扫描  Extra=‘Using where' 即没有使用索引
>
> ③解决：加上联合索引



##### 对文书基表进行垂直分表

> 因为文书基表中有一个`文书内容`字段，它是一个`BLOB`类型，存储的是文书的二进制流
>
> ①大字段导致**索引页**膨胀，影响数据库的缓存命中率，降低查询性能
>
> ②在查询中因为包含BLOB字段，就不能使用**内存临时表**，必须使用**磁盘临时表**进行，效率肯定降低。
>
> ③而且对于BLOB类型，读取一行数据时，**普通字段的值可以直接从索引页中取出**，而 BLOB 需要**额外的 I/O 操作**去访问，即BLOB 页MySQL还要进行**二次查询**，使性能变得很差
>
> 面试官反问：那我查询的时候不查这个字段不就行了？用的使用再查？
>
> ④**每次查询时即使没有select这个字段，也会影响性能**。因为行数据的读取、缓存管理、甚至行内存储方式都会受影响，导致查询变慢。比如每次读一页数据，里面有16行记录，如果把BLOB字段去了，那可以多读很多行出来，减少页的替换
>
> 解决：
>
> ①从sql上优化，避免 SELECT * 查询，尽量只查询所需字段
>
> ②进行**垂直分表**，把BLOB列分离到单独的扩展表中

##### 为什么不考虑对象存储服务？

> BLOB 数据存储到对象存储（如 MinIO、OSS、S3），数据库中只存文件 URL
>
> 有考虑，但是建于法院是纯内网开发的，所以暂时没有考虑这个
>
> 数据迁移用到了阿里的OSS

##### 有没有分表分库？

> 有的，有垂直分表 如上
>
> 也有水平分库和分表：将数据库**根据法院**分散开来，每次查询根据案件所属法院去获取文书
>
> 结合动态数据源切换来回答：当需要获取某个法院的数据时，动态切换数据源
>
> 使用Sharding-JDBC来路由sql请求

###### 分表分库后，如果使用order by等操作怎么办？

> 数据分散在不同表中，需要进行全局排序
>
> **全路由查询**：将 SQL 路由到所有相关分片上进行查询
>
> **各个分片本地排序**：每个数据库单独执行 `ORDER BY`
>
> **结果汇总**：ShardingSphere 把所有分片返回的部分结果收集起来
>
> **内存归并排序**：在 Proxy 或 JDBC 层做一次全局排序





##### 有没有实现读写分离？

> 读写分离：每个法院的数据库有两个主从备份，一个Master、两个Slave，用来实现读写分离
>
> **主库（Master）**：负责 **写操作（INSERT、UPDATE、DELETE）**，也可以处理读请求
>
> **从库（Slave）**：负责 **读操作（SELECT）**，并通过 **主从复制** 同步主库数据
>
> ###### 具体实现
>
> 1.springboot项目：动态数据源切换
>
> 2.**MySQL proxy**：由Mysql代理来实现请求转发 （单点故障，还需要额外的部署）
>
> 例如阿里巴巴的**Mycat**中间件
>
> <img src="https://cdn.tobebetterjavaer.com/stutymore/mysql-20241207121845.png" alt="piwenfei：mycat" style="zoom:50%;" />
>
> 3.**sharding-jdbc**：无需修改业务代码即可自动路由 读（从库） 和 写（主库） 操作
>
> 配置文件：
>
> ```yaml
> spring:
> shardingsphere:
>  datasource:
>    names: master,slave1,slave2
>    master:
>      type: com.zaxxer.hikari.HikariDataSource
>      driver-class-name: com.mysql.cj.jdbc.Driver
>      url: jdbc:mysql://master_host:3306/mydb?useSSL=false
>      username: root
>      password: root
>    slave1:
>      type: com.zaxxer.hikari.HikariDataSource
>      driver-class-name: com.mysql.cj.jdbc.Driver
>      url: jdbc:mysql://slave1_host:3306/mydb?useSSL=false
>      username: root
>      password: root
>    slave2:
>      type: com.zaxxer.hikari.HikariDataSource
>      driver-class-name: com.mysql.cj.jdbc.Driver
>      url: jdbc:mysql://slave2_host:3306/mydb?useSSL=false
>      username: root
>      password: root
> 
> 	# 路由规则
>  rules:
>    readwrite-splitting:
>      data-sources:
>        mydb_readwrite:
>          type: Static
>          props:
>            write-data-source-name: master
>            read-data-source-names: slave1,slave2
>            load-balancer-name: round_robin
>      load-balancers:
>        round_robin:
>          type: ROUND_ROBIN
> 
>  props:
>    sql-show: true # 打印 SQL，方便调试
> 
> ```
>
> 只要配置好，就可以自动做到读写分离，不需要修改业务代码，无单点故障
>
> ​	**写操作（INSERT、UPDATE、DELETE）** → 自动路由到 **主库（Master）**
>
> ​	**读操作（SELECT）** → 自动路由到 **从库（Slave）**，并根据**负载均衡策略**分配
>
> 
>
> ###### 读写分离问题：主从数据库一致的问题
>
> ①**事务内强制走主库**： 在 Sharding-JDBC 中使用 `HintManager` 强制走主库
>
> ​	就是一个事务中执行更新操作时，会自动路由到主库，之后再读取数据，会自动路由到从库，这里就可能发生数据不一致的问题，因为再读写操作之间从数据库不一定来得及同步。
>
> ​	所以这里可以使用HintManager来强制使用主库读数据，这样一个事务内的数据一致性可以得到保证
>
> ```java
> userService.insertUser(new User("Alice"));  // 插入主库
> 
> try (HintManager hintManager = HintManager.getInstance()) {
>  hintManager.setReadWriteSplittingAuto();  // 强制走主库
>  User user = userService.getUserByName("Alice");
>  System.out.println(user);  // ✅ 一定能查到
> }
> ```
>
> 
>
> ②**延迟读取**：高并发场景下，确保从库同步完成后再读
>
> ​	使用 Canal 监听 主库的 binlog是否已经同步到从库，如果同步了走从库，如果还没同步，走主库
>
> ```java
> // 判断binlog是否已经同步
> if (binlogService.isDataSynced("Alice")) {
>      userService.getUserByName("Alice");  // 已同步，走从库
>      // 也可以轮询等待同步后再去从库读
> } else {
>      // 为同步，使用HintManager强制走主库
>      try (HintManager hintManager = HintManager.getInstance()) {
>            hintManager.setReadWriteSplittingAuto();
>            userService.getUserByName("Alice");  // 强制走主库
>      }
> }
> ```
>
> 



##### 从哪方面来优化Mysql的性能？

> 1.分库分表
>
> 2.读写分离
>
> 3.索引的利用
>
> 4.Redis缓存



##### Redis缓存热点文书，减少数据库的压力

> 近几年的文书一般会是法官们想看的文书，可以适当进行热点文书的缓存
>
> 在海量文书检索系统中，对访问量高的文书进行redis缓存
>
> ①维护一个SortedSet来存放当天每篇文书的访问次数
>
> ②访问一篇文书时，更新其访问次数，并判断是否大于阈值（10），是的话就缓存这篇热点文书
>
> 在更新热点文书时，先更新数据库，再删除对应的缓存。
>
> 下次访问这篇文书，同样会触发判断访问次数是否大于阈值的逻辑，会将热点文书缓存进去



##### Redis缓存的热点文书是如何判定的？

> 我们使用Redis缓存主要包括以下几类数据：
>
> - **热点文书数据**：读取量高的文书ID及其元数据（标题、发布日期、文书内容等），以Hash结构存储
> - **用户搜索历史**：用SortedSet保存每个用户的搜索关键词（score为时间戳）
> - **热搜关键词**：对所有用户关键词进行ZSet累加评分，定时取Top N展示
>
> **热点文书判定规则**：
>
> - 每次用户点击文书详情时，我们会将该文书ID在Redis中`ZINCRBY`一次，如果访问次数到达阈值，就可以缓存起来。过期时间设置为**半个月**，根据一个案件的**平均审限**来设定



##### 大文件下载

> response.setContentType("application/octet-stream")





### 关于动态数据源切换

“动态数据源管理：利用Dynamic Datasource支持多数据源切换，提升系统的灵活性，适应不同数据存储需求”

##### 怎么用

两种方式：`DynamicDataSource` 或者 `@DS`

详情见博客：[动态数据源实现方式](https://blog.csdn.net/tanksyg/article/details/84849189?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-84849189-blog-138067201.235%5Ev43%5Econtrol&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-84849189-blog-138067201.235%5Ev43%5Econtrol&utm_relevant_index=2)

> spring-dynamic-datasource
>
> 在application.yaml中
>
> ```yaml
> spring:
> 	datasource:
> 		dynamic:
>             primary: tjzm #默认数据源
>             strict: true
>             datasource:
>                 tjzm:  #数据源1
>                     url: 
>                     driver-class-name:
>                     username:
>                     password:
>                 tjbh:  #数据源2
>                     url: 
>                     driver-class-name:
>                     username:
>                     password:
> ```
>
> `@DS("tjzm")`注解，加在Service或者Mapper的类或方法上
>
> 在**方法内**如果要切换数据源，可以使用`DynamicDataSourceContextHolder.push(ds)`来切换，`poll()`退出
>
> `DynamicDataSourceContextHolder`类中维护了一个`ThreadLocal<Deque<String>> LOOKUP_KEY_HOLDER`，存放了只有**当前线程可见的**数据源队列
>
> 如果@DS注解要动态解析数据源，可以自定义一个`DsProcessor`，重写`matches()`和`doDetermineDatasource()`
>
> ```java
> 
> @Configuration
> public class DatasourceConfig {
> 
>     // 创建一个自定义的DsProcessor
>     @Bean
>     @ConditionalOnMissingBean
>     public DsProcessor dsProcessor() {
>         MyDsProcessor myDsProcessor = new MyDsProcessor();
>         DsSessionProcessor sessionProcessor = new DsSessionProcessor();
>         DsSpelExpressionProcessor spelExpressionProcessor = new DsSpelExpressionProcessor();
>         myDsProcessor.setNextProcessor(sessionProcessor);
>         sessionProcessor.setNextProcessor(spelExpressionProcessor);
>         return myDsProcessor;
>     }
> }
> 
> 
> public class MyDsProcessor extends DsProcessor {
> 
>     private static final String HEADER_PREFIX_DS = "#ds";
> 
>     // 重写matches函数，如果@DS("#ds")，那么命中
>     @Override
>     public boolean matches(String key) {
>         return key.startsWith(HEADER_PREFIX_DS);
>     }
> 
>     // 解析#ds，并返回数据源名字
>     @Override
>     public String doDetermineDatasource(MethodInvocation invocation, String key) {
>         // 获取当前函数调用的参数
>         Object[] args = invocation.getArguments();
>         // 获取函数最后一个参数ds
>         String ds = args[args.length - 1].toString();
>      	// 返回ds
>         return ds == null ? "default" : ds;
>     }
> }
> ```
>
> 
>
> **DS原理**：根据 AOP 切面、注解、ThreadLocal 
>
> @DS注解的封装，可以在方法或类上直接指定数据源，而无需修改代码。这使得切换数据源变得更加简单和直接

##### 弊端

> 只支持同个数据源下的事务，不支持跨数据源事务（本身也没有这个业务）
>
> 分布式事务来解决







### 关于RabbitMQ :star:

##### 什么是RabbitMQ？

> RabbitMQ基于**AMQP（高级消息队列协议）**，使用**Erlang编写**的消息队列，用于系统中各模块之间的高效通讯
>
> ①支持高并发 ②支持可扩展 ③支持多种客户端 ④支持持久化

其中，AMQP是什么？

> AMQP（即 Advanced Message Queuing Protocol），一个提供统一消息服务的应用层标准 **高级消息队列协议**（二进制应用层协议），是应用层协议的一个开放标准，为面向消息的中间件设计，兼容 JMS。基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件同产品，不同的开发语言等条件的限制。
>
>
> **AMQP协议的三层**：
>
> - **Module Layer**: 协议最高层，主要定义了一些客户端调用的命令，客户端可以用这些命令实现自己的业务逻辑
> - **Session Layer**:  中间层，主要负责客户端命令发送给服务器，再将服务端应答返回客户端，提供可靠性同步机制和错误处理
> - **TransportLayer**: 最底层，主要传输**二进制数据流**，提供帧的处理、信道复用、错误检测和数据表示等
>
> AMQP模型的**三大组件**：
>
> * **交换器 (Exchange)**：消息代理服务器中用于把消息路由到队列的组件。
>
> * **队列 (Queue)**：用来存储消息的数据结构，位于硬盘或内存中。
>
> * **绑定 (Binding)**：一套规则，告知交换器消息应该将消息投递给哪个队列。
>
>   



##### 为什么用RabbitMQ？优势？劣势？

> **特点**：
>
> - **可靠性**: 如支持<u>持久化</u>、传输确认及发布确认等
> - **灵活的路由** : 在消息进入队列之前，通过交换器来路由消息。对于典型的路由功能， RabbitMQ 己经提供了一些内置的交换器来实现。针对更复杂的路由功能，可以将多个交换器绑定在一起， 也可以通过插件机制来实现自己的交换器
> - **扩展性**: 多个 RabbitMQ 节点可以组成一个<u>集群</u>，也可以根据实际业务情况<u>动态地扩展集群</u>中节点
> - **高可用性** : 队列可以在集群中的机器上设置<u>镜像</u>，使得在部分节点出现问题的情况下队列仍然可用
> - **多种协议**: RabbitMQ 除了原生支持 AMQP 协议，还支持STOMP， MQTT等<u>多种消息中间件协议</u>
> - **多语言客户端** :RabbitMQ 几乎支持所有常用语言，比如 Java、 Python、 Ruby、 PHP、 C#、 JavaScript 等
> - **管理界面** : RabbitMQ 提供了一个<u>易用的用户界面</u>，使得用户可以监控和管理消息、集群中的节点等
> - **插件机制** : RabbitMQ 提供了许多插件，以实现从多方面进行扩展，当然也可以编写自己的插件

###### 我的项目使用MQ做什么？

简历上写的：“<u>自定义线程池的拒绝策略，使用RabbitMQ进行任务持久化，避免任务丢失以及阻塞主线程</u>”

> 1.因为我用了**自定义线程池**，并自定义线程池满的时候将任务添加到消息队列中
>
> 使用ThreadPoolTaskExecutor配置线程池，自定义 **拒绝策略（RejectedExecutionHandler）**，将任务加入到消息队列中
>
> 默认的拒绝策略为AbortPolicy（满了抛出异常），也有CallerRunsPolicy（调用者线程来执行这个任务）、DiscardPolicy（直接丢弃）、DiscardOldestPolicy（丢弃队列中最早的）
>
> ```java
> @Configuration
> public class ThreadPoolConfig {
> 
>  @Bean
>  public ThreadPoolTaskExecutor taskExecutor() {
>      ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();	
>      // 配置其他
> 
>      // 配置自定义拒绝策略
>      executor.setRejectedExecutionHandler(new RejectedExecutionHandler() {
>          @Override
>          public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) {
>              // 自定义操作，比如将任务推送到 RabbitMQ 或者日志记录
>              System.out.println("Task rejected: " + r);
>          }
>      });
>  }
> }
> ```
>
> Q：什么时候线程池才算满了？
>
> ​	如果**当前同时运行的线程数量**达到**最大线程数量**，并且**队列也已经被放满了**任务时
>
> ​	`corePoolSize` : 核心线程数，任务队列未达到队列容量时，最大可以同时运行的线程数量
>
> ​	`maximumPoolSize` : 任务队列中放满时，当前可以同时运行的线程数量变为最大线程数
>
> ​	`workQueue`: 新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，新任务就会被存放在队列中
>
> <img src="https://oss.javaguide.cn/github/javaguide/java/concurrent/relationship-between-thread-pool-parameters.png" alt="线程池各个参数的关系" style="zoom: 50%;" />

> 2.RabbitMQ提供了一个<u>易用的用户界面</u>，使得用户可以监控和管理消息、集群中的节点等。



##### RabbitMQ架构

<img src="https://oss.javaguide.cn/github/javaguide/rabbitmq/96388546.jpg" alt="图1-RabbitMQ 的整体模型架构"  />

> **Producer**：生产者
>
> **Consumer**：消费者
>
> **Message**：消息
>
> ​	消息分为两部分：**消息头**（Label）和**消息体**（Payload）
>
> ​	消息头由一系列可选属性组成：
>
> ​		`contentType`：消息格式
>
> ​		`routing-key`：路由键（用来将消息路由到指定的queue）:star:
>
> ​		`priority`：消息优先级
>
> ​		`delivery-mode`：持久化标志，1=非持久化 2=持久化	
>
> ​		`expiration`：消息过期时间TTL（ms）
>
> ​		`headers`：用户自定义头部
>
> ​	消息体存放消息的内容，可以是 **JSON、XML、字符串、二进制数据、对象** 
>
> **Exchange**：交换器:star:
>
> ​	消息会经过交换器来路由到对应的Queue中，如果路由不到，丢弃或返回给生产者
>
> ​	四种类型：
>
> ​		`direct`：直连交换机，根据`rounting-key`**精确匹配**队列
>
> ​		`fanout`：扇出交换机，不看路由键，直接**广播所有**绑定的队列
>
> ​		`topic`：主题交换机，根据路由键**模糊匹配**，支持*、#等符号
>
> ​		`headers`：头交换机，不看路由键，看`headers`
>
> ​	具体的路由方式：依赖`BindingKey`（绑定键），每个Queue与Exchange之间的连接都有一个或多个绑定键，这个键可以重复，用来跟路由键匹配，匹配成功的就可以进行路由
>
> <img src="https://oss.javaguide.cn/github/javaguide/rabbitmq/70553134.jpg" alt="Binding(绑定) 示意图" style="zoom: 80%;" />
>
> **Queue**：消息队列
>
> ​	存储消息的地方，多个消费者可以订阅同一个队列，此时队列中的消息会被**平均分摊（轮询）**到多个消费者中进行消费（避免**重复消费**）
>
> 
>
> **Broker**：消息中间件的服务节点（类似Kafka的Broker，代理）
>
> ​	包裹着Exchange和Queue，直接同Producer和Consumer交互
>
> <img src="https://oss.javaguide.cn/github/javaguide/rabbitmq/67952922.jpg" alt="消息队列的运转过程" style="zoom:67%;" />

细讲Exchange的四大类型：

**Fanout Exchange**：将消息广播给所有与之绑定的队列

**Direct Exchange**：拿RoutingKey和BindingKey精确匹配，匹配上的就路由

![direct 类型交换器](https://oss.javaguide.cn/github/javaguide/rabbitmq/37008021.jpg)

​	如果路由键为warning，那么会路由到Q1和Q2

​	如果路由键为info，那么会路由到Q2

**Topic Exchange**：RoutingKey模糊匹配BindingKey

​	其中RoutingKey由`.`号分割开，如`java.util.concurrent`，

​	BindingKey也是由`.`号分割，每个单词可以换成`*`或者`#`，`*` 号用于匹配一个单词，`#`号用于匹配零个或多个单词，如`java.util.*`

​	如RoutingKey为`com.rabbitmq.client`，会路由到Q1和Q2

![topic 类型交换器](https://oss.javaguide.cn/github/javaguide/rabbitmq/73843.jpg)	

**Headers Exchange**：不推荐

​		Exchange和Queue之间不使用BindingKey，而是使用一组**键值对**

​		消息头的**headers**中也指定了键值对，路由给完全匹配的队列

​		性能差



##### 根据Exchange的不同，RabbitMQ有5种工作模式

| **模式**                          | **交换机类型**  | **消息路由规则**                           | **适用场景**               |
| --------------------------------- | --------------- | ------------------------------------------ | -------------------------- |
| **1. 简单模式（Simple）**         | 不使用交换机    | 一个生产者对接一个队列，一个消费者消费     | 基础消息队列               |
| **2. 工作队列模式（Work Queue）** | 不使用交换机    | 一个生产者对接一个队列，多个消费者分摊消息 | 任务分发（负载均衡）       |
| **3. 发布订阅模式（Fanout）**     | `fanout` 交换机 | 消息广播到所有绑定的队列                   | 系统通知、日志广播         |
| **4. 路由模式（Direct）**         | `direct` 交换机 | 按 `Routing Key` 精确匹配队列              | 日志级别、订单状态更新     |
| **5. 主题模式（Topic）**          | `topic` 交换机  | `Routing Key` 支持模糊匹配（`*`、`#`）     | 复杂消息路由（如日志分类） |



##### 如果消息经过Exchange没有找到具体要路由的Queue怎么办？

> 1.默认行为（消息丢弃）
>
> 2.使用死信队列来存放
>
> 3.设置消息TTL，过期之前会暂存在Exchange中，直到有队列匹配，过期后丢弃
>
> 4.生产者可以设置消息头中的 `mandatory` 标志位，表示找不到会返回给生产者



##### 什么是死信和死信队列？

> 当一个消息变成**死信**，会被重新发送给**死信交换器（DLX）**，它绑定的消息队列为**死信队列**
> 死信产生的原因：
>
> ①消息被拒
>
> ②消息过期
>
> ③队列满了
>
> ④消息一直消费失败，且达到了最大重试次数



##### 什么是延迟队列？RabbitMQ怎么实现延迟队列？

> **延迟队列**指的是存储对应的延迟消息，消息被发送以后，并<u>不想让消费者立刻拿到消息</u>，而是<u>等待特定时间</u>后，消费者才能拿到这个消息进行消费
>
> 
>
> 实现：**RabbitMQ本身没有延迟队列**
>
> ①使用 RabbitMQ 的**死信交换机** + 设置消息的存活时间 **TTL**
>
> ​	消费者监听死信队列来处理延期任务
>
> ②在 RabbitMQ 3.5.7 及以上的版本提供了一个插件（rabbitmq-**delayed-message**-exchange），直接设置消息的延期时间



##### 怎么处理消息的优先级？优先级队列

> RabbitMQ 自 V3.5.0 有优先级队列实现，优先级高的队列会先被消费
>
> 通过 `x-max-priority` 参数来实现优先级队列



##### 如何确保消息不丢失？

三种可能丢失的情况：生产者到MQ的过程中丢失，MQ自己搞丢，MQ到消费者的过程中丢失

> **生产者到MQ**：
>
> ​	①开启Confirm模式：生产者发送消息后等待MQ确认，确保MQ确认收到消息（同步/异步）
>
> ​	②事务机制
>
> **MQ自身**：
>
> ​	①持久化：消息持久化和队列持久化同时开启
>
> ​	②集群 —— 镜像模式（高可用）：消息自动复制到多个节点，每个队列有多个镜像副本分布在不同节点上，主队列崩溃后又镜像队列自动升级为主队列
>
> **MQ到消费者**：
>
> ​	①关闭自动ACK，改为手动ACK
>
> ​	②使用死信队列，防止消息意外丢失



##### 如何实现消息过滤？

> 可以在**Broker端**过滤，也可以在**Consumer端**过滤
>
> Broker端过滤：在 Broker 端按照 Consumer 的去重逻辑进行过滤，防止无用消息传入消费者端，但是加重了Broker压力
>
> ​	在RabbitMQ中，Broker根据路由键进行消息转发，如果没有匹配的绑定键，也可以直接被过滤掉
>
> Consumer端过滤：（推荐）
>
> * 根据Tag过滤
> * SQL 表达式过滤：SQL 表达式过滤更加灵活
> * Filter Server 方式：用户自定义过滤函数（最灵活）
> * 去重表
>
> 



##### RabbitMQ集群

> ①**单机模式**
>
> ②**普通模式**
>
> ​	多台服务器分别启动一个RMQ节点，成为一个集群
>
> ​	每个Queue只会放在一个RMQ节点上，但是每个Queue的**元数据**（一些配置信息等）会复制同步到每个RMQ节点上。
>
> ​	即：**每个节点都知道某条消息存在哪里**，但是每条**消息只存储一遍**，就在它所在的队列中。如果消费者向节点A获取消息M，但是消息M在节点B上，此时节点A通过Queue元数据知道了消息M在节点B上，所以就向节点B请求消息M，再返回给消费者（实现了高吞吐量，让多个节点来服务某个Queue读写）
>
> ③**镜像模式**
>
> ​	每个Queue都会在每个RMQ节点上有一个镜像，即每条消息都存储在所有节点上。每次写消息，先写入主队列，再自动同步到其他镜像队列上
>
> 



##### RabbitMQ中大量消息积压怎么处理？

消费者消费速度远跟不上生产者

> ①消费者扩容
>
> ②队列扩容：新建一个临时的 Topic，多设置一些队列，然后先用一些消费者把消费的数据丢到临时的 Topic中（转发）
>
> ③让消息先持久化，等资源空闲了再恢复处理
>
> ④设置死信队列 + 持久化
>
> ⑤**多线程消费**

###### 如果你的发送通知的消息队列中同时挤压了太多消息，怎么办？

> 回答了首先持久化，防止消息队列奔溃了导致消息丢失
>
> 其次增加消费者的数量，将通知服务部署到多个服务器上，服务的水平扩容
>
> 追问：如果不能增加消费者呢，就这一个消费者
>
> 回答了搭建RabbitMQ集群
>
> 追问：没有了解过消息队列中消费者可以设置多线程吗？
>
> RabbitMQ 的消费者默认是**单线程**，可以通过在消费者内部**使用多线程**来并行处理消息，从而提升处理速度

###### 为什么要用消息队列去通知，而不是直接调用接口通知？

> 可靠性：防止接口调用失败导致通知丢失，消息队列可以保证消息不丢失
>
> 解耦：让任务执行系统与通知服务系统解耦
>
> 支持多种通知方式，可以设置延迟重新推送等机制



##### 怎么使用幂等操作？怎么保证消息消费的幂等性？

为了保证消息不会重复消费，可以考虑消费的**幂等性**，或者对消息进行**去重**

消费**幂等性**，即 **同一条消息被消费多次** 时，结果不会发生不同的变化

**消息去重**，是指在消费者消费消息之前，先检查一下是否已经消费过这条消息，如果消费过了，就不再消费

> **唯一业务ID+去重表**：使用Redis来去重，防止消息的重复消费
>
> **乐观锁**：在数据库表中使用**版本号**，通过**乐观锁**机制来保证幂等性。每次更新操作时检查版本号是否一致，只有一致时才执行更新并递增版本号。如果版本号不一致，则说明操作已被执行过，拒绝重复操作。
>
> **分布式锁**：Redis/Kafka
>
> **Token机制**：服务端为每一个操作生成一次性Token，每次用户提交任务后该token就会从Redis缓存中删除，防止重复提交
>
> **使用幂等操作**：
>
> ​	当操作本身就是幂等的，系统设计可以更为简洁，不需要额外的机制去检测和防止重复执行相同的操作
>
> ​	例如Redis的SETNX操作，就是幂等的，多次操作可以防止重复执行



##### 为什么不用Kafka？RabbitMQ vs. Kafka

> 1.Kafka不支持优先级、RabbitMQ支持优先级（消息头中可以设置优先级）
>
> 2.应用场景上：Kafka用于日志采集、流式数据处理；
>
> ​						RabbitMQ用于实时的，对可靠性要求较高的消息传递上
>
> 3.消息推送上：RabbitMQ采用push的方式，当消息到达队列后，会将消息推到消费者端
>
> ​						而Kafka只有pull模式，需要消费者自行拉去
>
> 4.消费失败：RabbitMQ在消息出问题或者消费错误的时候，可以重新入队或者移动消息到死信队列，继续消费后面的；
>
> ​			Kafka当单个分区中的消息一旦出现消费失败，就只能停止而不是跳过这条失败的消息继续消费后面的消息

吞吐量低：RabbitMQ和Kafka都可以

吞吐量高：Kafka



##### 如果RabbitMQ挂了怎么办？

> 首先，我的项目中创建任务后是会进行消息持久化的，就是在Mysql数据库中存储任务的信息，所以如果MQ挂了可以恢复
>
> 其次，可以考虑RabbitMQ集群，来提高RabbitMQ的高可用



##### 如何保证消息消费有序

> ①使用单一队列可以保证消费有序，不同队列之间的消息顺序无法保证！
>
> ②路由，同一个用户的任务（如订单）应该有序进行，那么将同一个用户的任务路由到同一个队列中去



### 关于WebSocket

“实时任务进度展示：借助WebSocket技术，实时推送任务进度，前端可动态查看任务的执行状态，在任务执行完成后向用户发送通知，确保用户能及时获取结果”

问题：任务完成后，需要客户端手动刷新页面才能显示结束状态。如何在任务运行结束后前端自动刷新状态？

因为Http请求是单向的，即客户端到服务端。使用WebSocket可以建立起客户端和服务端的双向通讯，允许服务端向客户端发送数据。



> ##### WebSocket 的工作过程是什么样的？
>
> WebSocket 的工作过程可以分为以下几个步骤：
>
> 1. 客户端向服务器发送一个 HTTP 请求，请求头中包含 `Upgrade: websocket` 和 `Sec-WebSocket-Key` 等字段，表示要求**升级协议为 WebSocket**
> 2. 服务器收到这个请求后，会进行升级协议的操作，如果支持 WebSocket，它将回复一个 HTTP `101` 状态码，响应头中包含 ，`Connection: Upgrade`和 `Sec-WebSocket-Accept: xxx` 等字段、表示成功升级到 WebSocket 协议
> 3. 客户端和服务器之间建立了一个 WebSocket 连接，可以进行双向的数据传输。数据以**帧**（frames）的形式进行传送，WebSocket 的每条消息可能会被切分成多个数据帧（最小单位）。发送端会将消息切割成多个帧发送给接收端，接收端接收消息帧，并将关联的帧重新组装成完整的消息
> 4. 客户端或服务器可以主动发送一个关闭帧，表示要断开连接。另一方收到后，也会回复一个关闭帧，然后双方关闭 TCP 连接
>
> 另外，建立 WebSocket 连接之后，通过**心跳机制**来保持 WebSocket 连接的稳定性和活跃性。



#### 实践

##### 服务端

通过**WebSocketConfig**配置类，创建一个**ServerEndpointExporter**，接着创建一个**WebSocketServer**来维护客户端和服务端的websocket连接。

```java
@Component
@ServerEndpoint("/websocket/{sid}")
public class WebSocketServer {

    /**
     * 记录当前的在线连接数
     * AtomicInteger 线程安全类
     */
    private static AtomicInteger onlineSessionClientCount = new AtomicInteger(0);

    /**
     * 存放所有在线的客户端
     * ConcurrentHashMap 线程安全
     */
    private static final Map<String, Session> onlineSessionClientMap = new ConcurrentHashMap<>();
}
```

需要实现四个方法，分别处理前端的请求

```java
@OnOpen
public void onOpen(@PathParam("sid") String sid, Session session);

@OnClose
public void onClose(@PathParam("sid") String sid, Session session);

@OnMessage
public void onMessage(String message, Session session);

@OnError
public void onError(Session session, Throwable error)
```

onOpen：连接建立成功调用的方法。在该方法中可以维护onlineSessionClientMap，存储当前连接

onClose：连接关闭调用的方法。同样维护onlineSessionClientMap

onError：发生错误调用的方法。用于打印错误信息

onMessage：收到客户端消息后调用的方法。

还需要实现一个**sendMessage**方法来向客户端发送信息，通过sid来获取Session，在通过**session.getAsyncRemote().sendText(message)**来发送信息



##### 客户端

客户端通过websocket.js来创建Socket对象，并且绑定事件

```js
socketUrl = 'http://localhost:8085/websocket/' 
sid = fydm + "_" + username
// 创建连接
Socket = new WebSocket(socketUrl + sid)
// 绑定四个事件的处理方式
Socket.onopen = onopenWS
Socket.onmessage = onmessageWS
Socket.onerror = onerrorWS
Socket.onclose = oncloseWS
```

创建Socket的时机在login之后创建

断开Socket的时机在logout之后断开

在任务管理页面，绑定一个处理服务端发送消息的事件，用来处理服务端发送的通知。

```js
// 注册监听任务运行结束通知
window.addEventListener('onmessageWS', this.handleNotification)
// 处理服务端通知
handleNotification(e) {
    // 创建接收消息函数
    let data = e && e.detail.data
    if(data){
        // 任务运行结束
        let message = JSON.parse(data)
        // console.log(message)
        let title = message.title
        let taskId = message.taskId

        if(title === '任务完成通知') {
            this.$message.success("任务" + taskId + "运行结束")
            // 设置任务完成状态和完成时间
            this.taskList = this.taskList.map(item => {
                if (item.taskId === taskId) {
                    item.finishTime = message.finishTime
                    item.state = message.state
                }
                return item
            })
        }
    }
```

在runTask结束是，通过sendMessage来发送通知。前端收到通知后更新任务状态。

因为这里有多种类型的通知，包括任务完成和任务进度，所以定义一个**WebSocketMessage**来封装消息

```java
public class WebSocketMessage {
    // 标题
    String title;
    // 消息
    String msg;
    // 通知的任务信息
    Long taskId;
    String finishTime;
    String state;
    Double progress;
}
```

#### 如何做到进度通知

在任务运行的过程中，使用Redis来存储进度。Redis的key为sid+taskId，即fydm+username+taskid，可以做到唯一。只存的是progress

##### 方法一：前端轮询

前端通过轮询来从后端获取任务的进度

分为**短轮询**和**长轮询**

短轮询：客户端每次发送请求去查询结果，有没有结果都会返回

长轮询：客户端每次发送请求去查询结果，如果有结果就返回，没有结果服务端会hold住请求，直到超时或者有结果

（Nacos使用的就是长轮询）

![长轮询示意图](https://oss.javaguide.cn/github/javaguide/system-design/web-real-time-message-push/1460000042192386.png)

##### 方法二：后端发送通知

服务端推送

分为**Websocket** 和 **SSE**





#### 有没有了解过SSE？

SSE：Server-Sent Events，服务器发送事件

是一种**服务器端到客户端**的**单向**消息推送

服务端响应的不再是**一次性的数据包**而是`text/event-stream`类型的**数据流**信息，在有数据变更时从服务器流式传输到客户端

###### 实践

前端只需进行一次 HTTP 请求，带上唯一 ID，打开事件流，监听服务端推送的事件就可以了

```js
let source = null;
let userId = 7777
if (window.EventSource) {
    // 建立连接
    source = new EventSource('http://localhost:7777/sse/sub/' + userId);
    setMessageInnerHTML("连接用户=" + userId);
    /**
         * 连接一旦建立，就会触发open事件
         * 另一种写法：source.onopen = function (event) {}
         */
    source.addEventListener('open', function (e) {
        setMessageInnerHTML("建立连接。。。");
    }, false);
    /**
         * 客户端收到服务器发来的数据
         * 另一种写法：source.onmessage = function (event) {}
         */
    source.addEventListener('message', function (e) {
        setMessageInnerHTML(e.data);
    });
} else {
    setMessageInnerHTML("你的浏览器不支持SSE");
}
```

服务端创建一个`SseEmitter`对象放入`sseEmitterMap`进行管理

```java
// 管理SseEiter对象
private static Map<String, SseEmitter> sseEmitterMap = new ConcurrentHashMap<>();

/**
 * 创建连接
 */
public static SseEmitter connect(String userId) {
    try {
        // 设置超时时间，0表示不过期。默认30秒
        SseEmitter sseEmitter = new SseEmitter(0L);
        // 注册回调
        sseEmitter.onCompletion(completionCallBack(userId));
        sseEmitter.onError(errorCallBack(userId));
        sseEmitter.onTimeout(timeoutCallBack(userId));
        sseEmitterMap.put(userId, sseEmitter);
        count.getAndIncrement();
        return sseEmitter;
    } catch (Exception e) {
        log.info("创建新的sse连接异常，当前用户：{}", userId);
    }
    return null;
}

/**
 * 给指定用户发送消息
 */
public static void sendMessage(String userId, String message) {

    if (sseEmitterMap.containsKey(userId)) {
        try {
            // 通过userid获取对应的sseEmitter
            sseEmitterMap.get(userId).send(message);
        } catch (IOException e) {
            log.error("用户[{}]推送异常:{}", userId, e.getMessage());
            removeUser(userId);
        }
    }
}
```



###### SSE vs. Websocket

> SSE 是基于 HTTP 协议的，它们不需要特殊的协议或服务器实现即可工作；WebSocket 需单独服务器来处理协议
>
> SSE 单向通信，只能由服务端向客户端单向通信；WebSocket 全双工通信，即通信的双方可以同时发送和接受信息
>
> SSE 实现简单开发成本低，无需引入其他组件；WebSocket 传输数据需做二次解析，开发门槛高一些
>
> SSE 默认支持断线重连；WebSocket 则需要自己实现
>
> SSE 只能传送文本消息，二进制数据需要经过编码后传送；WebSocket 默认支持传送二进制数据

ChatGPT 就是采用的 SSE，对于需要长时间等待响应的对话场景，ChatGPT 采用了一种巧妙的策略：它会将已经计算出的数据“推送”给用户，并利用 SSE 技术在计算过程中持续返回数据



#### 如果用户关闭页面，重新登录，如何继续通知？

> 当用户关闭页面时，会触发onclose回调事件，客户端就会把该用户对用的Session对象从Map中移除
>
> 所以后端就无法继续为客户通知进度，当然既然用户选择关闭页面，那么也意味着用户不需要去看进度
>
> 当用户重新登陆时，onopen回调事件会重新创建一个Session存到Map中，这个Map的key其实时通过用户的信息（如用户id，用户所属法院等）来生成的，所以此时通知模块就可以重新在Map中获取到该用户对应的Session对象，就可以继续发送通知了。
>
> 注意：Map<String, Session> 中的Key是通过用户信息生成的token，而不是seesionID
>
> 



### 关于任务多线程和线程池 :star:

##### Future对象

> `Future` 接口有 5 个方法：
>
> - `boolean cancel(boolean mayInterruptIfRunning)`：尝试取消执行任务。
> - `boolean isCancelled()`：判断任务是否被取消。
> - `boolean isDone()`：判断任务是否已经被执行完成。
> - `get()`：等待任务执行完成并获取运算结果。
> - `get(long timeout, TimeUnit unit)`：多了一个超时时间。
>
> `CompletableFuture`实现了`Future`接口，并实现了`CompletionStage` 接口，提供了更强大的功能
>
> ​	任务的顺序执行



##### 怎么获取失败的任务以及查看原因？如何处理异常

> 在配置线程池的时候，自定义统一的异常处理`getAsyncUncaughtExceptionHandler()`
>
> ```java
> @Override
> public AsyncUncaughtExceptionHandler getAsyncUncaughtExceptionHandler() {
>     return new CustomAsyncExceptionHandler(); // 绑定自定义异常处理
> }
> ```
>
> 在里面可以设置task的失败原因给用户看，日志记录，加失败任务加入失败队列中，方便重跑

> 在代码中try-catch，如果 `@Async` 方法本身抛异常，并且没有 `try-catch` 处理，Spring 默认不会通知调用方，而是**静默地吞掉异常**
>
> 所以可以在异步方法中自行处理异常

> submit返回的Future对象，调用get()可以获取异常
>
> 也可以使用`CompletableFuture`自带的`exceptionally()`方法来处理，也可以用`handle()`
>
> ```java
> @Async
> public CompletableFuture<String> run() {
>  return CompletableFuture.supplyAsync(() -> {
>      System.out.println("异步任务执行...");
>      int result = 1 / 0;  // 触发异常
>      return "任务完成";
>  }).exceptionally(ex -> {
>      System.err.println("任务失败：" + ex.getMessage());
>      return "默认值";
>  });
> 
>  CompletableFuture<String> future = CompletableFuture.supplyAsync(() -> {
>      if (true) {
>          throw new RuntimeException("Computation error!");
>      }
>      return "hello!";
>  }).handle((res, ex) -> {
>      // res 代表返回的结果
>      // ex 的类型为 Throwable ，代表抛出的异常
>      return res != null ? res : "world!";
>  });
> }
> 
> 
> ```



##### 怎么重新运行某个任务？

> 任务持久化在数据库Task表中，通过submit提交到线程池中
>
> ```java
> public Future<String> submitTaskWithResult() {
>     return customExecutor.submit(() -> {
>         Thread.sleep(1000);
>         return "任务完成";
>     });
> }
> ```
>
> 任务的处理流程如下：
>
> 1. 线程数 < corePoolSize
>    - 直接创建新线程执行任务
> 2. 线程数 ≥ corePoolSize 且队列未满
>    - 任务进入**队列等待**执行
> 3. 队列已满 且 线程数 < maxPoolSize
>    - 创建新线程执行任务
> 4. 线程数 = maxPoolSize 且队列已满
>    - 执行**拒绝策略**（比如抛异常、丢弃任务等）





##### 怎么中断某个任务?

> 使用@Async的返回，代码中通常**new AsyncResult<返回类型>(返回值)**，返回一个`Future`对象，通过它来操作
>
> ①如果任务是在线程池中等待
>
> 只要submit()成功的，无论是**线程正在执行**，或是**在BlockingQueue中等待**执行，future.cancel()操作均可中断掉线程
>
> ②如果任务是在MQ中等待
>
> 通过消费者端来过滤，在提交任务之前判断任务的状态是否为CANCEL，是的话直接丢弃
>
> ③如果任务是在运行中
>
> 任务运行时，会返回一个FutureTask对象，将它保存下来，跟任务id关联，使用`ConcurrentHashMap<Long, Future>`保存
>
> 通过taskId去找到对应的Future，调用`future.cancel()`来取消任务运行
>
> **遇到的问题**：cancel不一定能中断任务

###### 分析Future.cancel

> 代码：底层是调用了`Thread.interrupt()`来实现的
>
> ```java
> public boolean cancel(boolean mayInterruptIfRunning) {
>     if(!(state == NEW 
>          && UNSAFE.compareAndSwapInt(this, stateOffset, NEW, mayInterruptIfRunning? INTERERUPTING: CANCELED))) // 先尝试修改NEW状态为中断/取消 失败则直接返回false
>         return false;
>     try {
>         if(mayInterruptIfRunning) {
>             try {
>                 Thread t = runner;
>                 if(t != null) t.interrupt();
>             } finally {
>                 UNSAFE.putOrderdInt(this, stateOffset, INTERRUPT);
>             }
>         }
>     } finally {
>         finishCompletion();  // 释放线程中持有的锁 
>     }
>     return true;
> }
> ```
>
> 其中：**`mayInterruptIfRunning`**参数指示如果任务正在执行，是否应该通过中断当前线程来强制取消任务
>
> **问题**：interrupt只能是停掉线程中有sleep,wait,join逻辑的线程，抛出一个`InterruptException`
>
> ​	 线程sleep是可以被Future.cacel()中断的
>
>  	线程中的IO阻塞时，线程无法被Future.cancel()中断
>																																																																																																							
>  	线程中的synchronized锁阻塞时，线程无法被Future.cancel()方法中断（Lock是可以被中断的）
>
> 能结束任务的唯一条件：FutureTask中必须能抛出 InterruptedException 异常
>
> **解决**：
>
> ①任务必须支持中断：**定期**判断中断位`Thread.currentThread().isInterrupted()`，对中断做出响应
>
> ```java
> if (Thread.currentThread().isInterrupted()) {
>     throw new InterruptedException(); 
> }
> ```
>
> ②正确处理中断
>
> 当线程在进行一些阻塞操作时，可以catch异常，并抛出`InterruptedException`异常



#### 问题

##### 1.线程池的参数是怎么设置的，有什么依据的原理？

比如核心线程数的设置，是否需要考虑服务器的硬件？

核心线程数：

> 一般来说：
>
> CPU密集型尽量配置少的线程，**核心线程配置：CPU核数**
>
> 有的说是N+1，是为了以备不时之需，如果某线程因等待系统资源而阻塞时，可以有多余的线程顶上去，不至于影响整体性能
>
> IO密集型线程池应配置多的线程，**核心线程配置：CPU核数 * M** 其中M>1，一般为2
>
> 更准确的计算公式：
>
> 最佳线程数 = N ∗ [ 1+WT（线程等待时间）/ ST（线程计算时间) ]
>
> ​	其中，N为CPU核数，WT（线程等待时间）= 线程运行总时间 - ST（线程计算时间）
>
> 可以通过 JDK 自带的工具 `jVisualVM` 来查看 `WT/ST` 比例

最大线程数：

> maxPoolSize=核心线程数+(最大允许等待任务数/任务处理能力)
>
> 计算密集型任务：一般等于 `corePoolSize`，不需要太多额外线程。
>
> I/O 密集型任务：可以大于 `corePoolSize`，但要考虑 CPU 和内存的承受能力

任务队列：

> **短任务推荐**：`SynchronousQueue`，不存储任务，每次直接交给线程执行，适用于高吞吐量
>
> **任务量较大**：`LinkedBlockingQueue`，可以存储大量任务，但会影响最大线程数的扩展
>
> **有限任务队列**：`ArrayBlockingQueue`，控制任务队列大小，避免任务无限增长



##### 如何判断线程数设置多了还是少了？

通过 `top` 命令观察 CPU 的使用率，如果 CPU 使用率较低，可能是线程数过少；如果 CPU 使用率接近 100%，但吞吐量未提升，可能是线程数过多

也可以使用 `jstack` 命令查看线程堆栈信息，查看线程是否处于阻塞状态

也可以使用`VisualVM`来观察

启动 VisualVM 后，在左侧的 **“本地” (Local) 进程列表** 中找到Java 应用程序

双击该进程，进入**监视**界面，在**“线程”选项卡**里可以看到 RUNNABLE线程数（WT），WAITING线程数（ST）

通过 **"线程 Dump"**，分析 WT（工作线程）/ ST（等待线程）比例，优化 `corePoolSize`。

> **WT 过高**（>80%）：说明任务繁忙，可以适当增加 `corePoolSize` 或 `maximumPoolSize`
>
> **ST 过高**（>80%）：说明线程大部分时间在等待任务，可能 `corePoolSize` 过大，或者任务队列 `workQueue` 过长
>
> **WT≈ST**：说明线程池负载适中



##### 2.如何监控线程池的状态？

1.SpringBoot自带的`Actuator`

2.`ThreadPoolExecutor` 的相关 API 

 <img src="https://oss.javaguide.cn/github/javaguide/java/concurrent/threadpool-methods-information.png" alt="img" style="zoom:67%;" />



##### 3.如果进来了一个大任务，非常耗时，导致其他任务等待，要怎么处理？

> 线程池本身的目的是为了提高任务执行效率，避免因频繁创建和销毁线程而带来的性能开销。如果将耗时任务提交到线程池中执行，可能会导致线程池中的线程被长时间占用，无法及时响应其他任务，甚至会导致**线程池崩溃**或者**程序假死**
>
> 解决：
>
> **1.任务拆分**：如果大任务是可拆分的，可以使用 **Fork/Join** 机制或 **多线程拆分任务**，让多个线程并行处理子任务，加快执行速度
>
> ​	手动分割：在项目中，因为统计任务是根据**时间年份**来的，所以可以根据年份来划分任务
>
> ​	`CompletableFuture`：CompletableFuture<T> headerFuture=CompletableFuture.**allOf**(task1,.....,task6)
>
> ​	<u>`CompletableFuture` 默认使用全局共享的 `ForkJoinPool.commonPool()` 作为执行器</u>(被问过一次)
>
> ​	
>
> **2.优先级队列**：
>
> ​	线程池的workQueue可以使用 **PriorityBlockingQueue**，让小任务优先执行，避免大任务长期占用线程池
>
> ​	消息队列也可以在**消息头**中设置priority
>
> 
>
> **3.任务超时取消**
>
> ​	可以为任务设置 **超时时间**，如果任务超过一定时间还未完成，则 **强制取消**
>
> ​	使用 `ExecutorService.submit()` 提交任务，并配合 `Future.get(timeout, TimeUnit)` 来设置超时
>
> ​	对于超时的任务在单独进行处理，比如任务拆分





##### 4.既然任务都存在Mysql里了，为什么不直接用定时任务去处理，而是用线程池？

> 在我们的业务场景中，**任务是由用户请求触发的，并且需要尽快处理**，所以选择了 **RabbitMQ + 线程池** 来实现 **异步任务调度**。相比之下，定时任务适用于**周期性、固定间隔执行的任务**，但不适合处理**高并发、实时性强的任务队列**。
>
> 而且任务的运行时间不固定，也不好评估，因为不同的法官会设置不同的任务条件。如果周期设置过长，可能也会导致任务堆积。而且定时任务一般**单线程或少量线程执行**，**无法并行处理大量任务**
>
> 可以使用定时任务来处理失败重跑的任务



##### 5.线程池断电怎么办？

> 第一，持久化任务。可以将任务持久化到数据库或者消息队列中，等电恢复后再重新执行。
>
> 第二，任务幂等性，需要保证任务是幂等的，也就是无论执行多少次，结果都一致。
>
> 第三，恢复策略。当系统重启时，应该有一个恢复流程：检测上次是否有未完成的任务，将这些任务重新加载到线程池中执行，确保断电前的工作能够恢复。



##### 6.线程池参数动态设置

1.使用线程池的接口：

![img](面经.assets/threadpoolexecutor-methods.png)

2.利用 Nacos 配置中心

```java
@Slf4j
@Configuration
@NacosConfigurationProperties(dataId = "threadpool-service.yaml", autoRefreshed = true)
public class ThreadPoolConfig {

    @NacosValue("${threadpool.coreSize:10}")
    private int corePoolSize;

    @NacosValue("${threadpool.maxSize:50}")
    private int maxPoolSize;

    @NacosValue("${threadpool.queueCapacity:100}")
    private int queueCapacity;

    private ThreadPoolTaskExecutor executor;
    
    ...
}
```

配置文件:

```yaml
threadpool:
  coreSize: 10
  maxSize: 50
  queueCapacity: 100
```

3.美团

自定义了一个叫做 `ResizableCapacityLinkedBlockIngQueue` 的队列来实现WorkQueue的动态调整

（主要就是把`LinkedBlockingQueue`的 capacity 字段的 final 关键字修饰给去掉了，让它变为可变的）





##### 7.线程池调优

![三分恶面渣逆袭：线程池调优](https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/javathread-82.png)

1.初始化线程池参数

​	根据IO密集型，核心线程数设置为 2N（2倍核数）

​	`maxPoolSize` 取决于 **任务的并发峰值**

2.监控

​	通过线程池自带的API	

​	通过SpringBoot Actuator

​	DynamicTp

监控如下信息：

​	**线程数** 是否长期保持在 `maxPoolSize`（表示任务过载）

​	**任务等待队列** 是否持续增长（可能意味着队列过小）

​	**任务执行时间** 是否过长（可能需要优化任务逻辑）

​	**CPU/内存占用** 是否超标（可能需要减少并发线程数）

3.动态调整：

​	Nacos配置中心来动态调整	

​	直接使用 DynamicTp 实现线程池参数的自动化调整

4.事后观察

​	DynamicTp **内置 Prometheus 监控指标**，可以通过 **Spring Boot Actuator** 采集数据	

​	通过内置的监控指标建立容量预警机制。比如通过 JMX 监控线程池的运行状态，设置阈值，当线程池的任务队列长度超过阈值时，触发告警



##### 8.使用ConcurrentHashMap存Future对象，如果服务重启了怎么办？

服务重启会导致hashmap中的future对象丢失，但是对应的任务也会被中断，所以就算这个future还存在也没有作用

在我的项目中，服务重启后，会将任务表中未完成的任务按时间顺序重新submit到线程池中去，所以这一过程中会重新生成一个future对象并存放到ConcurrentHashmap中的



##### 9.每次用户提交任务请求后创建一个线程去运行，这样效率不是更高吗?

线程池的核心价值不只是提高并发，而是“控制资源 + 提升性能 + 稳定运行”

①每次 `new Thread().start()` 都要：向操作系统申请栈空间（通常是 1MB）、创建线程上下文、调度结构等

②线程数不可控，可能拖垮系统：1万个并发请求 → 1万个线程 

③线程池可监控、可管理

④线程池可以复用线程，避免线程的频繁创建和销毁，减少GC压力



##### 10.为什么要使用线程池？

> 线程池的目的是控制并发线程的数量，防止系统资源耗尽。
>
> 线程复用，减少线程的创建开销
>
> 更方便的监控线程
>
> 保证系统稳定运行

**降低资源消耗**。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。

**提高响应速度**。当任务到达时，任务可以不需要等到线程创建就能立即执行。

**提高线程的可管理性**。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。



##### 11.如何完成线程池的预热？

> 默认情况下，Java线程池（如 `ThreadPoolExecutor`）在创建时并不会立即启动核心线程，而是要等任务到来时才启动。这样：
>
> - **第一次请求延迟高**：任务触发了线程创建；
> - **线程创建是昂贵的**，涉及内核资源和上下文切换；
> - **并发量大时更明显**：多个任务进来，多个线程“临时起”。
>
> 为了优化首批请求体验或系统高峰期响应时间，我们可以**在系统启动时就让核心线程就位**。
>
> 方法：
>
> ①直接调用 `prestartAllCoreThreads()` 同时创建所有的核心线程
>
> ②也可以按需调用 `prestartCoreThread()` 多次，每次创建一个
>
> 实际项目中，可以使用`@PostConstruct`来预热
>
> ```java
> @Component
> public class ThreadPoolWarmUp {
> 
>     @Autowired
>     private ThreadPoolExecutor taskExecutor;
> 
>     @PostConstruct
>     public void warmUp() {
>         int started = taskExecutor.prestartAllCoreThreads();
>         System.out.println("预热线程数：" + started);
>     }
> }
> ```
>
> 



### 关于ElasticSearch

##### 基础概念

Mysql和ES的概念对比：

| Mysql           | ES               |
| --------------- | ---------------- |
| Table（表）     | Index（索引）    |
| Row（行）       | Document（文档） |
| Column（列）    | Field（字段）    |
| Schema（约束）  | Mapping（映射）  |
| SQL（查询语言） | DSL（查询语言）  |



##### Index索引

> Index中包含了：
>
> ​	settings：设置索引的分片数、索引的副本数
>
> ​					设置索引的刷新时间（在大批量导入数据时可以先设置为-1即不刷新，导完后再打开）
>
> ​					可以自定义分词器analyses：filter字符过滤器、tokenizer分词器
>
> ​	mappings：设置映射，包括字段设计、每个字段的类型、分词器、搜索分词器（默认为分词器）
>
> ​	mappings是**不支持修改**的
>
> ```json
> PUT /documents
> {
>     "settings": {
>         // 索引的分片数 默认为5
>         "number_of_shards": 5,  
> 
>         // 分片的副本数
>         "number_of_replicas": 0,   
> 
>         // 暂时关闭自动刷新
>         "index.refresh_interval": -1,
> 
>         // 自定义分词器
>         "analysis": {
>             ...
>         },
> 
>             "mappings": {
>                 "properties": {
>                     // id
>                     "documentId": {
>                         "type": "keyword"  
>                     },
>                     // 全文
>                     "quanwen": {
>                         "type": "text",
>                         "analyzer": "ik_smart",
>                         "search_analyzer": "ik_smart"
>                     },
>                     ...
>                   }
>               }
>         }
> }
> ```
>
> 动态mapping：当往索引中添加的数据含有新字段时，会自动添加到mapping中
>
> Mapping的`dynamic = true/runtime/false/strict`
>
> * true：新字段被添加到mapping中（默认）
> * runtime：新字段作为运行时字段添加到mapping中，但不会编入索引，查询时加载
> * false：新字段会被忽略
> * strict：拒绝该文档并抛出异常
>
> 可以通过**映射限制**设置来防止动态映射产生**索引爆炸**的问题



##### 倒排索引

> 以**词条（Term）**作为索引，并记录了哪些**文档（Document）**含有这个词条
>
> 倒排索引中包含了：
>
> * 单词字典：所有的Term列表，一般由B+树存储
> * 倒排列表：记录每个Term与其对应的文档的集合
>   * **term**
>   * **docId**
>   * **TF**（词频）
>   * **Position**（在文档中的位置，用于检索）
>   * **offsets**（偏移量，用于高亮）
>
> <u>倒排索引是不能修改的！！！</u>
>
> 如果想要更新现有的文档，需要**重建索引**或者进行**替换**
>
> 好处：不需要**加锁**、写入单个大的倒排索引允许数据被**压缩**，减少磁盘 I/O 和 需要被缓存到内存的索引的使用量
>
> ​		  不存在脏页的清空
>
> 对于文档的更新和删除：
>
> ​	更新时，同一个文档在磁盘中同时会有两个索引数据：一个是<u>原来的索引</u>，另一个是<u>修改之后的索引</u>
>
> ​	删除时，实际上只是在`.del` 文件中被 **标记** 删除。一个被标记删除的文档仍然可以被查询匹配到， 但它会**在最终结果被返回前从结果集中移除**(逻辑删除)

###### 倒排索引的创建过程

> 1.对插入的文档进行编号`_docId`
>
> 2.分词器（Analyzer）工作
>
> - 字符流处理、分词(tokenization)、过滤token(如停用词)
>
> 3.构建倒排索引
>
> 4.写入**Segment**文件，Lucene 定期执行 **段合并**（refresh）
>
> 所以在大批插入数据时，可以把自动refresh先关了，插入后再打开，在refresh之后这批数据才可以被检索到
>
> 具体细节：
>
> 在这个过程中，文档会先写入 **Lucene Memory buffer**中(不可被搜索)，同时记录到**translog**中，保证事务的持久性
>
> 通过refresh操作，Lucene会将buffer中的内容生成一个segment文件，并刷到**磁盘**中（这里会生成很多小的segment文件），同时清空buffer和translog
>
> 同时Lucene会在内存中构建倒排索引 segment reader（每次es会在磁盘索引中搜索和内存中搜索）
>
> 接着定期的对**小的segment**进行**合并**，合成大的segment
>
> 通过flush刷到磁盘中，与原索引文件进行合并

###### ES中的segment以及段合并

> elasticsearch允许用户选择段合并政策（merge policy）及储存级节流（store level throttling）
>
> 当我们往 ElasticSearch 写入数据时，数据是先写入 **memory buffer**，然后定时（默认每隔1s）将 memory buffer 中的数据**写入一个新的 segment 文件**中，并进入 Filesystem cache（同时清空 memory buffer），这个过程就叫做 refresh；**每个 Segment 事实上是一些倒排索引的集合**， <u>只有经历了 refresh 操作之后，数据才能变成可检索的</u>
>
> ElasticSearch 每次 refresh 一次都会生成一个新的 segment 文件，这样下来 segment 文件会越来越多。那这样会导致什么问题呢？因为每一个 segment 都会占用文件句柄、内存、cpu资源，更加重要的是，每个搜索请求都必须访问每一个segment，这就意味着存在的 segment 越多，搜索请求就会变的更慢。
>
> **每个 segment 是一个包含正排（空间占比90~95%）+ 倒排（空间占比5~10%）的完整索引文件**，每次搜索请求会将所有 segment 中的倒排索引部分加载到内存，进行查询和打分，然后将命中的文档号拿到正排中召回完整数据记录。如果不对segment做配置，就会导致查询性能下降
>
> 那么 ElasticSearch 是如何解决这个问题呢？
>
> ​	 ElasticSearch 有一个**后台进程专门负责 segment 的合并**，定期执行 merge 操作，将多个小 segment 文件合并成一个 segment，在合并时<u>被标识为 deleted 的 doc（或被更新文档的旧版本）不会被写入到新的 segment 中</u>。合并完成后，然后将新的 segment 文件 flush 写入磁盘；然后创建一个新的 commit point 文件，标识所有新的 segment 文件，并排除掉旧的 segement 和已经被合并的小 segment；然后打开新 segment 文件用于搜索使用，等所有的检索请求都从小的 segment 转到大 segment 上以后，删除旧的 segment 文件，这时候，索引里 segment 数量就下降了
>
> 注意: elasticsearch在进行删除时，是不会直接物理删除，而是**对要删除的对象进行标记**，在进行段合并的时候不复制这些数据到新的索引段中
>
> 可以调用**reflesh API**进行手动的刷新索引，保证新的数据及时可以被搜索





###### 倒排索引的底层数据结构

> 跳表（Skip List）：在倒排列表中引入跳表结构，允许快速跳转到指定位置，加速查询速度。
>
> 前缀压缩（Prefix Compression）：对词典中的相邻词条进行前缀压缩，减少存储空间。
>
> 块索引（Block Indexing）：将倒排列表分成固定大小的块，每个块包含多个文档ID。查询时，可以快速定位到包含目标文档ID的块，从而减少遍历的时间。



######  posting list 

> 每个term都会有一条posting list，对应的是含有这个term的文档
>
> 跳表+压缩列表来优化posting list的存储，快速定位到某个 docID 段
>
> **所有 term 按照字典序排序**，并构建一个 **FST 前缀树**（trie 树 + 状态机）
>
> 整个查询过程就是先在字典树中查询到term，获取对应的posting list -> 解压 + 跳表 → 返回匹配的 docID



###### 正排索引

> 反过来
>
> docId -> Term1、Term2、... 



##### 分词器（Analyzer）

> 分词器的构成有3部分：
>
> * character filters 字符过滤器对原始文本进行预处理
> * tokenizer 分词器对预处理后的文本进行切分成一个个词条，形成词条流切分规则可以是空格、标点符号、某种特定算法(边缘n-gram)
> * token filters 词汇过滤器对切分后的词条进行处理，可以大小写转换、删除停用词、词干化（did -> do）等
>

> 自带的分词器：不适合中文，非中文分词器
>
> * Standard Analyzer 标准分词器：中文会一个一个字分
> * Keyword Analyzer 关键词分词器：整个输入作为一个关键词，不分词
>
> 中文分词器：**IK分词器**插件（专门为ES做了适配）
>
> * `ik_smart`：智能模式，粗粒度
> * `ik_max_word`：细粒度
>
> 拼音分词器：**pinyin分词器**：用于实现拼音搜索提示
>

> ES索引中，mapping中每个field都可以单独的指定**分词器**以及**搜索分词器**
>
> 搜索分词器默认和字段分词器一致
>
> ```json
> "mappings": {
>     "properties": {
>       "content": {
>         "type": "text",
>         "analyzer": "my_content_analyzer",
>         "search_analyzer": "standard",
>       }
>    	}
> }
> ```
>
> 分词器可以定义全局的分词器，在settings的analysisi中定义
>
> ```json
> PUT index_name
> {
>   "settings": {
>     "analysis": {
>       "analyzer": {
>       	"standard_with_stopwords":{   // 给分析器起名
>       		"type":"standard", 
>       		"stopwords":"_english_"   // 开启停用词 默认不开启
>       	}
>       }
>     }
>   }
> }
> ```
>
> 



##### Keyword类型

> 一个属性是Keyword类型，那么它不走分词器
>
> 使用keyword类型来查询效率更高
>
> 例如，在海量文书检索系统中，anhao属性就添加了一个keyword类型的子field，用来对案号进行精确的查询
>
> 使用term查询来精确查询keyword类型的字段
>
> ```json
> "anhao": {
>     "type": "text",
>     "fields": {
>         "keyword": {
>            	"type": "keyword"
>         }
>     }
> }
> ```
>
> 



##### 查询

1.match：标准的全文查询

2.Term查询：讲输入作为一整个关键词

3.sort：对查询结果进行排序

4.highlight：对词条进行高亮

5.**分页** :star:

​	`from size`：不推荐、当偏移量太大时会很慢

​	`scroll`：创建一个分页上下文，保存当前查询的快照

​				 请求url加上<u>?**scroll**=1m</u>参数来设置查询上下文有效时长，返回一个srcoll_id作为下一次查询参数

​	`search after`(推荐)：在下一次查询中传入上一次查询返回的sort值（其中包含了上一次查询结果在整个查询集中的顺序）

​	为了避免索引刷新，可以创建一个**PIT**(point in time)来保护本次查询。在查询请求中加上/_pit?keep_alive=1m会放回一个pit_id，下次查询可以带上它

6.track_total_hits设置为true，用于返回每次查询的准确的total hit，否则total最大不超过10000

7.timeout：在查询时加上这个参数可以设置查询超时直接返回

8.filter：过滤，用于数值类型的查询，效率高

9.range：范围查询

10.bool：用于组合不同的查询语句

11.agg：用于对查询结果进行聚合，类似于group by，实现二次筛选



全文查询

```
GET /documents/_search
{
  "query": {
    "multi_match": {
      "query": "",  // 用户输入
      "fields": ["anyou", "fayuanmingcheng","dangshiren", 
                 "lvshi", "shenpanren", "quanwen", 
                 "anjianmingcheng", "anhao", "keywords" ]
    }
  },
  "highlight": {
    "fields": { // 指定要高亮的字段
      "anjianmingcheng": {},
      "quanwen": {}
    }
  }
}
```

精确查询：

使用term和filter

##### 搜索提示

ES自带的四种Suggester：但是对中文不友好

- `Term Suggester`，基于编辑距离，对analyze过的单个term去提供建议，并不会考虑多个term/词组之间的关系。`quert -> query`
- `Phrase Suggester`，在Term Suggester的基础上，通过ngram以词组为单位返回建议。`noble prize -> nobel prize`
- `Completion Suggester`，FST数据结构，类似Trie树，不用打开倒排，快速返回，前缀匹配
- `Context Suggester`，在Completion Suggester的基础上，用于filter和boost



自己实现：使用search_as_you_type类型

当一个字段是search_as_you_type类型时，会创建如下几个子字段

* `my_filed` 自定义的字段
* `my_field._2gram` 用分词长度为2的shingle分词器对my_field进行分词
* `my_field._3gram` 用分词长度为3的shingle分词器对my_field进行分词
* `my_field._index_prefix` 用 **edge ngram** token filter 包装`my_field._3gram`进行分词

要让查询的terms严格的**按照顺序进行匹配**，可以对**根字段**使用`match_phrase_prefix`查询，但效率可能低于`match_bool_prefix`

```
PUT index_for_suggest
{
  "mappings": {
    "properties": {
     "title": {
        "type": "text",         
        "fields": { # 设置一个子字段 suggestion
          "suggestion": {
            "type": "search_as_you_type"
          }
        }
      }
    }
  }
}

// 使用match_phrase_prefix来查询title.suggestion
GET /index_for_suggest/_search
{
  "query": {
    "bool": {
      "should": [
        {
          "match_phrase_prefix": {
            "title.suggestion": "说今天是一个"
          }
        }
      ]
    }
  }
}
```

对于拼音的搜索提示：

例如："北京市"应该得到：b、bei、j、jing、s、shi、bj、beijing、js、jingshi、bjs、beijingshi，这样才能用于前缀和中缀匹配

做法：因为使用search_as_you_type可以自动生成_2gram和 _3gram，所以我们只需要让analyzer能将py_suggest字段分词为：b、bei、j、jing、s、shi即可。具体做法就是tokenizer在分词的时候只保留**每个字的首字母和全拼音**就可以



##### 有没有遇到ES慢查询？怎么排查解决？

可以回答深度分页的问题

> 1.开启慢查询日志，可以在 `elasticsearch.yml` 配置文件中添加，设置慢查询的阈值为1s（一般）
>
> 2.查看慢查询日志：从慢查询日志中可以找到执行缓慢的 `DSL` 语句，重点关注
>
> - `took` 字段：查询花费时间
> - `from` / `size` 参数：分页查询是否过大
> - `query` 结构：是否使用了低效查询，例如没有使用filter查询
>
> 3.使用`_profile`来分析查询语句
>
> ```
> GET <my_index>/_search?profile=true
> {
>   "query": {
>     "bool": {
>       "must": [
>         {
>           "match": {
>             "content": "ElasticSearch"
>           }
>         }
>       ]
>     }
>   }
> }
> ```
>
> 输出：
>
> ```
> "profile": {
>   "shards": [
>     {
>       "searches": [
>         {
>           "query": [
>             {
>               "type": "TermQuery",
>               "time": "12ms"
>             }
>           ]
>         }
>       ]
>     }
>   ]
> }
> ```
>
> 4.分析索引健康
>
> ​	看是否设置了不恰当的refresh_interval，导致频繁的索引刷新
>
> 5.优化查询：发现是from size深度分页的问题
>
>    将from size 转换为search after



##### search_after底层实现

> 在 **排序字段** 上创建 **位图索引（Bitmap Index）** 来加速排序和分页操作。
>
> 使用 `search_after` 时，Elasticsearch 会根据游标（上一页最后一条文档的排序字段值）来精确定位数据的位置，从而跳过不必要的文档扫描，直接定位到下一页的结果。
>
> 所以每次查询，带上上一次查询的sort，就可以直接定位到下一页的起始数据，也不需要重新扫描整个结果集
>
> `search_after` 基于各个节点查询合并后的排序结果进行分页，从而提高性能



##### agg聚合的底层实现

> ###### 桶聚合（Bucket Aggregations）
>
> 桶聚合是将数据分组的过程。根据查询条件，Elasticsearch 会按照某个字段的值（如 `category`、`date` 等）将文档分成不同的桶（bucket）。每个桶可能包含多个文档。桶聚合的实现是通过倒排索引进行的，具体操作如下：
>
> - 当查询指定了某个字段进行分组时，Elasticsearch 会首先扫描倒排索引，找到该字段的所有可能值，然后根据值将文档分配到相应的桶中。
> - 然后，它会对每个桶中的文档进行统计。
>
> **桶聚合的实现步骤：**
>
> 1. 在倒排索引中查找该字段的所有词项；
> 2. 根据字段值（如 `category`）将文档划分到不同的桶中；
> 3. 对每个桶中的文档进行统计（如计数、平均值等）。



##### 系统是如何设计ES索引结构的？具体有哪些字段做了分词，为什么这么设计？

> 我们根据文书的结构和用户的检索需求设计了ElasticSearch的索引的映射（mapping）
>
> 对于每个字段，我们详细的设计了它们的类型以及分词器，考虑是否需要分词，以及分词的粒度如何
>
> 同时给每个字段搭配一个搜索分词器
>
> - **需要分词的字段（text）**：
>   - 文书标题（title）：使用 `ik_max_word` 进行索引分词，`ik_smart` 作为搜索分词器。
>   - 文书正文（content）：同样使用 `ik_max_word`，提升召回率。
> - **不分词的字段（keyword）**：
>   - 案号、法院编号、发布日期、区域编码等，用于精确匹配、聚合和筛选
>   - 因为term查询和filter查询比match查询要快



##### 为什么不使用mysql的like而是选择ES？

> 传统的 `LIKE '%关键词%'` 查询在数据库中无法利用索引，效率极低，在百万级文书下会导致严重性能问题
>
> ES有集成的IK分词器，可以更好的适配中文，分词准确性高，查询结果更准确
>
> ES可以以集群的形式搭建，索引以分片的形式存储在不同的节点上，支持主从备份和故障转移
>
> 支持语义 相似度 评分



##### ES集群发生故障如何排查？

> 首先，系统使用Prometheus + Gavana来监控ES集群
>
> 当出现问题时，先查看集群健康状态 `_cluster/health` 和 `_cat/shards` 来判断是**分片、节点还是查询问题**
>
> 集群状态为`Yellow`: 说明主分片都分配成功，有副本分片分配失败
>
> ​	查看分配失败的分片：
>
> ​	①查看副本分片所在的节点是否正常，如果不正常，重启后会自动加入集群
>
> ​	②查看是否节点磁盘空间不足，进行清理
>
> 状态为Yellow不会影响查询结果
>
> 集群状态为`Red`：说明有主分片分配失败，此时会导致查询结果不准确，部分数据无法使用
>
> 集群状态为`Green`：正常



##### 如何实现语义分析搜索？

用户输入离婚，如何命中婚姻法等相关内容？

> 方案一：**基于搜索同义词扩展**
>
> ​	自行维护一个同义词词典 `synonyms.txt`，进行同义词的扩展
>
> ​	然后给相关的字段增加一个同义词分词器，在对原文本进行分词时，索引中就会存储对应的同义词
>
> ​	会增大索引的大小
>
> 方案二：基于**向量**的语义搜索（dense_vector + BERT）
>
> ​	对文书进行向量化，再对用户输入的内容进行向量化，计算两者的相似度
>
> ​	开源中文句向量模型：HuggingFace
>
> 方案三：**Query Expansion + ES**（智能问句改写）
>
> ​	使用大模型对用户的输入进行语义扩充，从而去扩大整个query





### 关于Canal

##### 为什么使用Canal来同步数据？

回答在logstash和canal中进行选择了canal

| 方案                     | 适用场景                     | 数据同步方式      | 延迟                    | 复杂度     | 可靠性   |
| ------------------------ | ---------------------------- | ----------------- | ----------------------- | ---------- | -------- |
| **Logstash**             | **数据量小，低频增量更新**   | 定期查询 MySQL    | **较高（秒级~分钟级）** | **简单**   | **一般** |
| **Canal（基于 Binlog）** | **高并发、大数据量、低延迟** | 监听 MySQL Binlog | **低（毫秒级）**        | **较复杂** | **高**   |

Logstash的原理：

​	在数据库表中添加一个同步追踪字段，例如last_updatetime，Logstash 通过一个**循环**来定期查看上一次迭代之后 Mysql 表中新插入的或者更新的数据，通过跟踪字段的值来记录每一次迭代结束的位置，并存储到:sql_last_value变 量中， 下一次轮询会找出 track_column 字段值大于:sql_last_value 的记录，同步更新到ElasticSearch中

​	需要自己写脚本，分为input（指定数据源）、Filter（过滤器，进行字段映射）、Output（指定ES）

​	**优点**：跟ES兼容，作为 Elastic Stack（ELK Stack）中的关键组件之一，用它来实现数据同步的逻辑非常简单

​	**缺点**：轮询、延迟高、**不支持删除同步**、只支持增量同步

Canal原理：

​	Canal **伪装成 MySQL salve**，通过 `slave协议` 连接 **MySQL 主库**监听 **Binlog（Binary Log）**，订阅数据库变更事件。Canal中有 **服务端deployer** 和 **客户端adapter** ，服务端负责从mysql中读取binlog，而客户端负责从服务端读取同步过来的binlog数据，处理后将同步数据发送到目标服务端

<img src="https://i-blog.csdnimg.cn/blog_migrate/a49c89eba6a4fc172b92b6d8ba40aa9c.png" alt="在这里插入图片描述" style="zoom: 80%;" />

​	Canal 解析 Binlog，提取数据变更内容将 SQL 语句转换为 **JSON 格式**

​	将解析出的数据变更可以推送到ES中

​	优点：延迟低，实时性高，适合高并发、大数据量，支持删除同步



##### Canal挂了怎么办？

> ①使用 Canal 集群（HA 模式）
>
> ​	启动多个 Canal 实例（一般 2 个或 3 个）
>
> ​	搭配 **ZooKeeper** 做 HA，防止主实例挂掉，自动进行主实例的选举
>
> ②对 Canal 服务做健康监控与自动重启（例如使用Prometheus）
>
> ③记录最后消费的 binlog 位点，一般 Canal 本身就会记录在 `meta.dat`，Canal 恢复后自动从上次的位置继续拉取 binlog
>
> ④保证es同步的幂等，再次同步时重复写入不会出错，也不会导致数据不一致
>



##### Canal如何解决延迟或丢失？

> **延迟处理措施**：
>
> - 使用消息队列进行异步削峰
> - 使用批量bulk来更新ES，提升吞吐
>
> **数据丢失防护**：
>
> - 记录最后消费的 binlog 位点，一般 Canal 本身就会记录在 `meta.dat`，Canal 恢复后自动从上次的位置继续拉取 binlog
> - 保证es同步的幂等，再次同步时重复写入不会出错，也不会导致数据不一致



##### Canal无法保证Mysql和ES的数据一致性

> ①同步双写：直接同步写到ES中，适合写少的场景，延迟低
>
> ②异步双写：使用消息队列







### 系统上线后有没有去监控？ :star:

> **系统指标监控**：使用 Actuator + Prometheus + Grafana 收集 JVM、CPU、内存、线程池、GC 等运行时指标，并做实时可视化，及时发现资源异常；
>
> ​	Springboot可以使用Actuator，Actuator 负责暴露应用指标，如暴露 ` /actuator/prometheus 接口` 接口，访问这个接口localhost:8080/actuator/prometheus会返回各种指标
>
> ​	Prometheus 负责采集这个**接口**上的指标，每个15s主动拉取指标
>
> ​	Grafana 负责可视化和告警	
>
> **应用日志监控**：通过 ELK 搭建日志服务系统
>
> ​	Logstash 支持从多种来源采集、过滤、转发日志
>
> ​	ElasticSearch 负责日志索引构建，加快日志的查询速度
>
> ​	Kanban 提供搜索、分析、可视化界面
>
> **ES 的健康状态**：对于海量文书检索系统，也对 ES 的健康状态进行了监控
>
> ​	同样可以使用 Prometheus + Grafana，定期收集ES集群的健康状态

##### 一般可以关注下面这些参数：

> ######  1. 应用运行指标（JVM、线程池）
>
> | 指标                  | 说明                           |
> | --------------------- | ------------------------------ |
> | jvm.memory.used / max | JVM 内存使用情况，是否接近 OOM |
> | jvm.threads.live      | 当前线程数量，是否暴涨         |
> | executor.active       | 活跃线程数                     |
> | executor.queue.size   | 线程池队列长度，任务是否堆积   |
> | gc.count / gc.time    | 垃圾回收次数和时间，是否频繁   |
>
> ------
>
> ###### 2. 接口指标（请求数、错误、耗时）
>
> | 指标                          | 说明                |
> | ----------------------------- | ------------------- |
> | http.server.requests.count    | 请求数量            |
> | http.server.requests.errors   | 错误数（4xx/5xx）   |
> | http.server.requests.duration | 请求耗时分布        |
> | es.query.latency / count      | ES 查询的耗时和次数 |
> | redis.command.latency         | Redis 命令耗时      |
>
> ######  3. 基础设施（数据库、Redis、队列）
>
> | 指标                      | 说明               |
> | ------------------------- | ------------------ |
> | db.connection.usage       | 数据库连接池使用量 |
> | redis.memory.used_percent | Redis 内存占用比例 |
> | rabbitmq.queue.size       | 消息积压量         |
> | es.index.search.time      | ES 查询时间        |

##### 告警：通过告警第一时间获知

> 我们配置了 Prometheus 的 `AlertManager`；
>
> 比如说，对一些关键指标设定阈值，如：
>
> - 接口5xx比例 > 5%
> - **某个接口平均响应时间 > 2s**
> - **JVM 内存使用率 > 80%**，及时排查内存泄漏的问题
> - **线程池队列长度 > 1000**，排查任务是否积压

##### 如果某台服务器出现问题怎么办？

> 单机部署：监控 + 预警 + 尝试重启
>
> ​				 如果该版本出现一些影响可用性的bug，先回滚到上一个版本，并紧急查看具体问题并修复
>
> ​			   k8s可以实现服务的自动重启
>
> 集群部署：
>
> ​	负载均衡使用Nginx，Nginx 会根据配置的 **健康检查机制** 自动将请求路由到其他正常节点，实现服务不中断
>
> ​	监控系统监控哪台服务器出问题，并尝试重启
>
> ​	Nginx单点故障： **Keepalived + 两台 Nginx 做主备**，构建一个高可用的负载均衡层





### 关于Vue

##### 1.vue的生命周期

Vue 3 通过 `onXxx` 形式提供组合式 API 版本的生命周期钩子

| Vue 2           | Vue 3 (Composition API)                  |
| --------------- | ---------------------------------------- |
| `beforeCreate`  | **已废弃**（setup 阶段替代）             |
| `created`       | **已废弃**（setup 阶段替代）             |
| `beforeMount`   | `onBeforeMount`                          |
| `mounted`       | `onMounted`                              |
| `beforeUpdate`  | `onBeforeUpdate`                         |
| `updated`       | `onUpdated`                              |
| `beforeDestroy` | `onBeforeUnmount`                        |
| `destroyed`     | `onUnmounted`                            |
| -               | `onRenderTracked`（调试用）              |
| -               | `onRenderTriggered`（调试用）            |
| -               | `onActivated`（keep-alive 组件激活时）   |
| -               | `onDeactivated`（keep-alive 组件停用时） |
| -               | `onErrorCaptured`（捕获子组件错误）      |

###### 1. 创建阶段（Initialization）

> **Vue 2**
>
> - `beforeCreate`：实例初始化，`data` 和 `methods` 还不可用。
> - `created`：实例已创建，`data` 可用，适合发起 API 请求或初始化数据。
>
> **Vue 3**
>
> - `setup()` 取代 `beforeCreate` 和 `created`，数据已初始化。
> - `onBeforeMount()`：Vue 模板编译完毕，虚拟 DOM 已创建但未渲染到真实 DOM。

###### 2. 挂载阶段（Mounting）

> **Vue 2**
>
> - `beforeMount`：Vue 虚拟 DOM 已创建，但未渲染到页面上。
> - `mounted`：Vue 组件已挂载，DOM 可操作，适合获取 `ref`、执行动画、发起 API 请求。
>
> **Vue 3**
>
> - `onMounted()`：等同于 `mounted`，在 `setup()` 中使用。

###### 3. 更新阶段（Updating）

> 当 `data` 发生变化时，组件会重新渲染。
>
> **Vue 2**
>
> - `beforeUpdate`：数据更新，但 DOM 还未更新。
> - `updated`：数据和 DOM 均更新完成。
>
> **Vue 3**
>
> - `onBeforeUpdate()`：等同于 `beforeUpdate`。
> - `onUpdated()`：等同于 `updated`。

###### 4. 销毁阶段（Unmounting）

> 组件被移除前的清理阶段。
>
> **Vue 2**
>
> - `beforeDestroy`：组件即将销毁，适合清除计时器、解绑事件。
> - `destroyed`：组件已销毁，所有 Vue 绑定都被移除。
>
> **Vue 3**
>
> - `onBeforeUnmount()`：等同于 `beforeDestroy`。
> - `onUnmounted()`：等同于 `destroyed`。

###### 5. 额外的 Vue 3 生命周期

> Vue 3 还提供了一些额外的生命周期钩子：
>
> - `onActivated()` / `onDeactivated()`：适用于 `keep-alive` 组件的激活和停用
> - `onRenderTracked()` / `onRenderTriggered()`：用于调试 Vue 响应式系统
> - `onErrorCaptured()`：用于捕获子组件的错误



##### 2.前后端联调时跨域问题怎么解决？

可以从前端、后端来解决

后端：CORS，**让后端添加跨域响应头**，允许前端访问不同源的资源

```java
@Configuration
public class CorsConfig {
    @Bean
    public WebMvcConfigurer corsConfigurer() {
        return new WebMvcConfigurer() {
            @Override
            public void addCorsMappings(CorsRegistry registry) {
                registry.addMapping("/**")
                        .allowedOrigins("http://localhost:3000")  // 允许的前端地址
                        .allowedMethods("GET", "POST", "PUT", "DELETE", "OPTIONS")
                        .allowCredentials(true);
            }
        };
    }
}
```

前端：反向代理

**Nginx 代理**

```
nginxCopy codeserver {
    listen 80;
    server_name frontend.example.com;

    location /api/ {
        proxy_pass http://backend.example.com:8080/;
        proxy_set_header Host $host;
    }
}
```

* 使适用于生产环境

**Vue / Webpack 开发代理** 修改 `vue.config.js`

```
jsCopy codemodule.exports = {
    devServer: {
        proxy: {
            '/api': {
                target: 'http://localhost:8080',  // 代理到后端
                changeOrigin: true,
                pathRewrite: { '^/api': '' }  // 省略 /api 前缀
            }
        }
    }
};
```

🔹 **适用于**：前端本地开发时解决跨域问题，生产环境不适用



##### 3.vue父子组件怎么传递数据

> ###### 父组件 → 子组件
>
> 方法 1：`props` 传递：父组件可以通过 `props` 向子组件传递数据
>
> ###### 子组件 → 父组件
>
> 方法 2：`$emit` 事件：子组件可以使用 `$emit` 触发父组件方法，从而传递数据
>
> ###### 兄弟组件之间
>
> 方法 3：`Mitt` 事件总线：Vue 3 没有 `this.$bus`，可以使用 `mitt` 作为事件总线
>
> A组件向eventBus提交事件，B组件在eventBus上监听该事件
>
> ###### 深层嵌套组件（跨多级组件）
>
> 方法 4：`provide/inject`
>
> `provide` 让祖先组件提供数据，`inject` 让后代组件使用数据。
>
> ###### Vuex/Pinia（状态管理，全局共享数据）
>
> 如果多个组件需要共享数据，可以使用 Vuex（Vue2）/ Pinia（Vue3）



##### 4.Vue的响应式编程原理



### 如何全局处理Controller的异常

在 Spring Boot 中，可以使用 **`@ControllerAdvice`** 结合 **`@ExceptionHandler`** 来全局处理 Controller 层抛出的异常。这样可以避免在每个 Controller 里都编写重复的异常处理代码

###### 1.首先定义所有的业务异常

基类

```java
public class CustomException extends RuntimeException {
    // http状态码  404 ...
    private final HttpStatus statusCode;
    // 系统自定义的业务状态码  例如1001表示未找到资源 1002表示...
    private final ErrorCode error;  

    public CustomException(String message, HttpStatus statusCode) {
        super(message);
        this.statusCode = statusCode;
    }

    public HttpStatus getStatusCode() {
        return statusCode;
    }
    
    public ErrorCode getError() {
        return error;
    }
}
```

具体异常类，继承基类

```java
/** 资源不存在异常 */
public class ResourceNotFoundException extends CustomException {
    public ResourceNotFoundException(Map<String, Object> data) {
        super(ErrorCode.RESOURCE_NOT_FOUND, data);
    }
}
```

###### 2.自定义异常响应体

当发生异常时，可以返回这个响应体给前端

```java

publicclass ErrorReponse {
    privateint code;    // 业务代码
    privateint status;  // http状态码
    private String message;  // 异常消息
    private Instant timestamp;// 时间戳

    public ErrorReponse() {
    }
    ...
}
```

###### 3.定义异常处理器

类上加`@RestControllerAdvice`注解

不同的异常处理方法上加`@ExceptionHandler(异常类.class)`

```java
@RestControllerAdvice
public class GlobalExceptionHandler {
    
    @ExceptionHandler(CustomException.class)
    public ResponseEntity<?> handleAppException(CustomException ex, HttpServletRequest request) {
        // 返回异常响应体
        ErrorReponse representation = new ErrorReponse(ex, request.getRequestURI());
        returnnew ResponseEntity<>(representation, new HttpHeaders(), ex.getError().getStatus());
    }

    @ExceptionHandler(value = ResourceNotFoundException.class)
    public ResponseEntity<ErrorReponse> handleResourceNotFoundException(ResourceNotFoundException ex, HttpServletRequest request) {
        // 返回异常响应体
        ErrorReponse errorReponse = new ErrorReponse(ex, request.getRequestURI());
        return ResponseEntity.status(HttpStatus.BAD_REQUEST).body(errorReponse);
    }
}
```

这里有两个`@ExceptionHandler`，看Controller中抛出什么异常，最匹配的优先

这两个方法中都返回了我们自定义的ErrorResponse响应

###### 4.Controller中抛异常

```java
@RestController
@RequestMapping("/person")
publicclass PersonController {

    @GetMapping("/getPersonById")
    public Person getPersonById (@RequsetParam("id") int id) {
        Person p = personService.findPersonById(id);
        if(p == null) {
        	throw new ResourceNotFoundException(ImmutableMap.of("person id:", p.getId()));
        } else {
            return p;
        }
    }

}
```

###### 5.前端收到的响应体

```json
{
    "status": 404,
    "code": "1001",
    "message": "未找到资源",
    "timestamp": "2025-03-09T12:34:56"
}
```



### 设计一个高并发/高可用的系统

开放性题目，可以一步一步地分析回答，从每个角度入手，”使用什么来保证/防止什么“

##### 1.实现高性能

①**热点数据**预热：比如某些秒杀活动的商品信息

​	并发量一般的可以把热点数据先预热到Redis缓存中

​	并发量较高的，可以在jvm内存中也缓存一份热点数据

②**静态资源**处理：CDN内容分发网络

​	比如一些商品的图片等等

③数据库**读写分离、冷热分离、分表分库**

④**缓存**：Redis

⑤数据库**索引**

##### 2.实现高可用

①**集群化**：搭建Redis集群，减少单点故障

②**限流**：采用漏桶法、令牌桶法进行限流

​	:star:接口限流：**Sentinel**（阿里巴巴的，经过双十一的考验）

​	问题/验证码：提交订单前回答验证码，进一步减少并发量

​	提前预约：过滤掉部分未预约的用户

③**负载均衡**：七层负载均衡算法：DNS负载均衡、Nginx方向代理

④**流量削峰**：使用**消息队列**来削峰

⑤**熔断/降级**：在**微服务**流行的时代，讲不同业务的服务以微服务的形式部署，不同业务直接互相调用接口进行交互

​	熔断：当某个接口的失败率过高时，系统会自动熔断该接口，短暂拒绝请求，防止**雪崩效应**

​				保证秒杀服务不会影响到其他服务的正常使用

​	降级：当系统资源紧张或服务不可用时，提供**备用方案**（如返回默认值、限制非核心功能），优先**保证核心业务**



##### 3.实现一致性

高并发的环境下，商品库存等数据会存在不一致的情况

①延迟双删，只要保证Mysql最终一致性就行

②Redis/Kafka实现分布式锁

③**乐观锁**：版本号+CAS

④接口幂等：重复操作的结果仍然一致





#### 4.系统测试

上线之前系统需要通过压力测试





### 如何实现==灰度发布==

> 1.通过反向代理或**网关**实现
>
> ​	**Spring Cloud Gateway** ：将请求动态分流到不同的服务版本
>
> ​	**Nginx**：配置基于 `header` 或 `cookie` 进行流量分流
>
> 2.通过负载均衡器
>
> 3.根据用户标签





### 关于系统的并发量

> QPS：每秒请求量  一分钟有1200个请求 1200/60 = 20
>
> ​	通过日志统计获得这些值：Nignx日志、ELK日志、Tomcat的access log统计	
>
> 并发用户量：3000 （假设法院5000人 * 80%）





### 关于定时任务

> 1.Timer：单线程
>
> 2.ScheduleExecutorService接口：线程池并发运行任务，如ScheduledThreadPoolExecutor
>
> 3.SpringTask：@Schedule(cron="") 支持cron表达式
>
> 分布式定时任务：
>
> 1.Redis/Redission：实现延时任务
>
> 2.MQ
>
> 分布式任务调度框架：
>
> 1.xxl-job
>
> 2.PowerJob
>
> 3.Cronjob（字节）



### 如何设计一个高铁票订单系统

考虑车次、余票、是否有座等这些

使用Redis+lua来实现余票的扣除



### 如何设计一个登录系统

> 主要是密码如何安全的存储
>
> ①肯定不可以明文存储
>
> ②对称加密和非对称加密，其实开发者都是能知道用户密码是什么的，所以不可以采取
>
> ③使用**不可逆的hash算法**来存储密码
>
> ​	不能使用MD5或者SHA，因为会被暴力破解出来
>
> ​	可以对密码进行一些特定规则的处理，比如补全到多少位，或者每几个字符中间插入一个字符
>
> ​	可以直接使用**bcrypt**：带**盐**的hash算法，为每个用户生成生成**随机 Salt**（盐就是一串随机字符串）
>
> ​	**用户密码+盐**，进行hash，得到的hash值才不会被破解出来
>
> ​	得到的hash：$2a$10$<u>E8y1rOQmZxZbCNzZ5rc2R</u>.jHiMVPsJru.6uqJq9WkUYGFi.NKOC1i
>
> ​	其中就包含盐为 E8y1rOQmZxZbCNzZ5rc2R

> 接着，客户端在登录时，账户名和密码需要加密后再传输
>
> 可以直接使用https来实现
>
> 也可以手动实现



### Feed流设计

> Feed流有：基于推荐的（如知乎）、基于Timeline的（如朋友圈）
>
> 微博的Feed流：关注的人发了微博，需要实时的推送到用户的主页
>
> ①**推模式**：当一个用户发了微博，那么向所有粉丝的feed表中插入这条微博
>
> ​	问题：大v发送一条微博后，需要向一千万个粉丝的feed表中各插入一条记录，数据写入量太大了
>
> ②**拉模式**：用户自己去拉去他所关注的人的所有微博
>
> ​	问题：用户需要去拉去所有关注的人的微博并进行聚合排序，实时性差，性能比Feed流还要查
>
> ③**推拉结合**：根据用户的粉丝量进行划分是大v还是普通用户，再根据用户的登录状态来确定是活跃用户还是非活跃用户
>
> ​	当大v发微博时，向活跃用户推送，非活跃用户自己拉取



### 短链系统

> 短链：对一个长url进行压缩得到一个短的url
>
> 访问一个短链的过程：
>
> 1. 用户点击短链
> 2. 服务器**查表**（数据库或者Redis），找到对应的长链
> 3. 服务端返回一个**302**临时重定向，是对应的长链
> 4. 浏览器重定向到长链
>
> 为什么不用301：301是永久重定向，浏览器会缓存这个长链地址，下次再访问短链就会直接拿出这个长链地址
>
> 短链生成：一般使用非加密哈希算法（比加密哈希算法MD5、SHA效率更高）
>
> 哈希冲突：多个长链经过哈希可能会得到同一个短链，怎么解决？
>
> 1. 在数据库中对短链连接字段添加**唯一索引**（效率低）
> 2. 使用**布隆过滤器**来判断url是否重复



### 开放题

##### 1.项目中遇到的难点，如何解决

> ①ES搜索提示如何实现的，从官方文档再到自己的研究
>
> ②设计上的一些分歧，团队之间如何讨论

##### 2.在开发过程中有没有跟队员有过意见分歧，如何解决？

> 在设计文书基本属性表的时候，每篇文书都需要关联一位法官，即完成这篇文书的法官
>
> 正常想法就是通过法官的id去关联，再通过联表来获取**法官姓名**
>
> 但是我觉得因为一篇文书既然已经完成了，那这个法官是不会变动的，我认为可以适当的冗余，存储法官的姓名。再加上法官的姓名也不大可能获取修改，不再需要去关联法官的id获取法官的姓名。
>
> 原因有两点：第一，文书任务是历史性的，一旦发起后承办人的信息应该固化记录下来，不应该随着人员表变化而变化；第二，这些字段在列表页、导出、查询等高频场景中都要用到，如果每次都去 join 人员表，会影响性能这样也可以保证对文书表的查询效率。
>
> 我觉得技术上的分歧是正常的，关键是团队能以“问题为中心”，在尊重彼此想法的基础上做出最优选择。这种磨合其实也提升了我们的协作效率。

#### 3.如何解决bug

> 思路：问题复现 => 请求定位 => 原因定位 => 构成测试用例 => 修复
>
> 收集问题的具体细节，如何触发，是否可以复现
>
> 接着在**测试环境尝试复现**，发现确实请求发出后服务端没有响应结果
>
> 查看触发的相关请求，查询日志
>
> 通过日志排查，发现接口确实被调用了，但在执行导出逻辑时抛出了一个“**空指针异常**”
>
> 构造测试用例进行调试后发现，问题出在某个用户没有设置某个可选字段，而我们的导出模板却默认去读取它，导致出错（**定位原因**）
>
> 我在代码中增加了对该字段的非空判断，并为空值添加了默认值处理逻辑。同时为了保险，还更新了导出模板做了容错处理（**修复方式**）
>
> 修复后我在测试环境用不同的用户进行了多组测试，并通过日志检查了潜在异常，确保没有遗漏，然后将修复发布上线（**验证回归**）
>
> 这次经历让我意识到，在处理用户可选字段时一定要考虑极端情况，并在日志中记录更多上下文，后续我们也补充了这块的单元测试（**总结提升**）

#### 4.实习过程中遇到的困难？

> 在刚开始实习的时候，困难主要是：
>
> 1. **对业务的理解不够深入**：我刚进团队时，对法院整体的业务流程还不熟悉，所以在法官反馈问题时，我甚至不知道他是如何触发的，在哪个页面做了什么行为。这些都是要问mentor或者其他团队成员来了解
> 2. **用户行为理解有限**：对法官的办公逻辑的了解
> 3. **代码阅读和开发环境适应**：每个人负责的项目比较多，代码不熟悉，也需要去熟悉这边的开发模式
> 4. **数据库的理解**：表很多，哪些数据存储在哪个表里面，表与表之间如何关联，都需要问清楚
>
> 为了解决这些困难，我一方面主动请教mentor和组内同事，另一方面通过阅读内部文档
>
> 慢慢地得心应手，改起代码来就比较顺手

> 实习中后期，困难主要在于设计上的一些分歧，可以举上面提到的分歧
>
> 也可以说如何平衡实习+学业+考试

#### 5.我的职业规划

> **目前来看**，我希望我能找到一个好的暑期实习，最好可以到大厂锻炼，提高自己的各方面的开发能力
>
> **短期来看**，我希望在研究生这两年能有所进步，顺利毕业。并找到一份满意的工作。<u>尽快适应从学校到职场的转变</u>，打好技术基础，熟悉业务流程，并在实际项目中不断积累经验，提高自己的编码能力、问题分析能力和团队协作能力。
>
> **长期来看**，我希望能在我的工作中，摸清我到底喜欢什么方向，逐渐深入

#### 6.自己的优点、缺点（或者用三个词形容自己）

> 优点：
>
> ​	①自主学习能力强
>
> ​	②自律，计划驱动
>
> ​	③不容易受外界干扰
>
> 缺点：
>
> ​	①太注重细节：比如有时候新需求的时间急，但还是会花时间去想着如何优化我刚刚写的这几个函数，接口如何优化。
>
> ​	②有时候会对即将到来的事情感到焦虑，但我认为不算是恶性的，是良性的，它会促使我更早地去做准备
>
> ​	③不大自信，就算自己有100%的把握是正确的，还是不敢举手回答

> ###### 相比别人，我的竞争优势在哪？
>
> ①我本科学校的培养，基础扎实，也有实践
>
> ②我在天津法院有一年的实习经历，相比别人可能上手快，代码、文档阅读能力强
>
> ③有团队协作的经验，能更快地融入团队并适应新的工作环境
>
> ④抗压能力大

#### 7.最近有没有在学习什么新的知识？

> 直接说大模型，因为大模型火，各个企业也在积极的推进大模型的嵌入



## 大模型

##### RAG (Retrieval-Augmented Generation)

**RAG（检索增强生成）= 检索技术 + LLM 提示**

例如，我们向 LLM 提问一个问题（answer），RAG 从各种[数据源](https://so.csdn.net/so/search?q=数据源&spm=1001.2101.3001.7020)检索相关的信息，并将检索到的信息和问题（answer）注入到 LLM 提示中，LLM 最后给出答案

完整的RAG应用流程主要包含两个阶段：

- 数据准备阶段：数据提取——>文本分割——>向量化（embedding）——>数据入库
- 应用阶段：用户提问——>数据检索（召回）——>注入Prompt——>LLM生成答案

![图片](https://i-blog.csdnimg.cn/blog_migrate/0d4e171e5bded3da2e030b7af16e5ea2.png)

数据检索：根据用户输入的问题，查找相似度最大的数据

注入prompt：可以将检索到的数据作为背景知识{text}填充到prompt中

```
【任务描述】
假如你是一个专业的客服机器人，请参考【背景知识】，回
【背景知识】
{content} // 数据检索得到的相关文本
【问题】
石头扫地机器人P10的续航时间是多久？
```

RAG实现

「第一步：数据检索」 将数据存入矢量数据库后，就可以将其定义为检索器组件，该组件根据用户查询和嵌入块之间的语义相似性获取相关上下文

```python
retriever = vectorstore.as_retriever()
```

「第二步：提示增强」 完成数据检索之后，就可以使用相关上下文来增强提示。在这个过程中需要准备一个提示模板。可以通过提示模板轻松自定义提示，如下所示。

```python
from langchain.prompts import ChatPromptTemplate
template = """
	你是一个问答机器人助手，请使用以下检索到的上下文来回答问题，如果你不知道答案，就说你不知道。
	问题是：{question},
	上下文: {context},
	答案是:	
"""
prompt = ChatPromptTemplate.from_template(template)
```

「第三步：答案生成」 利用 RAG 管道构建一条链，将检**索器、提示模板和 LLM** 链接在一起。定义了 **RAG 链**，就可以调用它了

```python
from langchain.chat_models import ChatOpenAI
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

rag_chain = (
    {"context": retriever,  "question": RunnablePassthrough()} 
    | prompt 
    | llm
    | StrOutputParser() 
)
```

「第四步：交互」

```python
query = "萧炎的表妹是谁？"
res=rag_chain.invoke(query)
print(f'答案：{res}')
```



##### Deepseek-v3 和 Deepseek-r1 各自的创新点在哪？

> **Deepseek-v3**：优势在于高效的多模态处理能力（文本、图像、音频、视频）和较低的训练成本
>
> 通过算法优化降低算力需求。其创新点包括负载均衡和多令牌预测技术，训练成本仅为同类闭源模型的1/20
>
> **DeepSeek-R1** ：专注于高级推理任务

###### Deekseek-v3为什么需要那么少的算力？

模型剪枝（Pruning） + 量化（Quantization） + 知识蒸馏（Knowledge Distillation）

- 剪枝：移除冗余神经元（如低权重连接）
- 量化：将32位浮点参数压缩为8位整数
- 蒸馏：用小模型模仿大模型的行为逻辑

知识蒸馏：用一个大型、性能强但计算开销大的“老师模型”，来训练一个小型、轻量但效果不错的“学生模型“



##### Deepseek本地部署

工具：Ollama

系统：Linux

1.拉取模型

选择7b参数的模型(基础版)：至少8GB空闲内存，至少6GB显存

33b参数：需要更高的显存要求,至少32GB空闲内存，至少20GB显存

```shell
ollama pull deepseek-r1:7b
```

2.运行模型

```shell
ollama run deepseek-r1
```



#### Agent

组成：

**大模型Agent由规划、记忆、工具与行动四大关键部分组成**，分别负责任务拆解与策略评估、信息存储与回忆、环境感知与决策辅助、以及将思维转化为实际行动

**规划**（Planning）：

​	定义：规划是Agent的思维模型，负责拆解复杂任务为可执行的子任务，并评估执行策略。

​	实现方式：通过大模型提示工程（Prompt）实现，使Agent能够精准拆解任务，分步解决。

**记忆**（Memory）：

​	定义：记忆即信息存储与回忆，包括短期记忆和长期记忆。

​	实现方式：短期记忆用于存储会话上下文，支持多轮对话；长期记忆则存储用户特征、业务数据等，通常通过**向量数据库**等技术实现快速存取。

**工具**（Tools）：

​	定义：工具是Agent感知环境、执行决策的辅助手段，如API调用、插件扩展等。

​	实现方式：通过接入外部工具（如API、插件）扩展Agent的能力，如ChatPDF解析文档、Midjourney文生图等。

**行动**（Action）：

​	定义：行动是Agent将规划与记忆转化为具体输出的过程，包括与外部环境的互动或工具调用。

​	实现方式：Agent根据规划与记忆执行具体行动，如智能客服回复、查询天气预报、AI机器人抓起物体等。

![img](https://img-blog.csdnimg.cn/img_convert/64ce9dc75c5df9be43579f1c4b193690.png)



RAG相当于Agent的外部知识库

![img](https://i-blog.csdnimg.cn/blog_migrate/6811368004cb8e65177ec436a49ec1ad.png)



#### MAS（Multi-Agent System, 多智能体系统）

由多个相互作用的智能体组成的系统，这些智能体可以是软件程序、机器人、传感器或者任何能够感知环境并作出决策的实体。在多智能体系统中，每个智能体都有自己的目标和能力，并且它们能够相互通信和协作，以解决复杂的问题或完成特定的任务



#### MCP（Model Context Protocol，模型上下文协议）

旨在**统一**大型语言模型与**外部数据源和工具**之间的通信协议，无需逐个适配

解决 **模型在不同系统或智能体之间传递与理解的问题**

MCP 的主要目的在于解决当前 AI 模型因**数据孤岛限制**而无法充分发挥潜力的难题，MCP 使得 AI 应用能够安全地访问和操作本地及远程数据，为 AI 应用提供了连接万物的接口

例如GPT之前的知识储备是在2022年之前，现在可以联网搜索

###### 框架：

MCP 主机（MCP Hosts）：发起请求的 LLM 应用程序

MCP 客户端（MCP Clients）：在主机程序内部，与 MCP server 保持 1:1 的连接

* 从MCP Server获取可用的工具，返回给LLM
* LLM自行决定是否调用这些工具
* 调用同样有MCP Client来调用MCP Server提供的方法，并将结果返回给LLM

MCP 服务器（MCP Servers）：为 MCP client 提供上下文、工具和 prompt 信息，连接着本地/远程的知识数据库

* 资源：可被客户端读取的文件等
* 工具：可被LLM调用的函数等
* 提示（Prompt）：预先编写好的提示词

本地资源（Local Resources）：本地计算机中可供 MCP server 安全访问的资源（例如文件、数据库）

远程资源（Remote Resources）：MCP server 可以连接到的远程资源

<img src="https://i-blog.csdnimg.cn/img_convert/77afbba7a4543387ddb1f1827dac0b71.png" alt="img" style="zoom:50%;" />



<img src="面经.assets/ab1d4655bff554153f2e35fac28472ad.png" alt="图片" style="zoom:50%;" />

###### 例如：

通过 [PostgreSQL MCP Server](https://github.com/modelcontextprotocol/servers/tree/main/src/postgres) 使 LLM 能够基于 PostgreSQL 中的数据来回答问题

用户提问：数据库中有哪些表？

LLM会通过MCP Server提供的工具接口来查询数据库中的表

可以理解为：LLM将用户提问（如：“查询金额最大的账户”）转换为对应的sql语句，再通过MCP Client来调用MCP Server提供的接口来查询结果并返回

